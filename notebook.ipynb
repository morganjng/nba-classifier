{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a370a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "773c346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural net class\n",
    "# This needs work. I think the brunt of the work here is getting from our, currently 3x120x120, jgps and 4x120x120 pngs to\n",
    "# a 151 length result vector through convolution. I think the LeNet5 diagram has a good indication of what\n",
    "# to do but if any of you have more of an idea let me know and put something in the README.md file.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sm = nn.Softmax()\n",
    "        self.pool1 = nn.AvgPool2d(3, stride=2)\n",
    "        self.pool2 = nn.AvgPool2d(3, stride=2)\n",
    "        self.pool3 = nn.MaxPool2d(3, stride=2)\n",
    "        self.conv1 = nn.LazyConv2d(96, 11, stride=4)\n",
    "        self.conv2 = nn.LazyConv2d(256, 7, padding=2)\n",
    "        self.conv3 = nn.LazyConv2d(384, 5, padding=1)\n",
    "        self.conv4 = nn.LazyConv2d(384, 5, padding=1)\n",
    "        self.flat = nn.Flatten(0)\n",
    "        self.lin1 = nn.Linear(3456, 151, )\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(F.relu(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(F.relu(x))\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(F.relu(x))\n",
    "        x = self.pool3(F.relu(x))\n",
    "        x = self.flat(x)\n",
    "        x = self.lin1(x)\n",
    "        return F.softmax(F.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6003135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing so that we can process images easily\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        transforms.Resize([224, 224]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def one_hot_tensor(x):\n",
    "    l = [0.0 for i in range(151)]\n",
    "    l[x - 1] = 1.0\n",
    "    return torch.tensor(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9f808a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations\n",
    "cnn = CNN()\n",
    "# cnn = torchvision.models.AlexNet(num_classes=151)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.01, momentum=0.9)\n",
    "# Paths to images for training data in our repo AKA pokedex numbers\n",
    "pdns = sorted([int(x) for x in os.listdir(\"train/\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb685ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    images = {\n",
    "        label: [\n",
    "            \"train/\" + str(label) + \"/\" + image_name\n",
    "            for image_name in os.listdir(\"train/\" + str(label) + \"/\")\n",
    "        ]\n",
    "        for label in pdns\n",
    "    }\n",
    "\n",
    "    count = 0\n",
    "    rl = 0.0\n",
    "    for i in pdns:\n",
    "        print(\"Training PDN: \" + str(i))\n",
    "        inputs, labels = (\n",
    "            [transform(Image.open(path).convert(\"RGB\")) for path in images[i]],\n",
    "            [one_hot_tensor(i) for x in images[i]],\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for j in range(len(inputs)):\n",
    "            x = inputs[j]\n",
    "            y = labels[j]\n",
    "            out = cnn(x)\n",
    "            # print(cnn.conv1.parameters())\n",
    "            print(out, torch.argmax(out))\n",
    "            opt = loss(out, y)\n",
    "            opt.backward()\n",
    "            optimizer.step()\n",
    "            count += 1\n",
    "            rl += opt.item()\n",
    "            if count % 50 == 0:\n",
    "                print(str(count) + \" with loss of \" + str(rl))\n",
    "                rl = 0.0\n",
    "\n",
    "    torch.save(cnn.state_dict(), \"cnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7754ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo_train():\n",
    "    test_images = {\n",
    "        label: [\n",
    "            \"train/\" + str(label) + \"/\" + image_name\n",
    "            for image_name in os.listdir(\"train/\" + str(label) + \"/\")\n",
    "        ]\n",
    "        for label in pdns\n",
    "    }\n",
    "\n",
    "    testx = []\n",
    "    for i in pdns:\n",
    "        for j in range(len(test_images[i])):\n",
    "            testx.append([test_images[i][j], torch.LongTensor([i - 1.0])])\n",
    "\n",
    "    print(len(testx))\n",
    "    random.shuffle(testx)\n",
    "    print(len(testx))\n",
    "\n",
    "    count = 0\n",
    "    rl = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for XY in testx:\n",
    "        # print(XY[0])\n",
    "        res = cnn(transform(Image.open(XY[0]).convert(\"RGB\")))\n",
    "        l = loss(res, XY[1][0])\n",
    "        print(torch.argmax(res).item(), XY[1][0].item(), l.item())\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        rl += l.item()\n",
    "        if count % 100 == 0:\n",
    "            print(str(count) + \" with loss of \" + str(rl))\n",
    "            rl = 0.0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    torch.save(cnn.state_dict(), \"cnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e19d94ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    cnn.load_state_dict(torch.load(\"cnn.pth\"))\n",
    "    test_images = {\n",
    "        label: [\n",
    "            \"test/\" + str(label) + \"/\" + image_name\n",
    "            for image_name in os.listdir(\"test/\" + str(label) + \"/\")\n",
    "        ]\n",
    "        for label in pdns\n",
    "    }\n",
    "\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    for i in pdns:\n",
    "        inputs, labels = (\n",
    "            [transform(Image.open(path).convert(\"RGB\")) for path in test_images[i]],\n",
    "            [one_hot_tensor(i) for x in test_images[i]],\n",
    "        )\n",
    "        \n",
    "        print(\"Testing PDN: \" + str(i))\n",
    "\n",
    "        prev_count = count\n",
    "        prev_correct = correct\n",
    "        for j in range(len(inputs)):\n",
    "            x = inputs[j]\n",
    "            y = labels[j]\n",
    "            res = cnn(x)\n",
    "            # print(res)\n",
    "            print(\"True: \" + str(torch.argmax(y).item()) + \" Guess: \" + str(torch.argmax(res).item()))\n",
    "            if torch.argmax(res) == torch.argmax(y):\n",
    "                correct += 1\n",
    "            count += 1\n",
    "\n",
    "        print(\n",
    "            \"For PDN: \"\n",
    "            + str(i)\n",
    "            + \" got \"\n",
    "            + str(correct - prev_correct)\n",
    "            + \" out of \"\n",
    "            + str(count - prev_count)\n",
    "            + \" correct\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "33a2c58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2804/1933967476.py:13: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  testx.append([test_images[i][j], torch.LongTensor([i - 1.0])])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4923\n",
      "4923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2804/2592652201.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(F.relu(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 92 5.017336368560791\n",
      "149 93 5.017368316650391\n",
      "149 149 5.016597747802734\n",
      "149 52 5.017221927642822\n",
      "149 123 5.017141819000244\n",
      "130 23 5.017319202423096\n",
      "149 6 5.0171427726745605\n",
      "149 61 5.017178535461426\n",
      "119 134 5.017364025115967\n",
      "86 63 5.017348766326904\n",
      "149 34 5.017387390136719\n",
      "130 135 5.01733922958374\n",
      "130 52 5.017303943634033\n",
      "130 140 5.017377853393555\n",
      "119 16 5.017364025115967\n",
      "102 42 5.017284870147705\n",
      "149 150 5.0172882080078125\n",
      "149 18 5.017344951629639\n",
      "149 64 5.017247676849365\n",
      "149 101 5.0172858238220215\n",
      "149 117 5.017370700836182\n",
      "130 127 5.017364978790283\n",
      "149 108 5.017337322235107\n",
      "130 3 5.017363548278809\n",
      "130 79 5.0173563957214355\n",
      "130 116 5.016959190368652\n",
      "119 13 5.017199516296387\n",
      "149 34 5.01737642288208\n",
      "130 43 5.0173659324646\n",
      "69 61 5.017155170440674\n",
      "149 101 5.01732063293457\n",
      "130 3 5.017378330230713\n",
      "130 23 5.017371654510498\n",
      "130 26 5.017277717590332\n",
      "130 111 5.017141342163086\n",
      "119 76 5.017033100128174\n",
      "149 8 5.0173444747924805\n",
      "149 116 5.017038345336914\n",
      "149 55 5.017265319824219\n",
      "130 84 5.0173659324646\n",
      "149 96 5.0172929763793945\n",
      "130 39 5.017348289489746\n",
      "149 32 5.017103672027588\n",
      "109 41 5.0173516273498535\n",
      "149 133 5.017307758331299\n",
      "149 67 5.017348766326904\n",
      "130 112 5.017368316650391\n",
      "130 70 5.01736307144165\n",
      "149 29 5.016960620880127\n",
      "130 19 5.01736307144165\n",
      "130 2 5.017350196838379\n",
      "149 32 5.017022609710693\n",
      "149 45 5.017201900482178\n",
      "130 15 5.017296314239502\n",
      "149 140 5.017329692840576\n",
      "69 10 5.0173468589782715\n",
      "119 142 5.017362594604492\n",
      "149 108 5.017340660095215\n",
      "130 29 5.0171051025390625\n",
      "40 52 5.017259120941162\n",
      "130 95 5.017374038696289\n",
      "149 129 5.017336368560791\n",
      "119 120 5.017194747924805\n",
      "149 109 5.0170674324035645\n",
      "149 71 5.01737117767334\n",
      "149 110 5.017329692840576\n",
      "149 74 5.017341613769531\n",
      "130 65 5.017366886138916\n",
      "119 42 5.017360210418701\n",
      "119 41 5.017370223999023\n",
      "149 48 5.0173540115356445\n",
      "149 52 5.017218112945557\n",
      "69 10 5.017317771911621\n",
      "149 56 5.017317295074463\n",
      "149 18 5.017174243927002\n",
      "119 133 5.017228603363037\n",
      "130 104 5.017246246337891\n",
      "130 105 5.017134666442871\n",
      "130 45 5.017263412475586\n",
      "119 90 5.0173468589782715\n",
      "130 24 5.017366886138916\n",
      "40 24 5.01737642288208\n",
      "149 63 5.017373085021973\n",
      "130 9 5.017273902893066\n",
      "119 150 5.017385959625244\n",
      "149 88 5.017329692840576\n",
      "149 110 5.0173444747924805\n",
      "149 56 5.017358779907227\n",
      "149 66 5.017266750335693\n",
      "149 114 5.017361640930176\n",
      "130 139 5.017331123352051\n",
      "149 90 5.017349720001221\n",
      "149 29 5.017049312591553\n",
      "149 7 5.017245769500732\n",
      "40 126 5.017303466796875\n",
      "149 137 5.017366886138916\n",
      "149 92 5.017334938049316\n",
      "130 77 5.017220973968506\n",
      "149 112 5.017373561859131\n",
      "149 121 5.017342567443848\n",
      "100 with loss of 501.7281222343445\n",
      "149 14 5.017171859741211\n",
      "149 133 5.017366409301758\n",
      "130 98 5.0173540115356445\n",
      "119 113 5.0173468589782715\n",
      "149 90 5.017350673675537\n",
      "130 57 5.017385005950928\n",
      "149 39 5.017385482788086\n",
      "149 7 5.017208576202393\n",
      "149 73 5.017327785491943\n",
      "130 59 5.017332553863525\n",
      "130 125 5.017338275909424\n",
      "130 80 5.01738166809082\n",
      "130 26 5.017271995544434\n",
      "149 80 5.017355442047119\n",
      "130 12 5.017357349395752\n",
      "130 42 5.017273426055908\n",
      "149 127 5.017330169677734\n",
      "130 122 5.017370223999023\n",
      "130 77 5.017212390899658\n",
      "130 126 5.017180919647217\n",
      "149 107 5.01735782623291\n",
      "130 119 5.016961097717285\n",
      "149 78 5.017376899719238\n",
      "149 90 5.017336845397949\n",
      "130 44 5.017365455627441\n",
      "149 11 5.017151832580566\n",
      "119 105 5.017212390899658\n",
      "149 11 5.017175674438477\n",
      "149 30 5.017282009124756\n",
      "149 126 5.01714563369751\n",
      "149 49 5.017334461212158\n",
      "149 58 5.017084121704102\n",
      "149 71 5.017370223999023\n",
      "149 77 5.017233371734619\n",
      "149 57 5.017378807067871\n",
      "130 40 5.016928672790527\n",
      "149 138 5.017272472381592\n",
      "130 37 5.017343997955322\n",
      "119 145 5.017237186431885\n",
      "130 149 5.016887664794922\n",
      "149 76 5.01713228225708\n",
      "149 25 5.017335414886475\n",
      "130 2 5.017342567443848\n",
      "149 93 5.017366886138916\n",
      "149 94 5.017185688018799\n",
      "130 44 5.017340183258057\n",
      "116 93 5.017331600189209\n",
      "130 123 5.017264366149902\n",
      "149 77 5.0172271728515625\n",
      "130 23 5.0173444747924805\n",
      "130 147 5.017267227172852\n",
      "149 113 5.017334461212158\n",
      "149 94 5.017330169677734\n",
      "86 124 5.01732873916626\n",
      "149 120 5.017224311828613\n",
      "119 95 5.017345428466797\n",
      "130 85 5.017394542694092\n",
      "119 14 5.01708984375\n",
      "130 48 5.017326354980469\n",
      "149 22 5.017242431640625\n",
      "130 22 5.017388820648193\n",
      "149 112 5.017374038696289\n",
      "86 127 5.017355442047119\n",
      "149 142 5.017378807067871\n",
      "149 141 5.017115592956543\n",
      "149 82 5.017257213592529\n",
      "69 138 5.017348766326904\n",
      "149 108 5.017323017120361\n",
      "40 24 5.017378807067871\n",
      "149 87 5.01732873916626\n",
      "40 69 5.017054557800293\n",
      "119 139 5.017261028289795\n",
      "40 144 5.017171859741211\n",
      "130 131 5.017369747161865\n",
      "149 93 5.017335891723633\n",
      "149 65 5.017354965209961\n",
      "130 125 5.0172624588012695\n",
      "130 114 5.0173563957214355\n",
      "149 108 5.01735258102417\n",
      "119 120 5.0172224044799805\n",
      "130 73 5.017319679260254\n",
      "149 48 5.017324447631836\n",
      "149 136 5.017354488372803\n",
      "130 99 5.017261981964111\n",
      "119 22 5.01729679107666\n",
      "130 105 5.0171661376953125\n",
      "149 93 5.017362117767334\n",
      "130 44 5.0173420906066895\n",
      "130 99 5.017242908477783\n",
      "69 86 5.017005443572998\n",
      "149 82 5.017309188842773\n",
      "149 6 5.017183303833008\n",
      "149 7 5.017221927642822\n",
      "40 121 5.017359256744385\n",
      "130 36 5.017359256744385\n",
      "149 15 5.01727819442749\n",
      "130 12 5.017348289489746\n",
      "130 54 5.017339706420898\n",
      "149 94 5.017272472381592\n",
      "130 106 5.017122268676758\n",
      "200 with loss of 501.7279529571533\n",
      "149 14 5.017096996307373\n",
      "130 118 5.017335414886475\n",
      "130 17 5.017248630523682\n",
      "40 24 5.017335414886475\n",
      "149 39 5.017374515533447\n",
      "130 8 5.0173540115356445\n",
      "130 11 5.017123699188232\n",
      "119 93 5.017368316650391\n",
      "149 61 5.017196178436279\n",
      "149 8 5.017330646514893\n",
      "130 13 5.0171356201171875\n",
      "149 53 5.017269611358643\n",
      "149 22 5.017236709594727\n",
      "149 113 5.017359256744385\n",
      "130 124 5.0172882080078125\n",
      "149 114 5.017355442047119\n",
      "149 21 5.017297267913818\n",
      "149 136 5.01734733581543\n",
      "119 119 5.016656398773193\n",
      "130 3 5.01737642288208\n",
      "130 47 5.017248153686523\n",
      "130 87 5.017366886138916\n",
      "149 2 5.01727819442749\n",
      "149 119 5.016919136047363\n",
      "149 2 5.01734733581543\n",
      "130 47 5.017345428466797\n",
      "130 43 5.017343044281006\n",
      "130 140 5.017385482788086\n",
      "149 85 5.0173845291137695\n",
      "119 21 5.017238616943359\n",
      "149 147 5.017312526702881\n",
      "119 132 5.017300605773926\n",
      "119 76 5.0170578956604\n",
      "149 8 5.017340183258057\n",
      "69 71 5.017381191253662\n",
      "149 131 5.0173468589782715\n",
      "149 141 5.017250061035156\n",
      "119 149 5.016833305358887\n",
      "149 74 5.017355918884277\n",
      "130 41 5.017360687255859\n",
      "130 120 5.0171027183532715\n",
      "149 40 5.017234802246094\n",
      "149 77 5.0171709060668945\n",
      "130 114 5.017357349395752\n",
      "130 123 5.017055988311768\n",
      "119 144 5.01737117767334\n",
      "130 16 5.017366886138916\n",
      "149 118 5.017359256744385\n",
      "130 45 5.017170429229736\n",
      "149 58 5.017186641693115\n",
      "119 65 5.017364025115967\n",
      "130 19 5.017354965209961\n",
      "130 121 5.01738166809082\n",
      "130 104 5.017157077789307\n",
      "130 21 5.01727294921875\n",
      "130 122 5.017330646514893\n",
      "130 29 5.017003059387207\n",
      "149 89 5.01724910736084\n",
      "149 93 5.017348766326904\n",
      "149 142 5.017362117767334\n",
      "130 75 5.017315864562988\n",
      "40 13 5.017104148864746\n",
      "130 24 5.017356872558594\n",
      "149 55 5.017277717590332\n",
      "149 35 5.017359256744385\n",
      "149 47 5.017336368560791\n",
      "149 54 5.017373085021973\n",
      "149 68 5.017391681671143\n",
      "149 113 5.017358779907227\n",
      "149 105 5.01719331741333\n",
      "149 148 5.017281532287598\n",
      "149 15 5.017230987548828\n",
      "130 12 5.017349720001221\n",
      "119 133 5.017252445220947\n",
      "119 144 5.0173187255859375\n",
      "130 67 5.017333030700684\n",
      "130 42 5.017259120941162\n",
      "130 17 5.01715612411499\n",
      "130 87 5.01729679107666\n",
      "149 60 5.017271518707275\n",
      "130 97 5.017289638519287\n",
      "119 145 5.017301082611084\n",
      "149 59 5.017338275909424\n",
      "149 6 5.017135143280029\n",
      "149 60 5.017295837402344\n",
      "130 27 5.017322063446045\n",
      "149 8 5.017373561859131\n",
      "149 114 5.017343521118164\n",
      "119 123 5.017275810241699\n",
      "149 127 5.017320156097412\n",
      "130 29 5.016973972320557\n",
      "119 149 5.016872406005859\n",
      "149 90 5.017344951629639\n",
      "119 106 5.017276287078857\n",
      "149 100 5.017358303070068\n",
      "40 119 5.016943454742432\n",
      "149 32 5.017143726348877\n",
      "149 66 5.017300128936768\n",
      "130 87 5.017298221588135\n",
      "130 10 5.017341136932373\n",
      "300 with loss of 501.72634744644165\n",
      "149 89 5.017359733581543\n",
      "130 17 5.017165660858154\n",
      "130 32 5.01719331741333\n",
      "149 134 5.017366409301758\n",
      "149 143 5.0173797607421875\n",
      "130 107 5.0173516273498535\n",
      "40 145 5.017289638519287\n",
      "119 144 5.017354488372803\n",
      "149 137 5.0173468589782715\n",
      "130 26 5.017366409301758\n",
      "119 62 5.017315864562988\n",
      "130 97 5.017273426055908\n",
      "29 118 5.0173563957214355\n",
      "130 21 5.0172810554504395\n",
      "130 87 5.0173749923706055\n",
      "149 32 5.017261505126953\n",
      "149 37 5.017348766326904\n",
      "149 110 5.017327308654785\n",
      "130 127 5.017362117767334\n",
      "119 95 5.017340183258057\n",
      "149 138 5.017301559448242\n",
      "130 118 5.017341613769531\n",
      "130 90 5.017341136932373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 67 5.017354965209961\n",
      "130 64 5.017355918884277\n",
      "130 146 5.017385005950928\n",
      "130 44 5.0173563957214355\n",
      "149 99 5.017118453979492\n",
      "149 94 5.017268657684326\n",
      "130 91 5.017299652099609\n",
      "149 138 5.017344951629639\n",
      "119 63 5.017353534698486\n",
      "149 150 5.017238616943359\n",
      "149 142 5.017364978790283\n",
      "149 108 5.0173258781433105\n",
      "149 133 5.017290115356445\n",
      "149 54 5.01737117767334\n",
      "149 30 5.017329216003418\n",
      "130 148 5.017288684844971\n",
      "119 21 5.0171799659729\n",
      "149 97 5.017327785491943\n",
      "130 87 5.017301082611084\n",
      "149 38 5.017280101776123\n",
      "149 100 5.017343997955322\n",
      "130 149 5.0168681144714355\n",
      "149 97 5.0173540115356445\n",
      "86 139 5.0172624588012695\n",
      "149 98 5.017330646514893\n",
      "86 46 5.017366886138916\n",
      "149 56 5.017344951629639\n",
      "130 44 5.017354488372803\n",
      "40 24 5.017378330230713\n",
      "130 43 5.017337322235107\n",
      "130 97 5.017347812652588\n",
      "149 60 5.0172438621521\n",
      "149 59 5.017389297485352\n",
      "149 92 5.0173659324646\n",
      "119 96 5.017264366149902\n",
      "149 83 5.017334938049316\n",
      "149 138 5.017347812652588\n",
      "149 27 5.017287731170654\n",
      "149 39 5.017369747161865\n",
      "40 24 5.017368316650391\n",
      "149 37 5.017356872558594\n",
      "149 38 5.017330646514893\n",
      "130 130 5.016829013824463\n",
      "149 34 5.017368793487549\n",
      "149 112 5.01737117767334\n",
      "102 30 5.017245292663574\n",
      "149 18 5.017257213592529\n",
      "130 63 5.017353057861328\n",
      "149 108 5.017348289489746\n",
      "149 93 5.017366886138916\n",
      "149 101 5.017307758331299\n",
      "149 115 5.017191410064697\n",
      "130 67 5.017331600189209\n",
      "130 2 5.017371654510498\n",
      "149 95 5.017342567443848\n",
      "40 62 5.0173211097717285\n",
      "119 140 5.017369747161865\n",
      "149 11 5.017152309417725\n",
      "130 131 5.017334461212158\n",
      "149 143 5.017375946044922\n",
      "149 139 5.01733922958374\n",
      "40 7 5.017185688018799\n",
      "149 34 5.017372131347656\n",
      "86 139 5.017334461212158\n",
      "149 146 5.01740026473999\n",
      "119 147 5.017378807067871\n",
      "149 146 5.017392635345459\n",
      "149 148 5.017292499542236\n",
      "130 42 5.017239093780518\n",
      "149 125 5.017336845397949\n",
      "149 121 5.017351150512695\n",
      "149 37 5.0173749923706055\n",
      "149 88 5.017348289489746\n",
      "102 41 5.0173492431640625\n",
      "109 75 5.01733922958374\n",
      "130 36 5.017149925231934\n",
      "29 13 5.016987323760986\n",
      "400 with loss of 501.73089361190796\n",
      "130 41 5.017375469207764\n",
      "130 56 5.017358303070068\n",
      "119 36 5.017162799835205\n",
      "149 86 5.016756057739258\n",
      "130 123 5.017155647277832\n",
      "40 26 5.017311096191406\n",
      "130 44 5.0173659324646\n",
      "149 94 5.017248153686523\n",
      "130 17 5.017206192016602\n",
      "149 142 5.017342567443848\n",
      "149 109 5.017112731933594\n",
      "130 26 5.017181396484375\n",
      "130 54 5.017372131347656\n",
      "149 56 5.017318248748779\n",
      "130 117 5.017391204833984\n",
      "149 129 5.017341136932373\n",
      "40 119 5.01699161529541\n",
      "149 133 5.017257213592529\n",
      "149 52 5.017213821411133\n",
      "149 46 5.01729154586792\n",
      "130 36 5.0172224044799805\n",
      "130 128 5.017189979553223\n",
      "86 105 5.017227649688721\n",
      "130 2 5.017329692840576\n",
      "130 12 5.01735258102417\n",
      "40 95 5.017352104187012\n",
      "119 64 5.017231464385986\n",
      "119 83 5.017270565032959\n",
      "149 19 5.017361164093018\n",
      "119 2 5.017331600189209\n",
      "119 95 5.017345905303955\n",
      "149 143 5.017387390136719\n",
      "149 130 5.01677942276001\n",
      "130 133 5.017343997955322\n",
      "149 147 5.017331600189209\n",
      "149 38 5.017358779907227\n",
      "86 25 5.017176151275635\n",
      "130 130 5.016848087310791\n",
      "119 144 5.017326354980469\n",
      "130 98 5.017339706420898\n",
      "130 132 5.017297267913818\n",
      "149 142 5.017373085021973\n",
      "130 52 5.017280101776123\n",
      "130 115 5.01728630065918\n",
      "149 58 5.0172200202941895\n",
      "130 136 5.017374515533447\n",
      "149 48 5.017350196838379\n",
      "130 136 5.017347812652588\n",
      "119 20 5.017048358917236\n",
      "40 24 5.017379283905029\n",
      "149 85 5.017395973205566\n",
      "130 19 5.017341136932373\n",
      "119 50 5.017354488372803\n",
      "149 82 5.0172553062438965\n",
      "149 147 5.0173258781433105\n",
      "149 37 5.017354488372803\n",
      "130 5 5.017358779907227\n",
      "119 148 5.017360210418701\n",
      "149 37 5.01735782623291\n",
      "130 116 5.017030715942383\n",
      "149 70 5.017369747161865\n",
      "149 116 5.017094135284424\n",
      "119 21 5.017248630523682\n",
      "149 74 5.017338752746582\n",
      "149 14 5.017090320587158\n",
      "149 137 5.017383098602295\n",
      "40 24 5.0173797607421875\n",
      "149 78 5.017347812652588\n",
      "130 14 5.017189979553223\n",
      "149 18 5.0173020362854\n",
      "130 21 5.0172200202941895\n",
      "130 67 5.0173659324646\n",
      "149 127 5.017336368560791\n",
      "119 62 5.017256736755371\n",
      "119 62 5.017271041870117\n",
      "130 57 5.017372131347656\n",
      "149 6 5.017182350158691\n",
      "149 120 5.017240524291992\n",
      "130 23 5.017329692840576\n",
      "130 43 5.017355918884277\n",
      "149 102 5.016968250274658\n",
      "119 14 5.017033576965332\n",
      "149 121 5.017353534698486\n",
      "149 138 5.017269611358643\n",
      "130 126 5.017233371734619\n",
      "149 37 5.01737642288208\n",
      "130 43 5.017319202423096\n",
      "149 55 5.017277717590332\n",
      "40 135 5.01737642288208\n",
      "14 85 5.0173869132995605\n",
      "130 1 5.017331600189209\n",
      "149 121 5.0173563957214355\n",
      "149 147 5.017299652099609\n",
      "119 122 5.017360210418701\n",
      "149 126 5.017329216003418\n",
      "130 114 5.017359733581543\n",
      "130 5 5.01737117767334\n",
      "130 118 5.017360210418701\n",
      "149 85 5.0173821449279785\n",
      "149 29 5.016956806182861\n",
      "500 with loss of 501.72732877731323\n",
      "149 76 5.017100811004639\n",
      "130 66 5.017252445220947\n",
      "130 123 5.017092704772949\n",
      "149 141 5.017232418060303\n",
      "130 118 5.01735782623291\n",
      "130 98 5.017337322235107\n",
      "130 23 5.017338275909424\n",
      "130 139 5.017381191253662\n",
      "149 106 5.017030715942383\n",
      "119 134 5.017366886138916\n",
      "130 13 5.01724910736084\n",
      "130 29 5.01693868637085\n",
      "130 143 5.017372131347656\n",
      "119 24 5.017355442047119\n",
      "149 54 5.0173540115356445\n",
      "119 126 5.017362117767334\n",
      "130 60 5.017318248748779\n",
      "149 40 5.016993522644043\n",
      "149 114 5.0173540115356445\n",
      "149 101 5.0172624588012695\n",
      "119 83 5.017302989959717\n",
      "130 62 5.017326354980469\n",
      "149 63 5.017343521118164\n",
      "119 132 5.017301082611084\n",
      "149 111 5.017207145690918\n",
      "130 44 5.017343997955322\n",
      "149 37 5.017364978790283\n",
      "149 37 5.017354488372803\n",
      "149 142 5.017384052276611\n",
      "119 50 5.0173468589782715\n",
      "130 123 5.017216682434082\n",
      "130 72 5.017351150512695\n",
      "149 111 5.017200469970703\n",
      "149 133 5.01731538772583\n",
      "130 142 5.017342567443848\n",
      "40 119 5.016908645629883\n",
      "130 125 5.017359256744385\n",
      "130 50 5.017332553863525\n",
      "149 114 5.017334938049316\n",
      "149 53 5.017308712005615\n",
      "149 14 5.017239093780518\n",
      "149 118 5.017379283905029\n",
      "130 26 5.017262935638428\n",
      "149 25 5.017305850982666\n",
      "149 67 5.0173563957214355\n",
      "149 67 5.017343997955322\n",
      "149 17 5.017248153686523\n",
      "149 122 5.017335891723633\n",
      "119 133 5.017361164093018\n",
      "119 149 5.017014980316162\n",
      "149 6 5.017176151275635\n",
      "69 72 5.017332077026367\n",
      "130 0 5.017348766326904\n",
      "130 50 5.017321586608887\n",
      "149 123 5.017194747924805\n",
      "130 6 5.017161846160889\n",
      "130 106 5.017164707183838\n",
      "130 69 5.017078399658203\n",
      "130 135 5.017367362976074\n",
      "149 97 5.017338752746582\n",
      "130 137 5.017367839813232\n",
      "130 129 5.017275810241699\n",
      "149 150 5.017152309417725\n",
      "149 81 5.017361164093018\n",
      "130 79 5.017354488372803\n",
      "130 68 5.017365455627441\n",
      "119 122 5.017308712005615\n",
      "149 64 5.017274379730225\n",
      "149 102 5.017080783843994\n",
      "130 109 5.016989707946777\n",
      "119 26 5.0173516273498535\n",
      "119 127 5.017343521118164\n",
      "130 58 5.017172813415527\n",
      "149 63 5.017362594604492\n",
      "149 52 5.017246246337891\n",
      "149 38 5.017332077026367\n",
      "149 80 5.017368316650391\n",
      "40 144 5.017259120941162\n",
      "149 47 5.0172271728515625\n",
      "149 86 5.016749382019043\n",
      "119 105 5.017240047454834\n",
      "69 67 5.017344951629639\n",
      "149 122 5.017339706420898\n",
      "111 41 5.017357349395752\n",
      "149 112 5.017365455627441\n",
      "130 44 5.0173211097717285\n",
      "130 22 5.017309188842773\n",
      "149 130 5.0169358253479\n",
      "130 42 5.017252445220947\n",
      "149 34 5.01735782623291\n",
      "130 84 5.017342567443848\n",
      "149 10 5.017381191253662\n",
      "149 45 5.017268657684326\n",
      "149 38 5.017348289489746\n",
      "149 21 5.01713752746582\n",
      "130 130 5.016761302947998\n",
      "130 56 5.017378807067871\n",
      "130 138 5.01732873916626\n",
      "149 105 5.017151832580566\n",
      "130 40 5.017143249511719\n",
      "600 with loss of 501.7263379096985\n",
      "149 141 5.017202377319336\n",
      "149 142 5.017374515533447\n",
      "149 96 5.017133712768555\n",
      "149 40 5.017072677612305\n",
      "130 52 5.0173726081848145\n",
      "149 80 5.0173492431640625\n",
      "149 86 5.016701698303223\n",
      "149 30 5.0172529220581055\n",
      "130 118 5.017357349395752\n",
      "149 110 5.017349720001221\n",
      "149 32 5.0172529220581055\n",
      "130 85 5.017365455627441\n",
      "149 30 5.017324447631836\n",
      "130 81 5.01733922958374\n",
      "149 121 5.017369270324707\n",
      "149 4 5.017189025878906\n",
      "149 58 5.017206192016602\n",
      "40 9 5.017258167266846\n",
      "130 114 5.017343997955322\n",
      "149 120 5.017229080200195\n",
      "119 45 5.017119884490967\n",
      "149 139 5.017327785491943\n",
      "130 115 5.017330169677734\n",
      "149 22 5.017333030700684\n",
      "149 18 5.01724910736084\n",
      "149 109 5.01704216003418\n",
      "109 29 5.017106533050537\n",
      "149 16 5.01737117767334\n",
      "130 73 5.017343997955322\n",
      "69 63 5.017373085021973\n",
      "130 70 5.017355442047119\n",
      "130 2 5.017329216003418\n",
      "130 121 5.017337322235107\n",
      "130 15 5.0172858238220215\n",
      "119 16 5.017364025115967\n",
      "149 113 5.017350196838379\n",
      "130 144 5.017036437988281\n",
      "119 23 5.017358303070068\n",
      "149 100 5.017261981964111\n",
      "149 60 5.017246246337891\n",
      "119 149 5.016826152801514\n",
      "149 58 5.017219543457031\n",
      "149 87 5.017287731170654\n",
      "40 79 5.017348289489746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 74 5.017373561859131\n",
      "130 135 5.017232418060303\n",
      "40 27 5.017275810241699\n",
      "149 37 5.0173540115356445\n",
      "149 39 5.01734733581543\n",
      "149 93 5.017337799072266\n",
      "149 112 5.017373561859131\n",
      "130 37 5.0173773765563965\n",
      "149 1 5.0173516273498535\n",
      "149 77 5.017151355743408\n",
      "149 85 5.017362117767334\n",
      "149 2 5.017329216003418\n",
      "149 56 5.01735258102417\n",
      "130"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2804/2791399821.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"A\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mneo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2804/1933967476.py\u001b[0m in \u001b[0;36mneo_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 )\n\u001b[1;32m    546\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn = \"A\"\n",
    "cnn = CNN()\n",
    "neo_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bc22c2d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x3456 and 2304x151)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2804/1991984311.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/1/0.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2804/2221767217.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x3456 and 2304x151)"
     ]
    }
   ],
   "source": [
    "cnn(transform(Image.open(\"test/1/0.jpg\").convert(\"RGB\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5a9425ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PDN: 1\n",
      "True: 0 Guess: 0\n",
      "True: 0 Guess: 0\n",
      "True: 0 Guess: 0\n",
      "True: 0 Guess: 0\n",
      "True: 0 Guess: 0\n",
      "For PDN: 1 got 5 out of 5 correct\n",
      "Testing PDN: 2\n",
      "True: 1 Guess: 0\n",
      "True: 1 Guess: 0\n",
      "True: 1 Guess: 0\n",
      "True: 1 Guess: 0\n",
      "True: 1 Guess: 0\n",
      "For PDN: 2 got 0 out of 5 correct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2804/2293414919.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(F.relu(x))\n",
      "/tmp/ipykernel_2804/204415514.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  res = F.softmax(cnn(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PDN: 3\n",
      "True: 2 Guess: 0\n",
      "True: 2 Guess: 0\n",
      "True: 2 Guess: 0\n",
      "True: 2 Guess: 0\n",
      "True: 2 Guess: 0\n",
      "True: 2 Guess: 0\n",
      "True: 2 Guess: 0\n",
      "For PDN: 3 got 0 out of 7 correct\n",
      "Testing PDN: 4\n",
      "True: 3 Guess: 0\n",
      "True: 3 Guess: 0\n",
      "True: 3 Guess: 0\n",
      "True: 3 Guess: 0\n",
      "True: 3 Guess: 0\n",
      "True: 3 Guess: 0\n",
      "For PDN: 4 got 0 out of 6 correct\n",
      "Testing PDN: 5\n",
      "True: 4 Guess: 0\n",
      "True: 4 Guess: 0\n",
      "True: 4 Guess: 0\n",
      "True: 4 Guess: 0\n",
      "True: 4 Guess: 0\n",
      "For PDN: 5 got 0 out of 5 correct\n",
      "Testing PDN: 6\n",
      "True: 5 Guess: 0\n",
      "True: 5 Guess: 0\n",
      "True: 5 Guess: 0\n",
      "True: 5 Guess: 0\n",
      "For PDN: 6 got 0 out of 4 correct\n",
      "Testing PDN: 7\n",
      "True: 6 Guess: 0\n",
      "True: 6 Guess: 0\n",
      "True: 6 Guess: 0\n",
      "True: 6 Guess: 0\n",
      "True: 6 Guess: 0\n",
      "True: 6 Guess: 0\n",
      "For PDN: 7 got 0 out of 6 correct\n",
      "Testing PDN: 8\n",
      "True: 7 Guess: 0\n",
      "True: 7 Guess: 0\n",
      "True: 7 Guess: 0\n",
      "True: 7 Guess: 0\n",
      "True: 7 Guess: 0\n",
      "For PDN: 8 got 0 out of 5 correct\n",
      "Testing PDN: 9\n",
      "True: 8 Guess: 0\n",
      "True: 8 Guess: 0\n",
      "True: 8 Guess: 0\n",
      "True: 8 Guess: 0\n",
      "True: 8 Guess: 0\n",
      "For PDN: 9 got 0 out of 5 correct\n",
      "Testing PDN: 10\n",
      "True: 9 Guess: 0\n",
      "True: 9 Guess: 0\n",
      "True: 9 Guess: 0\n",
      "True: 9 Guess: 0\n",
      "For PDN: 10 got 0 out of 4 correct\n",
      "Testing PDN: 11\n",
      "True: 10 Guess: 0\n",
      "True: 10 Guess: 0\n",
      "True: 10 Guess: 0\n",
      "True: 10 Guess: 0\n",
      "True: 10 Guess: 0\n",
      "True: 10 Guess: 0\n",
      "For PDN: 11 got 0 out of 6 correct\n",
      "Testing PDN: 12\n",
      "True: 11 Guess: 0\n",
      "True: 11 Guess: 0\n",
      "True: 11 Guess: 0\n",
      "For PDN: 12 got 0 out of 3 correct\n",
      "Testing PDN: 13\n",
      "True: 12 Guess: 0\n",
      "True: 12 Guess: 0\n",
      "True: 12 Guess: 0\n",
      "True: 12 Guess: 0\n",
      "True: 12 Guess: 0\n",
      "For PDN: 13 got 0 out of 5 correct\n",
      "Testing PDN: 14\n",
      "True: 13 Guess: 0\n",
      "True: 13 Guess: 0\n",
      "True: 13 Guess: 0\n",
      "True: 13 Guess: 0\n",
      "True: 13 Guess: 0\n",
      "For PDN: 14 got 0 out of 5 correct\n",
      "Testing PDN: 15\n",
      "True: 14 Guess: 0\n",
      "True: 14 Guess: 0\n",
      "True: 14 Guess: 0\n",
      "True: 14 Guess: 0\n",
      "For PDN: 15 got 0 out of 4 correct\n",
      "Testing PDN: 16\n",
      "True: 15 Guess: 0\n",
      "True: 15 Guess: 0\n",
      "True: 15 Guess: 0\n",
      "True: 15 Guess: 0\n",
      "True: 15 Guess: 0\n",
      "For PDN: 16 got 0 out of 5 correct\n",
      "Testing PDN: 17\n",
      "True: 16 Guess: 0\n",
      "True: 16 Guess: 0\n",
      "True: 16 Guess: 0\n",
      "True: 16 Guess: 0\n",
      "True: 16 Guess: 0\n",
      "For PDN: 17 got 0 out of 5 correct\n",
      "Testing PDN: 18\n",
      "True: 17 Guess: 0\n",
      "True: 17 Guess: 0\n",
      "True: 17 Guess: 0\n",
      "True: 17 Guess: 0\n",
      "True: 17 Guess: 0\n",
      "For PDN: 18 got 0 out of 5 correct\n",
      "Testing PDN: 19\n",
      "True: 18 Guess: 0\n",
      "True: 18 Guess: 0\n",
      "True: 18 Guess: 0\n",
      "True: 18 Guess: 0\n",
      "True: 18 Guess: 0\n",
      "For PDN: 19 got 0 out of 5 correct\n",
      "Testing PDN: 20\n",
      "True: 19 Guess: 0\n",
      "True: 19 Guess: 0\n",
      "True: 19 Guess: 0\n",
      "True: 19 Guess: 0\n",
      "True: 19 Guess: 0\n",
      "For PDN: 20 got 0 out of 5 correct\n",
      "Testing PDN: 21\n",
      "True: 20 Guess: 0\n",
      "True: 20 Guess: 0\n",
      "True: 20 Guess: 0\n",
      "True: 20 Guess: 0\n",
      "True: 20 Guess: 0\n",
      "True: 20 Guess: 0\n",
      "For PDN: 21 got 0 out of 6 correct\n",
      "Testing PDN: 22\n",
      "True: 21 Guess: 0\n",
      "True: 21 Guess: 0\n",
      "True: 21 Guess: 0\n",
      "True: 21 Guess: 0\n",
      "True: 21 Guess: 0\n",
      "For PDN: 22 got 0 out of 5 correct\n",
      "Testing PDN: 23\n",
      "True: 22 Guess: 0\n",
      "True: 22 Guess: 0\n",
      "True: 22 Guess: 0\n",
      "True: 22 Guess: 0\n",
      "For PDN: 23 got 0 out of 4 correct\n",
      "Testing PDN: 24\n",
      "True: 23 Guess: 0\n",
      "True: 23 Guess: 0\n",
      "True: 23 Guess: 0\n",
      "True: 23 Guess: 0\n",
      "True: 23 Guess: 0\n",
      "For PDN: 24 got 0 out of 5 correct\n",
      "Testing PDN: 25\n",
      "True: 24 Guess: 0\n",
      "True: 24 Guess: 0\n",
      "True: 24 Guess: 0\n",
      "True: 24 Guess: 0\n",
      "True: 24 Guess: 0\n",
      "True: 24 Guess: 0\n",
      "True: 24 Guess: 0\n",
      "For PDN: 25 got 0 out of 7 correct\n",
      "Testing PDN: 26\n",
      "True: 25 Guess: 0\n",
      "True: 25 Guess: 0\n",
      "True: 25 Guess: 0\n",
      "True: 25 Guess: 0\n",
      "True: 25 Guess: 0\n",
      "True: 25 Guess: 0\n",
      "For PDN: 26 got 0 out of 6 correct\n",
      "Testing PDN: 27\n",
      "True: 26 Guess: 0\n",
      "True: 26 Guess: 0\n",
      "True: 26 Guess: 0\n",
      "True: 26 Guess: 0\n",
      "True: 26 Guess: 0\n",
      "For PDN: 27 got 0 out of 5 correct\n",
      "Testing PDN: 28\n",
      "True: 27 Guess: 0\n",
      "True: 27 Guess: 0\n",
      "True: 27 Guess: 0\n",
      "True: 27 Guess: 0\n",
      "True: 27 Guess: 0\n",
      "For PDN: 28 got 0 out of 5 correct\n",
      "Testing PDN: 29\n",
      "True: 28 Guess: 0\n",
      "For PDN: 29 got 0 out of 1 correct\n",
      "Testing PDN: 30\n",
      "True: 29 Guess: 0\n",
      "True: 29 Guess: 0\n",
      "True: 29 Guess: 0\n",
      "True: 29 Guess: 0\n",
      "True: 29 Guess: 0\n",
      "For PDN: 30 got 0 out of 5 correct\n",
      "Testing PDN: 31\n",
      "True: 30 Guess: 0\n",
      "True: 30 Guess: 0\n",
      "True: 30 Guess: 0\n",
      "True: 30 Guess: 0\n",
      "True: 30 Guess: 0\n",
      "True: 30 Guess: 0\n",
      "For PDN: 31 got 0 out of 6 correct\n",
      "Testing PDN: 32\n",
      "True: 31 Guess: 0\n",
      "For PDN: 32 got 0 out of 1 correct\n",
      "Testing PDN: 33\n",
      "True: 32 Guess: 0\n",
      "True: 32 Guess: 0\n",
      "True: 32 Guess: 0\n",
      "True: 32 Guess: 0\n",
      "True: 32 Guess: 0\n",
      "True: 32 Guess: 0\n",
      "For PDN: 33 got 0 out of 6 correct\n",
      "Testing PDN: 34\n",
      "True: 33 Guess: 0\n",
      "For PDN: 34 got 0 out of 1 correct\n",
      "Testing PDN: 35\n",
      "True: 34 Guess: 0\n",
      "True: 34 Guess: 0\n",
      "True: 34 Guess: 0\n",
      "True: 34 Guess: 0\n",
      "True: 34 Guess: 0\n",
      "For PDN: 35 got 0 out of 5 correct\n",
      "Testing PDN: 36\n",
      "True: 35 Guess: 0\n",
      "True: 35 Guess: 0\n",
      "True: 35 Guess: 0\n",
      "True: 35 Guess: 0\n",
      "For PDN: 36 got 0 out of 4 correct\n",
      "Testing PDN: 37\n",
      "True: 36 Guess: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2804/756179265.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2804/204415514.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;31m# print(res)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"True: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Guess: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2804/2293414919.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 443\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79319c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
