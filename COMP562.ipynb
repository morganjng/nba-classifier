{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hLbjgX09Oa40"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ttn-Z0s0W0Ai"
   },
   "source": [
    "abbreviation key\n",
    "\n",
    "gp = games played\n",
    "\n",
    "net_rating = offRating - defRating\n",
    "\n",
    "offRating = 100*((points)/POSS)\n",
    "\n",
    "defRating = 100*((opp points/(opp POSS)))\n",
    "\n",
    "oreb_pct = offensive rebound percentage\n",
    "\n",
    "usg_pct = usage percentage is a measurement of the percentage of team plays utilized by a player while they are in the game\n",
    "\n",
    "ts_pct = true shooting percentage.  percentage of shots made factoring in threes and free throws.  \n",
    "\n",
    "ast_ptg = assist percentage.  Percent of field goals (2 or 3 point shots not including free throws) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g07GetHTUoXW",
    "outputId": "629c1d17-7b99-4952-d458-3bdbd622e7e5"
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/morganjng/nba-classifier/blob/main/all_seasons.csv\n",
    "csv = pd.read_csv(\"all_seasons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HggkbrXVPrQ",
    "outputId": "afc6e8b4-6abc-414f-ee33-ab53851e50a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'player_name', 'team_abbreviation', 'age',\n",
      "       'player_height', 'player_weight', 'college', 'country', 'draft_year',\n",
      "       'draft_round', 'draft_number', 'gp', 'pts', 'reb', 'ast', 'net_rating',\n",
      "       'oreb_pct', 'dreb_pct', 'usg_pct', 'ts_pct', 'ast_pct', 'season'],\n",
      "      dtype='object') 11700\n",
      "0\n",
      "Travis Knight\n",
      "LAL\n",
      "22.0\n",
      "213.36\n",
      "106.59412\n",
      "Connecticut\n",
      "USA\n",
      "1996\n",
      "1\n",
      "29\n",
      "71\n",
      "4.8\n",
      "4.5\n",
      "0.5\n",
      "6.2\n",
      "0.127\n",
      "0.182\n",
      "0.142\n",
      "0.536\n",
      "0.052\n",
      "1996-97\n"
     ]
    }
   ],
   "source": [
    "total_players = len(csv[\"player_name\"])\n",
    "print(csv.columns, total_players)\n",
    "for col in csv.columns:\n",
    "    print(csv[col][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcbTc_xTnL3M",
    "outputId": "f7da51dc-b84f-41e8-e61e-e61ae88881a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Connecticut', 'North Carolina-Wilmington', 'Iowa', 'Providence', 'None', 'Seward County Community College', 'Old Dominion', 'Temple', 'Georgia Tech', \"St. John's (NY)\", 'Washington State', 'Texas Tech', 'Seton Hall', 'American International', 'Massachusetts', 'Murray State', 'Drexel', 'Louisiana State', 'Long Beach State', 'New Mexico', 'Michigan', 'Pennsylvania', 'Michigan State', 'Missouri', 'Louisville', 'Georgetown', 'Louisiana Tech', 'Arkansas', 'Virginia', 'Cincinnati', 'Illinois', 'North Carolina State', 'Montevallo', 'Stetson', 'Wake Forest', 'West Florida', 'Notre Dame', 'UCLA', 'Kansas State', 'George Washington', 'North Carolina', 'Xavier', 'Wisconsin', 'Georgia Southern', 'San Diego State', 'Oklahoma', 'Memphis', 'Mt. San Antonio', 'Alabama', 'Arizona', 'Richmond', 'Syracuse', 'Gonzaga', 'Oklahoma State', 'Detroit Mercy', 'Arkansas-Little Rock', 'Maryland', 'McNeese State', 'Southern Mississippi', 'Purdue', 'Virginia Tech', 'Marquette', 'Ohio State', 'Georgia', 'La Salle', 'Jackson State', 'Coppin State', 'Montana', 'Nevada-Las Vegas', 'California', 'Texas', 'Iowa State', 'DePaul', 'Eastern Illinois', 'Villanova', 'Texas Christian', 'Central State (OH)', 'Kentucky', 'Minnesota', 'Wright State', 'Hartford', 'Florida', 'Oregon', 'Arizona State', 'Santa Clara', 'Tennessee', 'Auburn', 'Vanderbilt', 'Wisconsin-Stevens Point', 'Wyoming', 'Texas-El Paso', 'Utah', 'Missouri-Kansas City', 'Tulsa', 'Wichita State', 'Centenary (LA)', 'Southern California', 'Marist', 'Rice', 'Hampton', 'Washington', 'Miami (OH)', 'Kansas', 'New Mexico State', 'Weber State', 'Trinity Valley Community College', 'Brigham Young', 'Clemson', 'West Virginia Tech', 'California-Irvine', 'Mercer', 'Houston', 'Florida State', 'Central Arkansas', 'Longwood', 'Western Kentucky', 'Yale', 'Boise State', 'Southern Illinois', 'Duke', 'Pittsburgh', 'Virginia Union', 'Albany State (GA)', 'Cal State-Fullerton', 'Tennessee State', 'Eastern Michigan', 'Indiana', 'Baylor', 'Navy', 'Fayetteville State', 'Boston College', 'Central Michigan', 'Pfeiffer', 'Lamar', 'Stanford', 'Oregon State', 'Nevada-Reno', 'California-Santa Barbara', 'Augustana (SD)', 'Southern', 'Creighton', 'East Carolina', 'Tennessee-Chattanooga', 'Northwestern Oklahoma', 'Ohio', 'Western Carolina', 'Penn State', 'Oral Roberts', 'Jacksonville', 'South Carolina', \"St. Mary's (TX)\", 'Tulane', 'Grand Canyon', 'Bradley', 'New Orleans', 'Colorado', 'Northern Illinois', 'Pepperdine', 'Duquesne', 'Southeastern Oklahoma State', 'Pacific', 'Nebraska', 'Mississippi State', 'Delaware State', 'Auburn-Montgomery', 'Wisconsin-Green Bay', 'Morehouse', 'Morehead State', 'Davidson', 'Bowling Green', 'Colgate', 'College of Charleston', 'Austin Peay', 'South Carolina State', 'San Jose State', 'Cal State-Bakersfield', 'Central Connecticut State', 'Rhode Island', 'Long Island-Brooklyn', 'Nicholls State', 'Ball State', 'Valparaiso', 'Toledo', 'Liberty', 'Dayton', 'Florida A&M', 'Saint Louis', 'Colorado State', 'Miami (FL)', 'Thomas More', 'Fresno State', 'Augsburg', 'Central Oklahoma', 'North Carolina-Charlotte', 'George Mason', 'Texas State', 'Northwestern', 'Hawaii', 'South Florida', 'Barton Community College', \"Master's\", 'Louisiana-Monroe', 'Florida International', 'Lebanon Valley', 'Saint Rose', 'Butler Community College', 'Princeton', 'St. Bonaventure', 'Saint Vincent', 'Northern Arizona', 'Hofstra', 'Venezuela', 'Southern Methodist', 'Northwest Florida State', 'Blinn', 'Indian Hills Community College', 'Mississippi', 'Northeast Mississippi Community College', 'Wingate', 'Marshall', 'Fordham', 'Texas-San Antonio', 'Shaw', 'Western Michigan', 'Utah State', 'Idaho', 'North Dakota', 'Portland State', 'Central Florida', 'Nevada', 'Manhattan', 'Brigham Young-Hawaii', \"Saint Joseph's\", 'Kent State', 'Southeastern Illinois', 'William Paterson', 'Yonsei (KOR)', 'Fairfield', 'Texas A&M', 'Walsh', 'Utah Valley', 'Oakland', 'Louisiana-Lafayette', 'Alabama-Birmingham', 'Rutgers', 'Northeastern', 'Delta State', 'Eastern Washington', 'Missouri State', 'Meridian Community College', 'Midland', 'West Virginia', 'Rider', 'Indiana Purdue-Indianapolis', 'Virginia Military Institute', 'Robert Morris (IL)', \"Saint Mary's (CA)\", 'Northwestern State', 'Tennessee-Martin', 'Cleveland State', 'Virginia Commonwealth', 'Le Moyne', 'Portland', 'Harvard', 'Butler', 'Towson', 'Augusta State', 'Cal State-San Bernardino', 'Midwestern State', 'Siena', 'Jacksonville State', 'Cornell', 'Alabama A&M', 'Iona', 'Norfolk State', 'Tennessee Tech', 'South Dakota State', 'Bucknell', 'North Texas', 'Lehigh', 'California-Berkeley', 'Belmont', 'USC', 'Georgia State', 'UNLV', 'Westchester CC NY', 'Ohio U.', 'Miami (Fla.)', 'Stony Brook, N.Y.', 'Cal State-Long Beach', 'Molloy', 'University of California, Berkeley', \"St. John's, N.Y.\", 'Alabama Huntsville', 'Boston U.', 'Miami', 'University of Dayton', 'Louisana-Lafayette', ' ', 'California-Los Angeles', 'St. Louis', 'South Carolina Upstate', 'Va Commonwealth', 'Holy Cross', \"St. Mary's (CA)\", 'Loyola (IL)', 'Cal-Santa Barbara', 'American University', 'S.E. Missouri', 'University of Texas at Austin', 'Illinois State', 'Georgia Institute of Technology', 'University of Colorado Boulder', 'Lipscomb', 'Indiana-Purdue Indianapolis', 'Nebraska-Lincoln', 'Indiana-Purdue Fort Wayne', 'Texas-Austin', 'No College', \"St. John's\", 'California State-Long Beach', 'Radford', 'Florida Gulf Coast', 'Cal Poly', \"St. Joseph's (PA)\", 'Campbell University', \"St.Mary's College of California\", 'Texas-Arlington', 'Virginia Tech ', 'TCU', '                                   ', 'Truman State', 'Vermont', 'Montana State', 'Delaware', 'William & Mary'] ['USA', 'Jamaica', 'Serbia and Montenegro', 'Ukraine', 'Canada', 'Croatia', 'Lithuania', 'Nigeria', 'Congo', 'St. Vincent & Grenadines', 'US Virgin Islands', 'France', 'Slovenia', 'Dominican Republic', 'Germany', 'Georgia', 'New Zealand', 'Belize', 'England', 'Argentina', 'U.S. Virgin Islands', 'Greece', 'Senegal', 'China', 'Turkey', 'Finland', 'Mali', 'Puerto Rico', 'Mexico', 'Yugoslavia', 'Serbia', 'Spain', 'Venezuela', 'Haiti', 'Russia', 'Ireland', 'Brazil', 'Scotland', 'Poland', 'Netherlands', 'Czech Republic', 'Montenegro', 'United Kingdom', 'Democratic Republic of the Congo', 'Latvia', 'South Korea', 'Uruguay', 'Sudan (UK)', 'Australia', 'USSR', 'Italy', 'Switzerland', 'Gabon', 'Cameroon', 'Iran', 'Israel', 'Sweden', 'Tanzania', 'Panama', 'Bosnia', 'Great Britain', 'Macedonia', 'Bosnia & Herzegovina', 'Cabo Verde', 'Tunisia', 'South Sudan', 'Bahamas', 'Ghana', 'Austria', 'Bosnia and Herzegovina', 'Egypt', 'Japan', 'Trinidad and Tobago', 'DRC', 'Sudan', 'Angola', 'Saint Lucia', 'Republic of the Congo', 'Guinea'] ['1996', '1992', 'Undrafted', 'Undrafted', '1996', '1995', '1983', 'Undrafted', '1986', '1987', '1996', '1995', '1988', 'Undrafted', '1985', '1996', '1996', '1992', '1996', '1990', '1993', '1991', '1990', '1995', '1992', 'Undrafted', '1992', 'Undrafted', '1989', '1985', '1992', '1984', '1996', '1992', '1987', '1993', '1989', '1986', 'Undrafted', 'Undrafted', '1987', '1996', '1994', 'Undrafted', '1988', '1985', '1994', '1984', 'Undrafted', '1995', 'Undrafted', '1984', '1989', '1989', '1996', 'Undrafted', 'Undrafted', '1985', '1994', '1990', '1992', '1986', '1996', '1984', 'Undrafted', '1978', 'Undrafted', '1987', 'Undrafted', '1995', '1985', '1985', 'Undrafted', '1991', '1995', 'Undrafted', '1994', '1992', '1995', 'Undrafted', '1987', '1990', '1991', '1992', '1990', '1993', '1992', '1995', '1992', 'Undrafted', '1986', '1991', '1994', '1982', '1995', '1992', '1996', '1994', '1984', 'Undrafted', '1987', '1995', '1987', '1988', '1986', '1996', '1987', '1995', '1992', '1996', '1986', '1985', '1996', '1992', '1994', '1989', '1996', '1988', '1987', '1993', '1988', '1995', '1990', '1982', '1991', 'Undrafted', 'Undrafted', '1991', '1990', '1996', '1988', '1996', '1996', 'Undrafted', '1994', '1988', '1988', '1990', '1985', '1993', '1990', '1985', '1995', '1989', '1986', 'Undrafted', '1992', '1996', '1981', '1991', '1992', '1990', '1996', '1994', '1990', '1990', 'Undrafted', '1992', '1995', '1991', '1994', '1985', '1989', '1989', '1991', '1989', '1993', '1988', '1992', '1976', 'Undrafted', '1992', '1988', '1982', '1980', '1991', '1993', '1986', '1993', '1987', 'Undrafted', '1987', 'Undrafted', '1996', 'Undrafted', '1996', '1995', '1996', '1991', '1995', '1988', '1996', '1988', 'Undrafted', '1994', '1995', '1989', '1993', '1994', '1996', '1992', 'Undrafted', '1996', '1983', '1992', '1996', '1989', 'Undrafted', '1993', 'Undrafted', '1992', '1987', '1996', '1984', '1985', 'Undrafted', '1993', '1990', '1987', '1984', '1991', '1996', '1993', '1996', '1985', '1988', '1993', 'Undrafted', '1991', '1987', 'Undrafted', '1995', '1995', '1994', '1988', '1985', '1979', '1984', '1990', '1994', 'Undrafted', '1990', '1993', '1983', '1993', '1993', '1992', '1989', '1992', '1986', 'Undrafted', '1995', '1987', 'Undrafted', '1994', 'Undrafted', 'Undrafted', '1988', '1989', '1989', '1992', '1981', '1995', '1983', '1991', '1995', '1983', '1995', '1995', '1993', '1983', '1994', '1989', '1988', '1995', 'Undrafted', '1994', '1990', '1996', '1983', '1992', '1994', '1988', '1994', '1993', '1988', '1995', '1996', 'Undrafted', '1981', '1996', '1993', 'Undrafted', '1995', 'Undrafted', '1992', '1993', 'Undrafted', '1994', '1995', '1985', '1992', 'Undrafted', '1994', '1987', '1988', '1994', '1996', 'Undrafted', 'Undrafted', '1992', '1995', '1987', '1991', '1993', '1995', '1995', 'Undrafted', '1991', '1990', '1991', '1985', '1985', 'Undrafted', '1996', '1989', 'Undrafted', 'Undrafted', '1986', '1989', '1988', '1981', '1986', '1984', '1995', '1994', '1993', '1990', '1986', '1991', '1987', '1988', '1994', '1994', '1988', '1989', '1985', '1995', '1989', '1993', '1994', '1995', '1990', '1988', '1982', '1995', '1995', '1993', '1981', '1989', '1981', '1996', '1990', '1988', '1990', '1996', '1994', '1995', 'Undrafted', '1994', '1996', 'Undrafted', 'Undrafted', '1993', '1994', '1987', '1994', '1993', '1991', '1992', '1994', '1985', 'Undrafted', '1987', '1988', '1993', '1990', '1986', '1993', '1994', '1994', '1995', '1995', 'Undrafted', '1987', '1995', '1992', '1982', '1989', '1991', '1992', '1994', '1995', '1985', '1987', '1990', '1994', '1990', '1983', '1996', '1990', '1986', 'Undrafted', 'Undrafted', '1991', 'Undrafted', '1990', '1989', 'Undrafted', '1996', '1995', '1995', '1994', '1991', '1994', '1988', '1983', 'Undrafted', '1994', '1992', 'Undrafted', '1992', '1979', '1995', '1985', '1990', '1994', '1990', '1992', '1991', '1981', '1997', '1994', '1994', '1992', '1991', '1989', '1996', '1990', 'Undrafted', '1993', '1997', '1994', '1990', '1992', '1991', 'Undrafted', '1994', '1991', '1994', '1993', '1995', '1996', '1995', '1981', 'Undrafted', '1996', '1995', 'Undrafted', '1996', 'Undrafted', 'Undrafted', '1987', '1995', 'Undrafted', '1986', '1997', '1988', '1990', '1987', '1986', '1990', '1997', 'Undrafted', '1983', '1990', '1990', '1987', '1985', '1994', '1991', '1992', '1986', 'Undrafted', '1988', 'Undrafted', '1985', '1994', '1992', '1991', '1993', '1994', '1989', '1987', '1997', '1994', '1993', '1997', '1997', '1993', '1996', 'Undrafted', '1995', '1994', '1997', '1990', '1988', '1986', 'Undrafted', 'Undrafted', '1987', '1988', '1981', '1990', '1995', '1988', '1990', '1996', '1993', '1989', '1995', 'Undrafted', '1985', '1989', '1993', '1994', '1994', '1988', '1987', '1991', '1990', '1993', '1994', '1995', '1984', 'Undrafted', 'Undrafted', '1997', '1988', '1985', '1989', '1987', '1986', '1997', 'Undrafted', '1989', '1996', 'Undrafted', '1985', '1994', '1985', '1991', '1990', '1991', '1989', 'Undrafted', '1987', '1995', '1993', '1997', '1991', '1987', 'Undrafted', '1995', '1995', '1994', '1990', '1997', '1997', '1994', 'Undrafted', '1992', '1997', 'Undrafted', '1995', '1993', '1996', '1992', '1981', '1992', '1997', '1988', '1993', '1986', '1994', '1997', '1988', '1994', '1997', '1992', '1983', '1996', '1995', '1997', '1996', '1994', 'Undrafted', '1991', '1993', '1988', '1985', '1996', '1993', '1993', '1992', '1986', '1989', '1987', '1992', '1994', '1983', 'Undrafted', '1993', '1995', '1995', '1991', '1983', '1995', '1988', '1981', '1989', '1997', '1997', '1995', '1990', '1988', '1994', 'Undrafted', '1995', '1992', '1993', '1997', '1981', '1993', '1990', 'Undrafted', 'Undrafted', '1990', '1997', '1984', '1979', '1997', '1985', '1997', '1994', '1997', '1995', '1997', '1994', '1996', '1989', '1996', 'Undrafted', '1987', '1989', '1990', '1992', '1996', '1992', '1996', '1994', '1993', '1989', '1995', '1994', 'Undrafted', '1989', '1985', '1991', '1991', 'Undrafted', '1996', '1990', '1988', '1996', '1993', 'Undrafted', '1997', '1996', 'Undrafted', '1987', 'Undrafted', '1988', '1993', 'Undrafted', '1991', '1980', '1982', '1988', '1992', '1991', 'Undrafted', '1993', '1997', '1986', '1997', '1988', '1996', 'Undrafted', '1993', 'Undrafted', '1985', '1984', '1988', '1998', 'Undrafted', '1991', 'Undrafted', 'Undrafted', '1985', '1990', '1995', '1988', '1993', '1987', '1988', '1996', '1989', '1994', '1992', '1996', '1994', '1988', 'Undrafted', '1990', '1985', '1994', '1991', '1996', '1984', '1996', '1995', '1992', '1997', '1982', 'Undrafted', '1993', '1990', '1985', '1995', 'Undrafted', '1997', '1989', '1986', 'Undrafted', '1997', '1997', '1996', '1981', '1992', '1989', '1990', '1997', '1996', '1994', 'Undrafted', '1990', '1990', '1992', '1996', '1995', '1995', '1988', '1995', '1987', 'Undrafted', 'Undrafted', '1984', '1994', 'Undrafted', '1996', '1995', '1992', '1994', '1991', 'Undrafted', '1992', '1994', '1995', '1988', '1992', '1993', '1992', 'Undrafted', '1996', '1996', '1991', '1990', '1997', '1995', '1994', '1991', '1985', '1985', '1995', 'Undrafted', '1987', 'Undrafted', 'Undrafted', '1984', 'Undrafted', '1997', '1986', '1997', '1992', '1990', '1994', '1985', '1997', 'Undrafted', '1997', 'Undrafted', '1997', '1996', '1990', '1992', '1993', '1988', '1988', 'Undrafted', '1994', '1989', '1987', '1986', '1989', '1993', '1991', '1992', 'Undrafted', '1996', '1992', '1985', '1989', '1991', '1986', '1989', '1992', '1996', '1991', '1996', '1996', '1984', '1994', '1987', '1984', '1995', '1996', '1992', '1996', '1995', '1985', '1988', '1995', '1996', '1987', '1996', '1986', '1990', '1983', '1997', '1996', 'Undrafted', 'Undrafted', '1992', 'Undrafted', '1997', '1984', 'Undrafted', '1995', 'Undrafted', '1989', '1981', '1988', '1995', '1998', '1983', '1991', '1998', '1989', 'Undrafted', '1997', 'Undrafted', 'Undrafted', 'Undrafted', '1987', '1995', '1986', '1997', '1988', '1990', '1986', '1986', '1988', '1995', 'Undrafted', '1993', '1988', '1997', '1994', '1997', '1995', '1997', '1995', 'Undrafted', '1997', '1987', '1991', '1995', '1992', '1988', '1985', '1993', '1993', '1992', '1986', '1989', '1992', '1989', 'Undrafted', '1998', '1993', '1990', '1990', '1996', '1993', '1995', '1997', '1995', '1996', 'Undrafted', '1993', 'Undrafted', '1998', '1990', '1995', '1988', '1990', '1995', '1993', '1989', 'Undrafted', '1985', 'Undrafted', '1993', '1989', '1994', '1985', '1994', '1991', '1994', '1991', '1983', '1990', '1990', '1987', '1985', '1994', '1991', '1998', '1982', '1992', '1995', '1997', '1994', '1991', '1989', '1996', '1990', 'Undrafted', '1990', 'Undrafted', '1997', '1981', '1994', '1992', '1992', '1997', '1993', '1997', '1996', '1997', '1990', '1989', '1992', '1996', '1998', '1997', 'Undrafted', '1989', '1997', '1990', '1985', 'Undrafted', '1992', '1997', '1993', '1998', '1995', '1993', '1996', '1992', 'Undrafted', '1997', '1994', '1998', '1997', '1995', '1996', '1990', 'Undrafted', '1988', '1994', '1996', '1992', '1994', '1998', '1989', '1996', '1992', '1988', '1993', '1988', '1998', '1998', 'Undrafted', '1990', '1985', 'Undrafted', 'Undrafted', 'Undrafted', '1996', '1998', '1984', '1995', '1988', '1995', '1997', '1991', '1998', 'Undrafted', 'Undrafted', '1995', '1992', '1997', '1996', '1994', 'Undrafted', '1988', 'Undrafted', '1998', '1995', '1992', '1998', '1993', '1993', '1990', '1994', '1998', '1990', '1998', 'Undrafted', '1989', '1990', '1994', '1998', '1991', '1997', '1994', '1997', '1992', '1983', '1996', '1997', '1990', '1991', '1994', '1987', '1986', '1997', 'Undrafted', '1989', '1996', 'Undrafted', '1985', '1994', '1985', '1991', '1995', '1988', '1994', '1990', '1998', '1985', '1997', '1998', '1996', '1991', 'Undrafted', '1989', '1992', '1998', '1999', '1991', '1996', '1998', '1995', '1998', '1996', 'Undrafted', '1987', 'Undrafted', '1988', '1991', 'Undrafted', '1992', '1984', '1996', '1992', '1997', 'Undrafted', '1998', '1998', '1995', 'Undrafted', '1998', '1994', 'Undrafted', '1988', '1998', '1991', 'Undrafted', '1988', 'Undrafted', '1994', '1996', '1989', '1987', '1998', '1990', '1989', '1993', '1987', '1998', 'Undrafted', '1980', '1988', '1992', '1996', '1993', '1989', '1995', '1989', '1991', '1991', '1996', '1990', '1988', '1996', '1991', 'Undrafted', '1997', '1991', '1982', 'Undrafted', '1993', '1990', '1983', '1995', '1985', '1996', '1998', '1992', '1998', '1992', 'Undrafted', '1998', '1988', '1993', '1997', '1986', '1997', '1988', '1998', '1998', '1998', 'Undrafted', '1993', '1998', 'Undrafted', '1985', '1984', '1996', '1997', '1993', 'Undrafted', '1987', '1989', '1998', '1998', '1993', 'Undrafted', '1986', '1996', 'Undrafted', '1998', '1997', '1996', '1998', '1984', '1996', '1995', '1992', '1992', '1994', '1991', '1985', '1985', '1995', '1987', 'Undrafted', 'Undrafted', 'Undrafted', '1984', '1997', '1990', '1998', '1996', '1994', '1995', '1984', '1988', '1989', '1981', '1988', '1987', '1985', '1994', '1992', '1991', '1996', '1993', '1989', '1997', '1998', '1994', '1993', '1997', '1993', '1996', '1994', 'Undrafted', '1995', '1998', '1986', 'Undrafted', '1992', '1992', '1996', '1993', '1991', '1990', '1996', '1995', '1992', '1993', '1992', '1994', '1996', '1992', '1994', '1992', '1995', 'Undrafted', '1998', '1996', '1995', '1994', '1984', 'Undrafted', '1997', '1998', '1995', '1991', 'Undrafted', '1990', '1985', '1994', '1985', 'Undrafted', '1997', 'Undrafted', '1997', '1997', '1990', '1991', '1998', 'Undrafted', '1996', '1988', 'Undrafted', 'Undrafted', '1997', '1995', '1983', 'Undrafted', '1991', '1987', '1996', '1996', 'Undrafted', 'Undrafted', '1998', 'Undrafted', '1994', '1999', '1998', 'Undrafted', '1995', '1998', 'Undrafted', '1984', '1998', '1998', '1996', '1994', '1996', '1989', '1987', '1998', '1989', '1993', '1999', '1987', '1992', '1984', '1992', '1999', '1996', '1988', '1997', '1987', '1998', '1990', '1999', '1994', '1985', 'Undrafted', '1997', '1999', '1997', '1991', '1999', '1998', '1996', '1995', '1987', '1990', 'Undrafted', '1992', '1986', '1995', '1992', '1994', '1985', '1995', 'Undrafted', '1997', 'Undrafted', 'Undrafted', '1986', 'Undrafted', '1984', '1997', '1996', '1999', 'Undrafted', '1984', 'Undrafted', '1996', '1992', '1996', '1996', '1995', '1985', '1993', '1988', '1996', 'Undrafted', '1983', 'Undrafted', 'Undrafted', '1992', '1995', '1994', '1991', 'Undrafted', 'Undrafted', '1995', '1992', '1999', '1994', '1999', '1990', '1999', '1991', '1992', '1994', 'Undrafted', '1993', '1996', '1998', '1996', 'Undrafted', '1985', '1997', '1990', '1989', '1992', '1996', '1992', '1998', '1999', '1997', 'Undrafted', '1999', '1989', '1997', 'Undrafted', '1995', '1985', '1990', '1982', '1991', '1997', '1991', '1996', '1988', '1999', '1996', '1991', '1991', '1996', '1989', 'Undrafted', '1997', '1984', '1991', '1999', '1988', '1994', 'Undrafted', '1996', '1992', '1999', '1994', '1999', '1998', '1989', '1996', '1988', '1998', '1993', '1988', '1998', 'Undrafted', '1990', '1985', 'Undrafted', '1996', '1995', '1999', '1992', '1990', '1999', '1989', '1993', 'Undrafted', '1992', '1988', '1998', 'Undrafted', '1991', 'Undrafted', '1999', '1993', '1988', '1987', 'Undrafted', '1996', '1998', '1995', '1998', '1996', '1991', '1999', '1998', '1998', '1999', '1992', '1991', '1989', '1996', '1998', '1998', '1988', '1999', '1993', '1996', '1992', '1996', '1998', '1992', '1998', '1989', '1987', 'Undrafted', '1999', '1993', '1997', '1996', '1998', '1984', 'Undrafted', '1998', '1993', '1998', '1999', '1998', 'Undrafted', '1998', '1996', '1998', '1997', '1986', '1997', '1985', '1996', '1999', '1998', '1996', 'Undrafted', '1990', '1990', '1998', '1987', '1985', '1999', '1995', '1994', '1991', '1999', '1998', '1992', '1995', '1995', '1994', '1992', '1991', '1989', '1996', '1990', 'Undrafted', '1994', 'Undrafted', '1997', '1990', '1986', '1986', '1989', '1998', '1999', '1993', '1995', '1995', '1998', '1991', '1983', 'Undrafted', '1995', '1988', '1990', '1989', '1997', '1988', 'Undrafted', 'Undrafted', 'Undrafted', '1987', 'Undrafted', '1986', '1998', '1988', 'Undrafted', '1990', '1989', '1992', '1992', '1999', 'Undrafted', '1989', '1988', '1987', '1994', '1992', '1991', 'Undrafted', '1993', '1989', '1997', '1998', '1994', '1993', '1999', '1996', 'Undrafted', '1995', '1994', 'Undrafted', '1999', '1999', '1986', '1996', '1998', '1984', '1995', '1990', '1998', 'Undrafted', '1994', '1991', '1994', '1995', '1995', '1996', 'Undrafted', '1993', 'Undrafted', '1999', '1998', '1991', '1990', '1988', '1990', '1995', '1993', '1989', 'Undrafted', '1993', '1989', '1994', '1994', '1988', '1991', '1995', '1989', 'Undrafted', '1986', '1988', '1994', '1997', '1992', '1983', '1996', '1997', '1990', 'Undrafted', '1994', '1995', '1987', '1986', '1997', 'Undrafted', '1989', '1999', '1996', 'Undrafted', '1997', '1985', 'Undrafted', '1998', 'Undrafted', '1985', '1994', 'Undrafted', '1992', '1997', 'Undrafted', '1998', '1995', '1999', '1993', '1996', '1992', '1997', '1998', '1999', '1995', '1988', '1993', '1991', '1994', '1999', '1991', '1997', '1984', '1985', 'Undrafted', '1994', '1997', '1995', '1997', '1995', 'Undrafted', '1997', '1987', '1991', '1999', '1993', '1985', '1993', '1993', '1985', '1990', '1994', '1992', '1993', 'Undrafted', '1990', '1995', '1997', '1991', '1998', 'Undrafted', '1999', '1992', '1997', '1995', '1988', '1999', '1994', '1998', '1992', '1993', '1998', 'Undrafted', '1998', '1995', '2000', 'Undrafted', '2000', 'Undrafted', 'Undrafted', '1988', '1994', '1996', '2000', '2000', '1987', '1998', '1989', '1993', '1999', '1987', '1996', '1989', '1998', '1999', '1994', '1988', '1984', '2000', '1996', 'Undrafted', '2000', 'Undrafted', '1992', '1998', 'Undrafted', '1997', 'Undrafted', '1998', '1998', '1995', 'Undrafted', '1998', '2000', '1999', 'Undrafted', '1987', 'Undrafted', '1998', '1992', 'Undrafted', '1998', '1988', '1999', '1993', '1986', '1997', '1998', 'Undrafted', '1998', 'Undrafted', '1998', '1993', '1998', '1985', '1984', '1997', '1996', '1998', '1992', '1991', '1999', '1985', 'Undrafted', '1998', '1996', 'Undrafted', '1989', '1992', '2000', '1999', '1998', '1998', 'Undrafted', '1991', '1996', '1998', '1995', '1998', '1996', '1987', 'Undrafted', '1985', '1986', '1996', '1992', '2000', '1989', '1993', 'Undrafted', '1998', '1996', '1984', '1997', '2000', '1995', '1992', '1994', '1995', '2000', 'Undrafted', 'Undrafted', 'Undrafted', '1996', '1998', '2000', '2000', '1997', '1996', '1999', '1998', '1999', '1994', '2000', '2000', '1995', 'Undrafted', '1996', '1993', '1999', 'Undrafted', '1997', '1993', '2000', '2000', '1994', 'Undrafted', '1984', '1996', '1992', '1994', '1999', '1999', '1998', '1991', 'Undrafted', '1992', '2000', '1994', '1999', '1993', '1996', '1990', '1991', '1993', '1990', '1996', '2000', '1997', '1999', '2000', '1992', 'Undrafted', '1999', '1990', '1999', '1994', '1985', '1997', '1997', '1990', '1991', '1999', '2000', '1998', '1988', '1995', 'Undrafted', '1984', '2000', '2000', '1996', '1995', 'Undrafted', '1993', 'Undrafted', '1990', 'Undrafted', '1998', '1995', '1988', '1993', '1998', '1988', '1996', '1989', '1998', '1999', '1994', '1985', '1999', '1996', 'Undrafted', '1999', '1994', '1988', '1999', '1991', 'Undrafted', '1996', '1987', '1997', 'Undrafted', '1995', '1992', 'Undrafted', '1996', '1995', '1999', '1998', '1989', '1992', '1989', 'Undrafted', '1992', '1993', '1993', '2000', '1985', '1993', '2000', '1999', '1991', '1997', '1992', '1996', '1999', '1992', '1989', '1990', '1997', '1996', '1990', '1990', '1997', '1992', '1999', '1999', '1997', '1994', '1985', '1995', 'Undrafted', '1990', '1997', '1996', '1992', '1994', '1988', '1997', '1994', 'Undrafted', '1998', '1993', 'Undrafted', '1995', '1999', '1997', '1992', '1996', '1993', '1995', '1998', 'Undrafted', '1997', '1992', 'Undrafted', '1994', '2000', '1985', 'Undrafted', '1993', '1986', 'Undrafted', '1997', '1990', '1994', '1999', '1993', '1999', '1993', '1998', '1992', '1995', '1998', 'Undrafted', '1998', '1988', '1994', '2000', '1997', '1992', '1995', 'Undrafted', '1998', '1997', '1995', 'Undrafted', '1990', '1991', '1994', 'Undrafted', '1999', '1997', '1995', '1995', '2000', '1989', '1999', '1994', '1987', '1988', '2000', '2000', '1984', '1995', '1990', '1998', '1991', '1988', '1994', '1994', '1989', 'Undrafted', '1989', '1993', '1995', '1990', '1988', 'Undrafted', '1995', '1990', '1998', '1999', 'Undrafted', '1993', '1997', 'Undrafted', '1995', '1990', '1995', '1987', 'Undrafted', '1989', '1992', '1998', '1996', '1992', '1996', '1993', '1989', '1999', '1994', '1989', 'Undrafted', '2000', '1991', '1997', '1996', '1999', 'Undrafted', '1988', '1996', '1991', '2000', '1997', 'Undrafted', '1991', 'Undrafted', '1985', '1996', '1991', '1995', '1996', '1997', '2000', '1986', '1990', '1988', '2000', '1986', 'Undrafted', 'Undrafted', '1987', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', '2000', '1988', '1997', '1989', 'Undrafted', '1989', '2000', '1988', '2000', '1995', 'Undrafted', '2000', '1991', '2000', '1990', '1990', '1998', '1987', '1998', '1995', '1994', '1994', 'Undrafted', '1999', '1991', '1992', '1990', 'Undrafted', '1994', '2000', 'Undrafted', 'Undrafted', '2000', '1995', '2000', '1990', '1999', '1991', '1999', '1998', '1992', '1985', '2000', '1989', '1994', '1992', '1991', '1995', '1997', '1989', '1999', '1997', '1999', '1990', '1997', '1996', '1990', '2001', 'Undrafted', '1992', '1995', '1991', '1991', '2001', '1997', 'Undrafted', '2000', '1991', '2001', '1996', '1988', 'Undrafted', '1999', '1996', '1997', '1985', '1997', '1998', '1999', '2000', '1994', '2001', '1999', '2001', '1994', '1999', '1996', '1992', '1999', '1994', '1999', '1998', '2001', '1989', '1996', '1988', '1993', '1991', '2001', '1998', 'Undrafted', '1990', '2001', 'Undrafted', '2001', '1998', '1996', '1995', '1992', 'Undrafted', '1992', '1999', '1999', 'Undrafted', '1987', '1996', 'Undrafted', '1998', '1995', '1998', '1996', '1991', 'Undrafted', '1998', '1998', '2001', '1999', '2000', '1997', '1992', '1996', '1998', '2001', '1985', '1998', '1992', '1996', 'Undrafted', '2000', '1999', '2000', 'Undrafted', '1991', '1998', '1989', '1993', '1996', '1992', '2001', '1996', '1998', '1992', '1998', '1987', 'Undrafted', '1999', '1997', '2001', '1996', '1985', 'Undrafted', '1993', '1998', 'Undrafted', '1998', '2001', '1997', '2001', '1993', '1999', '1988', '1998', 'Undrafted', '1994', 'Undrafted', '1993', '1986', 'Undrafted', '1998', '2001', '1996', '1999', '1998', '1999', '2001', '1994', '2000', '2001', '2000', '1995', '2001', 'Undrafted', '1996', '1998', '2000', '1996', '2001', '1999', '1992', '1986', '1996', '1984', 'Undrafted', '2001', 'Undrafted', '2000', '1995', '2001', 'Undrafted', '1992', '2001', '1995', 'Undrafted', '1990', '1999', '2000', '1995', '1990', '1998', '1991', '1988', '1994', '1994', '1989', '2001', '2001', '1989', '1993', '1995', '1990', '1995', '1984', '2000', 'Undrafted', '2000', '2001', '2000', '1994', '2000', '2000', '1998', '1997', '1993', '2000', 'Undrafted', 'Undrafted', 'Undrafted', '1991', '1992', '1994', '1987', '1993', '1990', '1999', '1985', '1998', 'Undrafted', '2001', '1999', 'Undrafted', 'Undrafted', '1998', '1992', 'Undrafted', '2000', 'Undrafted', '1996', '2000', '1987', '1988', '1998', '1995', '1984', '1998', '1998', '2000', '1989', '1996', '1994', 'Undrafted', '1988', '1997', 'Undrafted', 'Undrafted', '2000', 'Undrafted', '1998', 'Undrafted', '1999', '2000', 'Undrafted', '1994', '2000', '1997', '2001', '2000', '1984', 'Undrafted', '1995', '1996', '1998', '2000', '1999', '2001', '1991', '1990', '1997', '1997', '2001', '1996', '1995', '2001', '1992', '2000', '1996', 'Undrafted', '1993', '1996', '2001', '1993', '1996', '1999', '1994', '2000', '1992', 'Undrafted', '1998', '1994', '1999', '1999', '1998', '1999', '2000', '1999', '1993', '1999', 'Undrafted', '1994', '1997', '1985', '1997', 'Undrafted', '1994', '1997', '1995', 'Undrafted', 'Undrafted', '1997', '1993', '1998', '1992', '1998', '1995', '1997', '2001', '1998', 'Undrafted', '2001', '2001', '1987', '1995', '1997', '2000', '1994', '2001', '1988', '1998', 'Undrafted', '1992', 'Undrafted', '1991', '1993', 'Undrafted', '1989', '1997', '1988', '2000', 'Undrafted', 'Undrafted', 'Undrafted', '1987', 'Undrafted', '2001', '2000', '1988', 'Undrafted', '1990', '1989', '1988', '2000', '2001', '1993', '1993', '1992', 'Undrafted', '1989', '1992', '1989', '2000', '1999', '1995', '2000', '1998', '1991', '2000', 'Undrafted', '1995', '1993', '1990', 'Undrafted', '1999', 'Undrafted', '2001', 'Undrafted', '1991', '1992', '1994', '1995', '2000', '1995', '1998', '1999', '1991', '1994', '1999', '2000', '2001', '2000', '1994', 'Undrafted', '1993', '2000', 'Undrafted', '1996', '2000', '1995', '1995', '1987', '1994', 'Undrafted', '1999', '1991', '1992', '1990', '2000', '2001', '1994', '1998', '1990', '1996', '1993', '1998', '1998', 'Undrafted', '1997', '1988', '1992', '1995', '1996', '1990', 'Undrafted', '1995', '2001', 'Undrafted', '1997', 'Undrafted', '1997', 'Undrafted', '1999', '1996', '1997', '2000', '1989', '2000', '1994', 'Undrafted', '1997', '1999', 'Undrafted', '1995', '1993', '1996', '1992', '2001', '2001', '1997', '1998', '1996', '1997', '2001', 'Undrafted', '2000', 'Undrafted', '2002', '1998', 'Undrafted', 'Undrafted', '2000', '1998', '1984', 'Undrafted', '1995', '1998', 'Undrafted', '2001', '1999', 'Undrafted', '2002', '2001', '1997', 'Undrafted', '1998', '1998', 'Undrafted', '1994', '1996', '2002', '2000', '2000', 'Undrafted', '1997', '1992', 'Undrafted', '1996', 'Undrafted', '1998', '2001', '1998', 'Undrafted', '1992', '1996', 'Undrafted', '2000', '2002', '1993', '2002', '1998', 'Undrafted', '2000', '2000', '2000', 'Undrafted', '2000', '2001', '2000', '1984', 'Undrafted', '1995', '1996', '1998', '2000', '1999', '2001', '1991', '1990', '2001', '1997', '1997', '2001', '1985', '2002', '1994', 'Undrafted', '1999', 'Undrafted', '2002', '1996', '1998', '1995', '1992', '1987', '1988', '1997', '2002', '2000', '1996', '1997', '1999', '2000', '1996', 'Undrafted', '1998', '1993', '1996', '2001', '2002', '1993', '1999', '1994', '2000', '1992', '1998', '1999', '2001', '1999', '1998', '1991', '1997', '2001', '1990', '1996', '1997', '1990', '1992', '1999', '2002', '1997', '1992', '1989', 'Undrafted', '1995', '2001', '2002', '1997', '2002', '2000', '1991', '2001', '1996', '1997', '1995', '1996', '1998', 'Undrafted', '1994', '2001', '2002', '1994', '1999', '1996', '1992', '1999', '1994', '1998', '2001', '1989', '1996', '2002', '1998', '1993', '2001', '1998', '1990', '2001', 'Undrafted', '2001', '1988', '1999', '1996', '1997', '1997', '2002', '2001', '1993', '1988', '1998', 'Undrafted', '1992', '2002', '1998', '1991', 'Undrafted', '2001', '1999', 'Undrafted', '1987', 'Undrafted', '1996', '2002', '1998', '1995', '1998', '1996', '2001', 'Undrafted', '1998', '1998', '1991', '2000', '2000', 'Undrafted', 'Undrafted', '1999', '1989', '1993', '1996', '1992', '2001', '1996', '1998', '1992', '1998', '2001', '1987', 'Undrafted', '1999', '1997', '1996', '1993', '2002', 'Undrafted', '1996', '1992', '2002', '1993', '2002', '1993', '2000', '2002', '1987', '1997', 'Undrafted', 'Undrafted', '1995', '1997', '1994', '1997', '1985', 'Undrafted', '2002', '2002', '2002', 'Undrafted', '1999', '1993', '1993', '1998', '1993', '1998', '2002', 'Undrafted', 'Undrafted', 'Undrafted', '2000', '1988', '1997', '1989', '2000', '2002', '2002', '2001', '1995', 'Undrafted', '2000', '1991', '2002', '1998', '2000', '1995', '1993', '1999', '1998', '1989', '1992', '1992', '1987', 'Undrafted', '1988', '1992', '1988', '1997', '1994', 'Undrafted', '1998', '1998', '1993', '1995', '1999', '1999', '2002', '2000', '1994', 'Undrafted', 'Undrafted', '1997', 'Undrafted', '1998', '1995', '1993', '1996', '2001', 'Undrafted', '1998', '1996', '1990', '2001', '1994', '2000', '1997', '1995', '2001', 'Undrafted', '2001', 'Undrafted', '2002', '1998', '2001', '1997', '1995', 'Undrafted', '1990', 'Undrafted', '1999', 'Undrafted', '1997', '1986', 'Undrafted', 'Undrafted', '1997', '1984', 'Undrafted', '2000', '2001', '2002', 'Undrafted', '2001', '1999', '2002', '1993', '2000', '2001', '2000', '1995', '1994', '2000', '1998', '1997', 'Undrafted', 'Undrafted', '2000', '1992', '1994', '1987', '2000', '2000', '2000', '2001', '1994', 'Undrafted', '1996', 'Undrafted', '2000', '1995', '2001', '2002', '1992', '2001', '1995', '1996', '2000', '1998', '1996', 'Undrafted', '1998', '2001', '1996', '1999', '2002', '1998', '1999', '2001', '1999', '2001', '1995', '1998', '2001', 'Undrafted', '1994', '2000', '2001', '2001', 'Undrafted', '2002', '1992', '1994', '1995', '2000', '1995', '1998', '1999', '1991', 'Undrafted', '1999', '2000', '1990', '1996', '1997', 'Undrafted', '2000', '1990', '1996', '1999', '1988', '1994', '1999', '1994', '1989', '2001', '2001', '1993', '1995', '1990', '1990', '2002', '2002', '2002', '1999', '2000', '1993', 'Undrafted', '1996', '1995', '1995', '1994', 'Undrafted', '1995', 'Undrafted', '1997', '2001', '1994', '1998', 'Undrafted', '1995', 'Undrafted', '1993', '2003', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', '2000', 'Undrafted', '2002', '1998', '2003', '2003', 'Undrafted', '1999', '2000', '1998', '1995', '1998', 'Undrafted', '2001', '1999', 'Undrafted', '2002', '2001', '1996', '1997', '2000', '2003', 'Undrafted', '1998', '1998', '2002', '1999', '2000', '2000', '1997', '1992', '1996', 'Undrafted', '1998', '1997', '2001', '1998', '1992', '1996', '2001', '1992', '1987', '2002', '1993', '2002', '1998', '1996', 'Undrafted', 'Undrafted', '1999', '2003', '2003', '1992', '1998', '1994', '1999', '2003', '2001', '1995', '1996', 'Undrafted', '2003', '2000', '1984', 'Undrafted', '1995', '1996', '1998', '1999', '1991', '2003', '1990', '1999', '1998', '1993', '2002', '2002', '2000', 'Undrafted', '2000', '1996', '2000', '1987', '1997', '2002', '2000', '1996', '2003', '1997', '1999', '2000', '1996', 'Undrafted', '2003', '2003', '2003', '1993', '1996', '2001', 'Undrafted', '1998', '1995', '1998', '1987', '2001', '1992', '1998', '1996', '2001', 'Undrafted', '1992', '1996', '1993', '1999', 'Undrafted', '2003', '2000', '1991', '1997', '1996', '2003', '1999', '1996', '2001', '1991', '2000', 'Undrafted', '2003', '1999', '2003', 'Undrafted', 'Undrafted', '1991', '1998', '2002', '1992', 'Undrafted', '1998', '1988', '1999', '1993', '2001', '2002', '1997', 'Undrafted', '2001', '1998', 'Undrafted', '1998', '2002', '1993', '1996', '2001', '1997', '2002', '2002', '1995', '1989', '2001', '1998', '1994', '1999', '1996', '1999', '1994', '2003', '2002', '2001', '2003', '2003', '1994', 'Undrafted', '2003', '1996', '1987', 'Undrafted', '2003', '1996', '2001', '2002', '1996', '1998', '1993', 'Undrafted', 'Undrafted', 'Undrafted', '1997', '1997', '1992', '1990', '1997', '1996', '1990', '2001', '1998', '1997', '1997', '1995', '2003', '2003', '1998', '2001', '2003', 'Undrafted', '2001', '1990', '1998', '2001', '1992', '1997', 'Undrafted', '2003', '2002', '1995', '1993', '1999', '1989', '1992', 'Undrafted', '1992', '2002', '1993', '1993', '2000', '2003', '2002', '1997', '2003', 'Undrafted', '1995', '1997', '1994', '1985', 'Undrafted', '2002', '1998', '2002', '2003', '1991', '1990', '1996', '1997', '2000', '2000', '2001', '2003', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', '2003', '2002', '2000', '1997', 'Undrafted', '1989', '2000', '2002', '2002', '2001', '1995', 'Undrafted', '2002', '2003', '2002', 'Undrafted', '1999', 'Undrafted', '1997', '1995', '1990', '1997', '1996', '1992', '1997', '1994', 'Undrafted', '1998', '1998', '1993', '1995', '1999', '1999', '2002', '1996', '1997', '2001', '1992', '1996', 'Undrafted', 'Undrafted', '1990', 'Undrafted', '1999', '1993', '1993', '1998', '1998', 'Undrafted', 'Undrafted', '1998', '2001', '1994', '2003', '1998', '2000', '1995', '2001', '2003', '2001', 'Undrafted', '2002', '2003', '1998', '2001', '1997', '1995', '1997', '1999', 'Undrafted', 'Undrafted', '1999', '1998', '1999', '2001', '1994', '2003', '2000', '2001', '2000', '2003', '2001', '2002', 'Undrafted', '2001', '1999', '2003', '2002', '1993', '2000', '2001', '2000', '1994', '2000', '1996', '2000', '2001', '1996', '2001', '1985', '2002', '2000', '1994', '1999', '2002', '2003', 'Undrafted', '1999', '1992', '1996', '2002', '2000', '1995', '2001', '2002', '1992', '1995', '1996', '2000', '1998', '2003', '1998', '1998', '1997', '2000', '1995', '1994', '1999', '1990', '2000', '2001', 'Undrafted', '1994', '2000', 'Undrafted', '2001', 'Undrafted', '2003', '2002', '1991', '1992', '1994', '2000', '1998', '1999', '1991', 'Undrafted', '1999', '1995', '1996', 'Undrafted', '1993', 'Undrafted', 'Undrafted', 'Undrafted', '1992', '1994', '1987', 'Undrafted', '2000', '1995', '1998', '1999', '2000', '1994', '2001', '2001', '1993', '1995', '1990', '2002', '1995', '2002', '1999', '2002', '2000', '1989', '2000', '1994', '2003', '1999', '2000', '1998', '1995', '1998', 'Undrafted', '2001', '1999', '2002', '2001', '1997', 'Undrafted', 'Undrafted', '1998', '2004', 'Undrafted', '2003', '2002', '2000', 'Undrafted', '2000', '1996', '2000', '2004', 'Undrafted', '2003', '2003', '1998', 'Undrafted', '1998', '2001', '1998', '2004', '1992', '1996', '2001', '1999', '2002', '1993', '2003', '2002', '2002', '2003', '1998', '2000', '1996', '2003', 'Undrafted', 'Undrafted', 'Undrafted', '2000', 'Undrafted', '2002', '1997', '2002', '2002', '2000', '2003', '2001', '1995', '2004', '1996', '2004', 'Undrafted', '2003', '2000', '1984', 'Undrafted', '2004', '1995', '1996', '2000', '1999', '1991', '2003', '1990', '1997', '1997', 'Undrafted', '2001', '2002', '2000', '1999', '2004', '1994', '1998', '1996', '2003', '1997', '1999', '2000', '1996', 'Undrafted', '2003', '2004', '2003', '2003', '2004', '2004', '1993', '1996', '2001', '2002', '2004', 'Undrafted', '1993', '1999', '2003', '2003', '1994', '1992', '1999', '1996', '2004', '1997', '1995', '1997', '1998', '2001', '1990', '1996', 'Undrafted', '1997', '2004', '1990', '1992', '1997', '1997', 'Undrafted', 'Undrafted', '1995', '2002', '2002', '2000', '1991', '2001', '1996', '1999', '2003', '1996', '2003', '1997', '1998', '2004', '2003', 'Undrafted', '1994', '2003', '2003', '2001', 'Undrafted', '2002', '2003', '1994', '1999', '1996', '1999', '1994', '2001', '1989', '1996', '1998', '1993', '2004', 'Undrafted', '2001', '1998', '2001', 'Undrafted', '2001', '1994', '1991', 'Undrafted', '2004', '1992', '1998', 'Undrafted', 'Undrafted', '2001', '1999', '1987', 'Undrafted', '2003', '1996', '2001', '2002', '1998', '1995', '1998', '1996', 'Undrafted', '1998', '2004', '1998', '2002', 'Undrafted', '2000', '2000', '1998', '2000', '1988', '1993', 'Undrafted', '1999', '1993', '2004', '1996', '1992', '2001', '1996', '2004', '1998', 'Undrafted', '1999', '1997', '2004', '2003', '2001', '1996', '1993', '2002', '1998', '1998', '2004', 'Undrafted', '1997', '2001', '1999', '2004', 'Undrafted', '1999', '1992', 'Undrafted', '1992', '2002', '1993', '2000', '2003', '2004', '2003', 'Undrafted', '1997', '1994', '2002', '2002', '2003', '2003', '2002', 'Undrafted', '1999', '1993', '1993', '1998', 'Undrafted', 'Undrafted', '1998', '1989', '2001', '1999', '2002', '2001', '2003', 'Undrafted', '2004', 'Undrafted', 'Undrafted', 'Undrafted', '2003', '2002', '2000', '1997', 'Undrafted', '2002', '2002', '2001', '1995', 'Undrafted', 'Undrafted', '1991', '2002', '2003', 'Undrafted', '1998', '2002', '1995', '1995', '2000', '1994', '2000', '1997', '1994', 'Undrafted', '1998', '1998', '1993', '2004', '1995', 'Undrafted', '1999', '1999', '2004', '2004', 'Undrafted', '2004', '2002', '1992', '1996', '1993', 'Undrafted', '1999', '1995', '2004', '1998', 'Undrafted', '1994', '2003', '1992', '2004', '1997', '1995', '2001', '2003', 'Undrafted', 'Undrafted', '2002', '2003', '1998', '2001', '1997', '1995', 'Undrafted', 'Undrafted', '2004', '2004', 'Undrafted', 'Undrafted', '2004', '1999', 'Undrafted', '1997', '1995', '1990', '1997', '1996', '1996', '2004', '1997', '2000', '2001', '2000', '2003', '2001', 'Undrafted', '2002', 'Undrafted', '2001', 'Undrafted', '1999', '2003', '2004', '2002', '2000', '2001', '2000', '1994', '2000', '2000', '1998', '1997', '2004', 'Undrafted', '2000', '2003', '2004', '1994', '1999', '2002', '2004', '2003', 'Undrafted', '2004', '1999', '1992', '1997', '2002', 'Undrafted', '2000', '1995', '2001', '2002', '1992', '1995', '1996', '2000', '1998', '1996', 'Undrafted', '1998', '1996', '1999', '1998', '2001', '2000', 'Undrafted', '1994', '2000', '2001', '1994', '2000', '2001', 'Undrafted', '2001', 'Undrafted', '2003', '2004', '2002', '1992', '2004', '1994', '2004', '2000', '1998', '1999', '1991', '2004', 'Undrafted', '1999', '2000', '1990', '1996', '1990', 'Undrafted', '1999', '1994', 'Undrafted', '2000', '2004', '1995', '1998', '1994', '1999', '1994', '2001', '2001', '1993', 'Undrafted', '1990', '2002', '1995', '2002', '1999', '2002', '2000', '1993', 'Undrafted', 'Undrafted', '1996', '1995', '1995', '2004', '1997', 'Undrafted', '2004', '1995', '2001', '2000', '1994', '2001', '2000', '1999', '2004', '1994', '1995', '1995', 'Undrafted', '1996', 'Undrafted', '2000', 'Undrafted', '2002', '1999', '2005', '2002', '1990', 'Undrafted', '2005', '2001', '1993', '2004', '1998', 'Undrafted', '2005', '2004', '2005', '2002', '1997', '1992', '1996', 'Undrafted', '2005', '1994', '2004', '1992', '2002', 'Undrafted', '2004', '2003', 'Undrafted', 'Undrafted', '2001', 'Undrafted', 'Undrafted', '1994', 'Undrafted', 'Undrafted', '1997', '2001', 'Undrafted', '1999', '1998', '2001', '2000', '2003', '1994', '2005', '2001', '1999', '1998', '1999', '1996', 'Undrafted', '2003', '1996', '1995', '1992', '2002', '2001', '1995', '1998', '2005', '2005', 'Undrafted', 'Undrafted', '2002', '1998', '2001', '2005', '2002', '1995', '2004', '2005', '2000', '1994', '2005', 'Undrafted', 'Undrafted', '2004', 'Undrafted', '2004', '1997', '2000', '2000', '1994', '2000', '2001', '2000', '2002', '2004', '2003', '1999', 'Undrafted', 'Undrafted', 'Undrafted', '1994', '2004', '1999', 'Undrafted', '2002', '1995', 'Undrafted', 'Undrafted', '1991', '2003', 'Undrafted', '1998', '1995', '1999', '1989', '2002', 'Undrafted', '2002', '1993', '2005', '2005', '2000', '2003', '2004', '2003', 'Undrafted', '1997', '2005', 'Undrafted', '2005', '1997', '2005', '1991', '2005', '2004', 'Undrafted', 'Undrafted', '1999', '2000', '2003', '2005', '1996', '1997', '2000', 'Undrafted', '2004', '2000', '2001', '2003', 'Undrafted', '2005', '2004', 'Undrafted', 'Undrafted', '2003', '2002', '2000', 'Undrafted', '1997', '2005', '2002', '2004', 'Undrafted', '2004', '1999', 'Undrafted', '1997', 'Undrafted', '1995', '1990', '1997', '2004', '2005', '1996', 'Undrafted', '1997', 'Undrafted', '1994', 'Undrafted', '1998', '1993', '2004', '2005', '2005', 'Undrafted', '1999', '2004', '1997', 'Undrafted', '1997', '2003', '2003', '2002', 'Undrafted', '1999', '1993', '2005', '1993', 'Undrafted', '1998', '2001', '1994', '2003', '2000', '1997', '1995', '2001', '2005', 'Undrafted', '2005', '2002', '2003', '1998', 'Undrafted', '2001', 'Undrafted', '1992', '2000', 'Undrafted', 'Undrafted', 'Undrafted', '1998', '1992', '2004', '2005', 'Undrafted', 'Undrafted', '2005', '2004', '1998', '1998', '2001', '2005', '1993', '1996', '2001', 'Undrafted', '2003', '2004', '1997', '1999', '1998', '2005', '2004', '2004', '2005', '1999', 'Undrafted', '2003', '1996', '1992', '2004', 'Undrafted', '1998', '2001', '1998', '2004', '1996', '2000', '2000', 'Undrafted', '2002', '1998', '2004', '1998', 'Undrafted', '2002', '1996', '2005', '1998', '1995', '1998', '2002', 'Undrafted', '1996', '2005', '1996', '2001', '1992', '1996', '2004', 'Undrafted', '2001', '1998', '2001', 'Undrafted', '2004', '1993', '1998', '1996', '2001', '2005', '1994', '1999', '1996', '2005', 'Undrafted', '2003', '2002', '2005', '2001', '2003', '2003', '1994', 'Undrafted', '2003', '1996', '2001', '2005', '2003', '1997', 'Undrafted', '2004', 'Undrafted', '1999', 'Undrafted', 'Undrafted', '2000', '1991', 'Undrafted', '1997', '1996', '2003', '1999', '1996', '2001', '2000', '2003', '2002', '2001', '1995', '1997', '1997', '1990', '2004', '1997', '1996', '2001', '2005', '1999', 'Undrafted', '1993', '1995', '2001', '2003', '1999', '1994', '1999', '1998', '2005', '2003', '2003', '2004', '1999', '1993', 'Undrafted', '2002', '2001', '2002', '2005', '2004', '2003', 'Undrafted', '2003', '2005', '1996', '2004', '2003', '2004', 'Undrafted', '2003', 'Undrafted', '2004', '2002', '2005', '1999', '2004', '1994', '2002', '2003', 'Undrafted', '1997', '1997', '2003', '1999', '2000', 'Undrafted', '1995', '2004', 'Undrafted', '2000', '2004', '2005', '1996', 'Undrafted', '2001', '2002', 'Undrafted', '1999', '2001', '1998', '1995', '1998', '2000', '1999', '2003', '2003', '1998', '1997', '2002', '2000', 'Undrafted', 'Undrafted', 'Undrafted', '2003', '2005', '1996', '2000', '2005', '1998', '2002', '2002', '2003', 'Undrafted', '2003', 'Undrafted', '2003', '2005', 'Undrafted', '1998', 'Undrafted', 'Undrafted', '2005', '2005', 'Undrafted', '2002', '2000', '1997', '2002', '2000', '1996', '2003', '1997', '1999', '1996', '2000', '2000', '2005', '1997', '2005', '1996', 'Undrafted', '1997', '1997', 'Undrafted', '1998', '1999', '2006', '2003', '2004', 'Undrafted', '2005', '1999', '2002', '1997', 'Undrafted', '2006', 'Undrafted', '1991', '2002', '2003', 'Undrafted', '1995', '1998', '1995', '1999', '1989', 'Undrafted', 'Undrafted', '2002', '1993', '2006', '2004', '2002', '2005', '2000', '2004', '2006', '2000', '2001', '2003', 'Undrafted', '2006', '2005', '2004', 'Undrafted', 'Undrafted', '2003', '2002', '2005', '1997', '2006', 'Undrafted', '2005', 'Undrafted', '2003', '1995', '2001', '2006', '2005', 'Undrafted', '2005', '2002', '2003', '1998', '2006', '1997', 'Undrafted', '2004', '2004', 'Undrafted', '2004', '1999', '1997', '1995', '2000', '2001', '2004', '2003', '1997', '2005', 'Undrafted', '2005', '2006', 'Undrafted', '2002', '2003', '2003', '2002', 'Undrafted', '1999', '2005', 'Undrafted', '1998', '2003', 'Undrafted', '2001', '2005', '1992', '1996', '2005', '2003', '2002', '2002', '1998', '2005', '2006', '2006', '2005', '2003', '2005', 'Undrafted', 'Undrafted', '2000', 'Undrafted', 'Undrafted', '2000', '2004', 'Undrafted', '1998', '1996', '2006', 'Undrafted', '2006', 'Undrafted', '1998', '2004', '1998', 'Undrafted', '2006', '2000', '2000', 'Undrafted', '1996', '1998', '2006', '2006', '2001', '2006', '2002', '2005', '1998', '2005', '2006', '2006', '2006', '1996', '2003', '1997', '1999', '1996', '1997', 'Undrafted', 'Undrafted', '2005', '2004', '2003', 'Undrafted', '2003', '2004', '2005', '1998', '2000', '2000', 'Undrafted', '2003', '1999', '2000', '1998', '1995', '1998', '1999', '2002', '2001', '1997', 'Undrafted', '2006', '1998', '2004', 'Undrafted', '2003', '2002', '2005', '2005', '2003', 'Undrafted', '1998', '1998', '2002', 'Undrafted', '2003', '2000', 'Undrafted', '2001', '2006', '1996', '2006', '1999', '1996', '1997', 'Undrafted', '2000', '2006', 'Undrafted', 'Undrafted', '2006', '2003', '1995', '1997', '1997', '1996', '1998', '2004', '2004', '2005', 'Undrafted', '2001', '2006', '1998', '2001', 'Undrafted', '2004', '2001', '2003', '2005', '1997', '2001', '1997', '2004', '2006', '1995', '1999', '2004', 'Undrafted', '2006', 'Undrafted', 'Undrafted', '2002', '2006', '1992', 'Undrafted', '2005', '1998', '1999', 'Undrafted', '2006', 'Undrafted', '2005', '1996', 'Undrafted', '2002', '2001', '2004', '1998', '2006', '1996', '1992', '2006', '2001', '1998', '2006', '2004', '2005', '1998', '1999', '1997', '2004', '2003', 'Undrafted', '2001', '1993', '2005', '2006', '2005', 'Undrafted', 'Undrafted', '1996', '1993', '2005', '1996', '1995', '1995', '1994', '2004', '1999', '2000', 'Undrafted', '2001', '2000', '2001', '2001', 'Undrafted', 'Undrafted', '2003', '2004', 'Undrafted', '1994', '2002', '2000', '2002', 'Undrafted', 'Undrafted', 'Undrafted', '2005', '2006', '2000', '2006', '2005', 'Undrafted', '1998', '1999', '2001', '2001', '2005', '1990', '2002', '2005', '1999', '1994', '1992', '2004', '1994', '2005', '1994', 'Undrafted', '2006', '1997', 'Undrafted', '1998', '2004', '1996', 'Undrafted', '2006', 'Undrafted', '1996', '1992', '1997', '2002', '2005', '2004', '1995', '2003', '2001', '2005', 'Undrafted', '1998', '1991', '2005', '2004', 'Undrafted', '1999', '2000', 'Undrafted', '2005', '1996', '1997', 'Undrafted', '2005', '2006', 'Undrafted', '2003', 'Undrafted', '2002', 'Undrafted', 'Undrafted', '2006', 'Undrafted', '1999', '2003', '1997', 'Undrafted', 'Undrafted', '2003', '1994', 'Undrafted', '2000', '2004', '1999', '2005', '2002', '2004', 'Undrafted', '2003', '2004', '2006', 'Undrafted', '1995', '2004', 'Undrafted', '2005', '2006', '2003', '2003', '2005', '1998', '1999', '2006', '2006', '2003', '2001', '1995', '2004', '1996', '2004', '2003', '2000', 'Undrafted', '1984', 'Undrafted', '2004', 'Undrafted', '2002', '2005', 'Undrafted', '2002', 'Undrafted', '2006', 'Undrafted', '1999', '2003', '2003', '2006', '2004', '2002', '2000', '2001', '2000', '1994', '2000', '2000', '1997', '2001', '2006', '2003', '2000', '1999', '2005', '2005', '2000', '1995', '2001', '1995', '1998', '1996', 'Undrafted', '1996', '1999', '1998', '1999', '2001', '2005', '1994', '2003', '2001', 'Undrafted', '2001', '2007', '2002', '1999', '2007', '1998', '1999', '2006', 'Undrafted', '1998', '1998', 'Undrafted', '2006', '2007', '2007', 'Undrafted', '2005', '1998', '1995', '1998', '2002', '1996', '2005', 'Undrafted', '2006', '1999', '2001', '2006', 'Undrafted', '1996', 'Undrafted', 'Undrafted', '2002', '2002', '2004', 'Undrafted', '2007', '2003', '2004', '2006', 'Undrafted', 'Undrafted', '1998', '2006', '2005', '2005', '2000', 'Undrafted', '1995', '2001', '2007', '1995', '1998', '2007', '1996', '2002', '1994', '1998', '2004', '1999', '2006', '2006', 'Undrafted', '2006', '2007', '2007', 'Undrafted', '1997', '1996', '2003', 'Undrafted', '1999', '2006', '2001', '2000', '2003', 'Undrafted', '2007', '2002', '2006', '2007', '1995', 'Undrafted', '1996', '1992', '1996', '2006', '2006', '2007', '2002', 'Undrafted', '2006', 'Undrafted', '2005', '2004', '1998', '2006', '1998', '1992', '2005', '2005', '1993', '2001', '2003', '2004', '1997', '1998', '2007', '2004', '2006', '2001', '2006', '2000', '2002', 'Undrafted', '2000', 'Undrafted', 'Undrafted', '2002', '2007', '1998', '2003', '2005', '1999', '2000', '1995', 'Undrafted', '1998', '2002', '2001', 'Undrafted', '2006', '1998', 'Undrafted', '2003', '2002', '2005', '2005', 'Undrafted', '1999', '2000', 'Undrafted', '2005', '1993', '2000', '2000', '1996', '1998', '2006', '2006', '2001', '2006', '1998', '1992', '2003', '1996', '2006', '2007', '2007', '2003', '2002', '2002', '1998', '2005', '2006', '2000', '2007', '2005', '2000', '1997', 'Undrafted', 'Undrafted', '1999', '2006', '2007', '2006', '2003', '2001', '1995', '2004', '2006', '1996', '1998', '2004', '2000', 'Undrafted', '2004', '1995', '2007', '2000', '1999', '2003', 'Undrafted', 'Undrafted', '2003', '2003', '2003', '2003', '2006', '2006', '2007', '2006', '2006', '1996', '2003', '2007', '2005', '1999', '1996', 'Undrafted', '1998', '2005', '2004', '2003', '2003', '2004', '2002', '2005', 'Undrafted', '1996', '2001', 'Undrafted', '1993', '2005', '1997', '1997', '2007', '2001', '2007', '2007', '2006', '1996', '1992', '2002', '2005', '2004', '2005', 'Undrafted', 'Undrafted', '2004', '2004', 'Undrafted', '2006', '1999', 'Undrafted', '2005', '2005', '2004', '1993', '1998', 'Undrafted', '1997', '1999', '2007', '1998', 'Undrafted', '2002', '2005', '2006', '2002', '2002', '1995', 'Undrafted', 'Undrafted', '2003', '2007', '2007', 'Undrafted', '1998', '2006', 'Undrafted', '1999', '2007', 'Undrafted', 'Undrafted', 'Undrafted', '2007', '2007', 'Undrafted', '2007', '1997', '1997', 'Undrafted', '1996', '1997', 'Undrafted', '2005', '1999', '2007', 'Undrafted', '2002', '2003', '2003', '2002', '2002', '2006', 'Undrafted', '2005', '2005', '1997', '2006', 'Undrafted', '2003', '2004', '2003', '2000', '2004', 'Undrafted', '2007', 'Undrafted', '1998', '2001', '2003', '1995', '2007', '1997', 'Undrafted', '1999', '2004', 'Undrafted', '2004', 'Undrafted', 'Undrafted', 'Undrafted', '1997', '2006', '2001', '1998', '2003', '2002', 'Undrafted', '2007', '2005', '2006', '2001', '1995', '1997', '2000', '2005', '2003', '2005', 'Undrafted', '2001', '2003', '2001', '2005', '2002', '2007', 'Undrafted', '1999', '2003', '2006', '2004', '2000', 'Undrafted', '2001', '2000', '2000', '1997', '2004', '2006', 'Undrafted', 'Undrafted', 'Undrafted', '2005', '2005', '2000', '2006', '2003', '2005', 'Undrafted', '2003', '2001', '2004', 'Undrafted', '2001', '1998', '2006', '2001', 'Undrafted', '2004', '1994', '1998', '2005', '1999', 'Undrafted', '2003', '2007', 'Undrafted', '2002', '2007', '2001', '2003', '1996', '2001', '2000', '2005', '2006', '2004', 'Undrafted', '2002', '2004', '1994', '2007', '1998', '1991', '2004', 'Undrafted', '2000', '2003', '2005', '1997', '2000', '2007', '2004', '2000', '2001', '2003', '2005', '2004', 'Undrafted', '2005', '1996', 'Undrafted', '1999', '1997', 'Undrafted', '1994', '1999', '2007', '2001', '2001', '2005', '2002', '2005', '1999', '2002', 'Undrafted', '2007', '1998', '1996', 'Undrafted', '2000', '1994', '2001', '2001', '1999', '2004', '1994', '1995', '2000', 'Undrafted', '2000', '1995', '2008', '1999', '2002', '2001', 'Undrafted', '2006', '1998', '2003', '2006', '2005', '2005', '2008', 'Undrafted', '2000', '2000', '1997', 'Undrafted', '2008', '2006', '1999', '2002', '2003', '2006', '2007', 'Undrafted', '2001', '2006', 'Undrafted', '2006', '2008', '2008', '2007', '2003', '2002', '2002', '1998', '1998', '2005', '2006', '2000', '2007', '2005', '2003', 'Undrafted', '2008', '2000', 'Undrafted', '2002', '2008', '2007', '1999', '2003', '2003', '2001', '1995', '2004', '2008', '1996', '2003', '2000', 'Undrafted', '2004', '2008', '2006', '1995', '2000', '1999', '2003', 'Undrafted', '2003', '2002', '1994', '2007', '2002', '2004', '2006', '2007', '1996', '2008', '2006', '2007', '2005', '2007', '1999', '1996', '2008', 'Undrafted', '1998', '2005', '2004', '2003', '2007', '2003', '2008', '2005', 'Undrafted', '1996', 'Undrafted', '1993', '2005', '2006', '2003', '2003', '1998', '2002', '1998', '2004', 'Undrafted', '2001', '1997', '2004', '1997', '1997', 'Undrafted', '1995', '2007', '2006', '2002', 'Undrafted', '1997', '2003', '2000', 'Undrafted', '2006', '1996', '2003', '1996', '1997', 'Undrafted', '2007', '2000', '2008', '2007', '2006', '2005', '2001', '2007', '1996', '2003', '2001', '2007', '2002', 'Undrafted', '2007', '2003', '2001', 'Undrafted', '2003', '2008', '1999', '2005', '2001', '1998', 'Undrafted', '2001', '2006', '1998', '2001', 'Undrafted', '2004', 'Undrafted', '2006', '2006', '1999', '2008', '2004', 'Undrafted', '1998', '2001', '1999', '2006', 'Undrafted', '2005', '1996', '2002', '2006', '1998', '1998', '2005', '2006', 'Undrafted', '2007', '2006', 'Undrafted', '1998', 'Undrafted', '2006', '2000', '1995', '2007', '2002', '2005', 'Undrafted', '1992', '2006', '2001', '2006', '2004', '2007', '2008', '2005', '1998', '2004', '2003', '2001', '2006', '2005', '1998', '2008', '2008', '2006', '2007', '2004', '2008', '2005', 'Undrafted', '2006', '1996', '2003', '2005', 'Undrafted', '2004', '2008', '2003', 'Undrafted', '2006', '1997', '2005', 'Undrafted', '2005', '2006', 'Undrafted', '2003', '2002', '2002', '2007', '1999', '2005', 'Undrafted', 'Undrafted', '2008', '1998', '2001', '2003', '2000', '2003', '1997', '2000', 'Undrafted', '2008', '2003', '2002', '2000', '2005', '2008', '2006', '2002', 'Undrafted', 'Undrafted', '2003', '2005', '2007', 'Undrafted', '2008', '1998', '2006', 'Undrafted', '2008', '1999', '2007', 'Undrafted', 'Undrafted', '2002', '2008', '2008', '1995', '2008', '2005', 'Undrafted', '1999', '2006', '1999', '2004', 'Undrafted', '2005', '2004', '2005', '2002', '2005', '1996', '2005', '2007', '2007', '2004', '2007', '1998', '1997', '2006', '2007', '2007', '2006', '2008', '2001', '2004', 'Undrafted', '2006', '2005', '2007', 'Undrafted', '2003', '2001', '1997', 'Undrafted', 'Undrafted', '2004', 'Undrafted', '1998', '2004', '1997', '2007', '1995', '1997', '2005', 'Undrafted', 'Undrafted', '2008', '1997', 'Undrafted', '1997', '1999', '2005', '2007', '2008', '2001', '2003', '2001', '2005', '2002', '2007', '2006', 'Undrafted', '1999', '2003', '2004', '2000', 'Undrafted', '2000', '2000', '1997', '2008', '2004', '2006', '2008', '2008', 'Undrafted', 'Undrafted', '2005', '2000', '2006', '2003', '2005', '2003', '2002', '2005', '2008', '2000', 'Undrafted', '1995', '2001', '2008', '2008', '2007', '1994', '2008', '1998', '2007', '1996', 'Undrafted', '2007', '1999', 'Undrafted', '2007', '2008', '1999', '2001', '1995', '2008', '2006', '2006', '2004', 'Undrafted', '2002', '2004', '1994', 'Undrafted', '2008', '2007', '1998', '2000', '2004', '2003', 'Undrafted', '2000', 'Undrafted', '2008', '2005', '1996', '2007', '2004', '2006', '2000', '2001', 'Undrafted', '1999', '2001', '1991', '2000', 'Undrafted', '2005', '2007', '1998', '1994', '2008', '2007', '2001', '2001', '2005', '2008', '2007', 'Undrafted', '2005', '1999', '2002', 'Undrafted', '2000', '1996', '2008', '2004', '1999', '2000', '2001', '2002', '2006', 'Undrafted', 'Undrafted', '2005', '1998', '2002', '1998', '1995', '1999', '1996', '2001', '2002', '2008', '2006', '2007', '2009', '2005', '2006', 'Undrafted', '2005', '2008', '2004', '1998', '2006', '2002', '2007', '2002', '2003', '2007', '2008', '2008', '2006', 'Undrafted', '2009', 'Undrafted', 'Undrafted', '2006', 'Undrafted', '2009', '2006', '2006', '1998', '1996', 'Undrafted', '2000', '2000', 'Undrafted', '1998', 'Undrafted', '2006', '2001', '2007', '2007', '2008', '1996', '2006', '2001', 'Undrafted', '2003', '2009', '2009', '2002', '2009', '2006', '2007', '2003', '1995', '1997', '2009', '2004', '1997', '2001', '1997', '2005', '2003', '2001', '2004', '1998', '1997', '2006', '1997', '2009', '2008', '1998', '2005', '2006', '2009', '1999', '2001', '2003', '2004', '1998', '2005', 'Undrafted', '2007', '2008', '2006', '2001', '2006', '1992', '2004', 'Undrafted', '1999', '2006', '2006', '2008', '2004', 'Undrafted', '2000', '2005', '2008', 'Undrafted', '2005', '2004', '2003', '2003', '2002', '2008', '2005', 'Undrafted', '1993', '1999', '2009', '2003', '2003', '1998', '1999', '2006', '2007', '2008', '2006', '2003', '2001', '1995', '2006', '2004', '2007', '2007', '2001', '2003', '2000', '1995', '2008', '1999', '2001', 'Undrafted', 'Undrafted', '2003', '2002', '2005', '2005', '2008', 'Undrafted', 'Undrafted', '2008', '2006', '2006', '2009', 'Undrafted', '2002', '1996', '2003', '2005', '2008', '2008', '2003', '2005', '2008', '2000', 'Undrafted', '1995', '2001', '2008', '2009', '2007', '2008', '1995', '2005', '2009', '2009', '2007', '2002', 'Undrafted', 'Undrafted', '2000', 'Undrafted', 'Undrafted', '2003', '2005', '2000', '1996', '1996', '2002', '2009', 'Undrafted', '2004', '2008', '1995', '2007', '2000', '1999', '2003', 'Undrafted', '2003', '2002', '2009', '1994', '2009', '2004', 'Undrafted', '2007', '2003', '2006', 'Undrafted', '2009', '2006', '2009', '1999', '2007', '2009', '2008', '2009', '2005', '2003', 'Undrafted', '2004', '2008', '2003', 'Undrafted', '1997', '2009', '2005', 'Undrafted', '2005', 'Undrafted', 'Undrafted', '2002', '2003', '2003', '2002', 'Undrafted', '2007', '2005', 'Undrafted', '2009', 'Undrafted', '2007', '2002', 'Undrafted', '2009', '2008', '2008', '2003', '2002', '2009', '2009', '2005', '2008', '2006', '2002', '2008', 'Undrafted', '2007', '2009', '2008', 'Undrafted', '2008', '2006', '2008', '1999', '2007', 'Undrafted', 'Undrafted', '2003', '1998', '2001', '2003', '2005', 'Undrafted', '1999', '2006', '1999', '2004', '2005', '2004', '2005', '2002', 'Undrafted', '2005', '1996', '2007', '2007', '2004', '2007', '1998', '2006', '2007', '2007', '2007', '2009', '1998', '2008', '2002', '2004', 'Undrafted', '2000', '2001', '2008', '2006', '2009', '2005', 'Undrafted', '2003', '2001', 'Undrafted', '2004', '1998', 'Undrafted', '1999', '2009', '2007', '1995', 'Undrafted', '2005', 'Undrafted', '2008', '1997', 'Undrafted', '1997', '2004', '2009', '2005', '2009', '2003', '2001', '2005', '2002', '2007', 'Undrafted', '2007', 'Undrafted', '1999', '2003', '2009', '2009', '2004', 'Undrafted', '2000', '2001', '2000', '2008', '2004', '2006', '2008', '2008', 'Undrafted', '2001', '2000', '2003', '1994', '2006', '2001', 'Undrafted', '1998', '2001', '2009', 'Undrafted', 'Undrafted', 'Undrafted', '2003', '2007', 'Undrafted', 'Undrafted', '2001', '2003', '1996', '2009', 'Undrafted', '1998', '2008', '1999', '2007', '2001', '2005', '2007', '2005', '2006', '2008', '2009', 'Undrafted', 'Undrafted', '2003', '2004', 'Undrafted', '2002', '2004', '2008', '2007', '1998', '2001', '2004', '1999', '2000', '2008', '2009', '2005', '1996', '2004', '2000', '2001', '2009', '2009', 'Undrafted', '2003', '2000', '2000', '2000', '2009', 'Undrafted', '2005', '2007', '1994', '2008', '2007', '2001', '2001', '2009', '2001', '2008', '2005', '1999', '2002', 'Undrafted', '2000', '2005', '1996', '2009', '2008', '2004', '1999', 'Undrafted', '2007', '2008', '2009', 'Undrafted', '2006', 'Undrafted', '2003', '2002', '2005', '2005', '2008', 'Undrafted', 'Undrafted', '2008', '2001', '2009', '1996', '2003', '2007', '2005', '2007', '1999', 'Undrafted', 'Undrafted', '2005', '2004', '2003', 'Undrafted', '2003', '2002', '2008', '2010', 'Undrafted', '2005', '2009', '2008', '2008', '2008', '2008', '2003', '2002', '2002', '1999', '1998', 'Undrafted', '2000', '2005', '2003', '2000', 'Undrafted', '2002', '2007', '1998', '2003', '2000', '2005', '2010', '2010', '2002', '1995', '2007', '2000', '2003', '2003', '1994', '2007', '2009', '2004', 'Undrafted', '2007', '2008', '2003', 'Undrafted', '2009', '2006', '2010', '2009', '2009', '2010', '2002', 'Undrafted', '2005', '2005', '2004', '2004', '2010', '2000', '2008', '2005', 'Undrafted', '2005', '2009', '2006', '2003', '2003', '2010', '2010', 'Undrafted', '2010', '2010', '1999', '2006', '2007', '2008', '2006', '2003', '2001', '1995', '2004', '2008', '1996', '2003', '2008', '2010', '2009', '2001', '2001', '1997', '2004', '2009', 'Undrafted', '1997', '2007', '1995', '2007', '2006', '2009', '1997', '2002', '2003', 'Undrafted', '2006', '1996', '2003', '1997', 'Undrafted', '2009', '2007', '2008', '2006', '2009', '2003', '2004', '2010', '1996', '2003', '2001', 'Undrafted', '2007', '2002', '2010', '2007', '2010', '2003', 'Undrafted', 'Undrafted', '2010', '2009', '2005', '2001', '1998', 'Undrafted', '2001', '2006', '2009', '2009', '2009', '2001', 'Undrafted', '2010', '2008', 'Undrafted', '2006', '2006', '2008', '2001', '1999', '2006', 'Undrafted', 'Undrafted', '2005', '1996', '2002', '1998', '2007', '2006', '2006', 'Undrafted', 'Undrafted', '2010', '2000', 'Undrafted', 'Undrafted', '1996', '1998', '2006', '2010', '2007', '2009', '2002', 'Undrafted', '1999', '2004', '1992', '2006', '2001', '2008', '2008', '2004', '1998', '2004', '2003', '2001', 'Undrafted', '2009', '2006', '2005', '2008', '2008', '2006', '2007', '2004', '2008', '2005', 'Undrafted', '2006', '2006', '2000', '2007', '1995', '2009', '2002', 'Undrafted', '2005', '2003', 'Undrafted', '2004', '2008', '2003', 'Undrafted', '1997', 'Undrafted', '2009', 'Undrafted', '2005', 'Undrafted', '2002', '2003', '2003', '2002', 'Undrafted', '2007', '2005', 'Undrafted', '2005', '2009', '2010', '1999', '2003', '2005', '2002', '2009', '2008', '2003', '2002', '2009', '2009', '2005', '2008', '2007', '2006', '2010', 'Undrafted', '2003', '2007', '2009', '2008', 'Undrafted', '2008', '2006', '2010', '2008', '2002', '2008', '1998', '2001', '1998', '2010', '2004', '2005', '2005', 'Undrafted', '1999', '2006', '1999', '2004', '2005', 'Undrafted', '2004', '2002', 'Undrafted', 'Undrafted', '2010', '2007', '2004', '2007', '1998', '2007', '2007', '2007', '2005', 'Undrafted', '1997', '2008', '2003', '2000', '2001', '2008', '2006', '2009', '2005', '2007', 'Undrafted', '2003', '2001', '2009', '2004', 'Undrafted', 'Undrafted', '2004', '1999', '2010', '2009', '2007', '2010', '1995', '1997', '2005', 'Undrafted', '2008', '2009', '2008', '2010', '2009', '2009', '2010', '2004', 'Undrafted', '2000', '2000', '2008', '2004', '2006', '2008', '2003', 'Undrafted', 'Undrafted', '2005', '2005', '2006', '2008', '2000', '2010', '2009', 'Undrafted', '2010', '2005', 'Undrafted', '1999', '2007', '2002', '2009', '2001', '2009', '2007', '2008', '1995', '1996', 'Undrafted', '2010', '2009', '2007', '1999', 'Undrafted', 'Undrafted', '1998', '2008', '1999', '2007', '2001', '2005', '1994', '2003', '2001', '2001', '2005', '2010', '2010', '2009', '2010', '2009', 'Undrafted', 'Undrafted', '2003', '2004', '2002', '2004', '2008', '2010', '2007', '1998', '2001', '2010', '2010', '2008', '2010', '2010', '2009', '2005', '2009', '2004', '2000', '2001', '1994', '2004', '2010', '1996', '2000', '2000', '2007', '2001', '2009', '2008', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', '2010', '2005', '2001', 'Undrafted', '2010', '1999', '1999', '2004', '2010', '2008', '2008', '1996', '2005', '2000', '2009', '2010', '1999', '2005', '2003', 'Undrafted', '2004', '2003', '2009', '2007', '1998', '2007', 'Undrafted', '2010', '2011', 'Undrafted', '2002', '2004', '2005', '2004', '2006', '2011', '2004', '2004', '2005', '2006', 'Undrafted', '2005', '2008', '2010', '2006', '2002', '2010', 'Undrafted', '2003', '2007', '2009', '2008', 'Undrafted', '2008', '2010', '2008', 'Undrafted', '2011', 'Undrafted', '1999', '2007', '2010', 'Undrafted', '2009', '2002', 'Undrafted', '2007', '2005', '2001', '2000', '2003', '2001', '1998', '2008', '2009', 'Undrafted', '2005', '2007', '2008', 'Undrafted', '2003', '2003', '2002', 'Undrafted', '2011', '2005', '2011', '2009', '2009', '2005', '2002', '2011', '2009', '2005', '2011', '2004', '1998', 'Undrafted', 'Undrafted', '1997', '2008', 'Undrafted', '2010', '2007', '2009', '2010', '1999', '2004', 'Undrafted', 'Undrafted', '2004', '2011', 'Undrafted', '2009', '2001', '2003', 'Undrafted', '2005', '2009', '2005', '2011', 'Undrafted', '2010', '2005', 'Undrafted', '2010', 'Undrafted', 'Undrafted', '2010', '1994', '2010', '2009', '2008', '2001', '2001', '2009', '2005', '2008', 'Undrafted', 'Undrafted', '2005', '1999', '2010', '2007', '2010', '2000', '2008', 'Undrafted', 'Undrafted', '2003', '2009', '2009', '2010', '2004', 'Undrafted', '2000', '2001', '2000', '2008', '2011', '2004', '2006', '2008', 'Undrafted', 'Undrafted', 'Undrafted', '2011', '2011', '2005', '2005', '1996', '2010', '2009', 'Undrafted', '2011', '2008', '2010', '2007', '2009', '2005', '1996', 'Undrafted', '2004', '2000', '2010', '2001', '2010', '2009', '2009', '2011', '2008', '2003', '2005', 'Undrafted', '2009', '2003', '2009', '2004', '2010', '1998', '2010', '2011', '2004', '1999', '2010', '2010', 'Undrafted', '2000', '2001', '2010', '2001', '2009', 'Undrafted', 'Undrafted', '2011', '2003', '2004', '2002', '2004', '2008', 'Undrafted', '2010', '2007', '2008', '1997', '2010', '2002', '1996', '2005', 'Undrafted', '2011', 'Undrafted', '2006', '1999', '2001', '2009', '2008', '2007', '2009', '2002', '2006', 'Undrafted', '2005', '2008', '2004', '2007', '2006', '2008', '2008', '2005', '2006', '2010', '2009', 'Undrafted', '2001', '2003', '2002', '1998', '2006', '2007', '2000', 'Undrafted', '2003', '2005', '2007', 'Undrafted', '2005', '1998', '2002', '2003', '2007', '2008', '2008', '2011', '2007', '2011', '2008', '2008', '2009', '2010', '2009', '2001', '2007', '2010', '2006', '1998', '2000', '2010', 'Undrafted', '2006', '2011', '2004', '2008', '2008', '2004', '2010', 'Undrafted', '2011', '2011', '2001', '2009', '2009', '2011', '2009', '2006', '2001', 'Undrafted', '2011', '2003', '1998', '2005', '1982', '2009', '2010', 'Undrafted', 'Undrafted', '2003', '2007', '2010', '2011', '2007', '2001', '2003', '2011', '2001', 'Undrafted', '2011', '2001', '2001', '2006', '2004', '1999', '2006', '2006', '2011', '2010', '2006', '2007', '2009', '1997', '2003', '1996', '1997', '2006', '2003', '2009', '2002', 'Undrafted', '2009', '2006', '2007', '2007', '1997', 'Undrafted', '2011', '2009', '2004', '1997', 'Undrafted', '2002', 'Undrafted', '1998', 'Undrafted', 'Undrafted', '2011', '1996', 'Undrafted', '2010', '1963', '1995', '2008', '2011', '2011', '2007', '2009', '2001', 'Undrafted', '2000', '2005', 'Undrafted', '2002', '2010', '2009', '2011', '2009', '2009', '2010', '2006', '2011', '2009', '2011', '2010', '2009', '1999', 'Undrafted', '2003', '2000', '2008', '1999', '2001', 'Undrafted', 'Undrafted', '2003', '2002', '2005', '2005', '2008', 'Undrafted', '2011', 'Undrafted', '2008', '2011', '1996', '2007', '2005', '2007', '2001', '2003', '1994', '2005', '2007', '1999', '2008', 'Undrafted', '2009', '2009', '2004', '2001', '2011', '2010', 'Undrafted', '2010', '1998', '2003', 'Undrafted', '2010', '2011', '2010', '2009', '2005', '2003', 'Undrafted', '2005', '1999', '2008', '2010', '2010', '2003', '2003', '2004', '2011', 'Undrafted', 'Undrafted', '1999', '2011', '2007', '2005', 'Undrafted', 'Undrafted', '2002', '2007', '2006', '2003', '2011', '2004', '2009', 'Undrafted', 'Undrafted', '2011', '1994', '2011', '2011', '2003', 'Undrafted', '2011', '2011', '2011', '2000', '2007', '1995', '2008', '2004', '2000', '2003', '2011', '1996', '2008', '2004', '1995', '2001', '2003', '2006', '2007', '2003', '2011', '2011', '2011', '1996', '2008', '2011', 'Undrafted', '2005', '1995', '2006', '2007', '2006', '2010', '2012', '2012', '2000', '2012', '1998', '2006', '2010', '2001', '2009', '2010', '2009', 'Undrafted', '2006', '2012', '2009', '2008', '2008', '1999', '2007', 'Undrafted', '2011', '2008', '2009', '2011', '1996', '2007', '2005', '2007', '2011', '2012', '1999', '2011', 'Undrafted', '2004', 'Undrafted', '2003', '2003', '2010', '2002', '2008', 'Undrafted', 'Undrafted', '2008', '2005', '2005', '2003', '2002', '1998', '2005', '2009', '2005', '2003', 'Undrafted', '2012', '2012', '2000', 'Undrafted', '2002', '2007', '2003', '2005', '2012', '2008', '2012', '2012', 'Undrafted', '2003', '2002', '2008', '1999', '2007', '2009', '2009', '2009', '2001', '2011', '2010', '2004', '2003', '2012', '2012', '2001', '2004', '2009', '2011', 'Undrafted', 'Undrafted', '1997', '2005', '2011', '2012', '2009', '2006', '2003', '2001', '2010', '2007', '2003', 'Undrafted', 'Undrafted', '2012', '2012', 'Undrafted', '2009', '2001', '1998', '2009', 'Undrafted', 'Undrafted', '2001', '2012', '2010', '2001', '2007', '2009', '2001', 'Undrafted', '2009', '2006', '2008', '2008', '2006', '2004', '2008', '2005', 'Undrafted', '2006', '2002', '2009', '2007', '2008', '2012', '2003', 'Undrafted', '2004', '2008', '2012', '2012', 'Undrafted', '2002', '2009', '2006', '1996', '2003', '2006', '1997', '2007', '2006', '2011', '1999', 'Undrafted', '2004', '2006', '2001', '2009', 'Undrafted', '1998', '2003', '2010', '2008', '2005', '2009', '2009', '2012', '2006', '2012', '2003', '2008', '2009', '2005', '2003', '2008', '2011', '2010', 'Undrafted', '2012', '2011', '2002', 'Undrafted', 'Undrafted', '2010', '2007', '1999', 'Undrafted', '2011', '2008', '2008', '2008', '2009', '2007', '2003', '2011', '2009', '2009', '2010', '2012', '2012', '2002', '2004', '2003', '2011', 'Undrafted', '2009', '2001', '2010', '2001', '2010', '1999', '2004', '2011', '2004', 'Undrafted', '2011', '2010', '2009', 'Undrafted', '2001', '2000', '1996', '2005', '2010', '2011', '2010', '2004', '2010', 'Undrafted', '2012', '1998', '2007', 'Undrafted', '2005', '2003', 'Undrafted', '1999', '2005', '2011', '2012', '2004', '1998', '2012', 'Undrafted', '2008', 'Undrafted', '2012', 'Undrafted', '2007', '2009', '2012', '2006', '2010', '1999', '2012', '2009', '2007', '2007', '1998', '2007', '2004', '2010', 'Undrafted', '2011', 'Undrafted', 'Undrafted', '2002', '2005', '2004', '2005', '2004', '2010', '2004', '2004', '2003', '2003', '2002', 'Undrafted', '2011', '2011', '2005', '2009', '1997', 'Undrafted', '2003', 'Undrafted', '2005', '2004', 'Undrafted', '2002', 'Undrafted', '2007', 'Undrafted', '2012', '2011', '2009', '2003', '2012', '2007', '2005', '2009', '2011', '2006', '2008', '2001', 'Undrafted', '2008', '2009', '2005', '2008', '2008', '2005', 'Undrafted', '2004', '2011', '2003', '2007', '2011', 'Undrafted', '2004', 'Undrafted', 'Undrafted', 'Undrafted', '1994', '2011', '2003', '2009', '2009', '2011', '2010', '2007', '2009', '2001', 'Undrafted', '2006', '2000', '2005', '2012', '2012', 'Undrafted', '2002', '2010', '2011', '2009', '2011', '2011', '2012', '2011', 'Undrafted', '2003', '2006', '2012', '2011', '2011', '2006', '1999', '2010', 'Undrafted', '2010', '2010', '2011', '2009', '2003', '2010', '2001', '2011', '1995', '2004', '2000', '2007', '1995', 'Undrafted', '2008', '2004', '2012', '2010', '2000', '2012', '2012', '2003', '2011', '1996', '2008', '2012', '2011', '2003', '1995', '2005', 'Undrafted', '2012', '2009', '2000', 'Undrafted', 'Undrafted', '2005', '2011', 'Undrafted', 'Undrafted', 'Undrafted', '2008', '2006', '2011', 'Undrafted', 'Undrafted', '2010', '2010', '2012', '2008', '2012', '2005', 'Undrafted', 'Undrafted', '2008', '2005', '2009', '2001', '2007', '2008', '2010', '1994', '2004', '2008', '2010', '2012', '2001', '2007', '1999', '2008', 'Undrafted', '2007', '2009', '2012', '2010', '2012', 'Undrafted', 'Undrafted', '2012', '2011', '1996', '2005', '1994', '2009', '2001', '2000', '2004', '2001', '2010', '2009', '2009', '2006', '2011', '2003', '2012', '2007', '2002', '2012', '2005', '2010', 'Undrafted', '2002', '2010', '1998', '2006', '2007', '2006', '2012', '1998', 'Undrafted', '2013', '2012', 'Undrafted', '2010', '2006', '1996', '2012', '2013', '2004', '2013', '2001', '2008', '2005', '2006', '2002', '2007', '2012', '2008', '2012', 'Undrafted', '2012', '2009', '2013', '2001', 'Undrafted', '2011', 'Undrafted', '2005', 'Undrafted', '2000', '2010', '2005', '2009', 'Undrafted', '2005', '2003', 'Undrafted', 'Undrafted', '2013', '2012', '2013', 'Undrafted', 'Undrafted', '2002', '2007', '2006', '2012', '2012', '2009', '1998', '2002', '2009', 'Undrafted', '2006', '2013', 'Undrafted', '2012', '2009', '2013', '2008', '2008', '2011', '2011', '2008', '2007', '2003', '2009', '2012', '2013', 'Undrafted', '2008', '2009', '2009', 'Undrafted', '2011', '2013', '2010', '2004', '2012', '2003', '2012', '2012', '2013', '2001', '2013', 'Undrafted', '2004', 'Undrafted', '2009', '2012', '2001', '2013', '2003', '2001', '2010', '2007', '2003', 'Undrafted', '2012', '2010', '2009', 'Undrafted', '1998', '2013', '2009', 'Undrafted', 'Undrafted', 'Undrafted', '2009', '2008', '2011', '2013', 'Undrafted', '2004', '2006', '2013', 'Undrafted', '2001', '2013', '1999', 'Undrafted', '2008', 'Undrafted', '2004', '2001', '2013', '2006', '2005', '2013', '2006', '2011', '2013', '1997', '2007', '2012', '2007', '2006', '2012', '2012', '2002', '2009', '2013', '2006', '1996', '2003', '1997', '2009', '2007', '2006', 'Undrafted', '2008', '2008', '1999', '2009', '2012', '2012', '2011', '2008', '2009', '2005', '2009', '2003', '2011', '2009', '2009', '2010', '2009', '2013', '1996', '2008', '2005', '2005', '2010', 'Undrafted', '2003', '2005', '2011', 'Undrafted', 'Undrafted', 'Undrafted', '2010', '2013', '2007', '2011', '2008', '2008', 'Undrafted', '2008', '2012', '2010', '2008', '2010', '2011', '2010', 'Undrafted', '2010', '1999', '2011', '2010', '2008', '2010', '2009', '2005', '2012', '2010', '2005', 'Undrafted', 'Undrafted', 'Undrafted', '2013', '2008', '2001', '2011', '2004', 'Undrafted', '2010', 'Undrafted', '2012', 'Undrafted', '1998', '2009', 'Undrafted', '2004', '2012', '2012', '2002', 'Undrafted', '2004', '2003', '2011', 'Undrafted', '2003', 'Undrafted', '2012', '2004', '1999', '2013', '2006', '1999', '2005', '2005', '2005', '2004', '1998', '2013', '2012', 'Undrafted', '2008', 'Undrafted', '2012', '2013', '2004', '2002', '2012', '2009', '2007', '2007', 'Undrafted', '1998', '2007', '2005', '2004', 'Undrafted', '2011', '2013', 'Undrafted', '2008', '2013', 'Undrafted', '2010', 'Undrafted', '2012', 'Undrafted', 'Undrafted', '2013', '2005', '2007', '2002', '2003', '2002', '2009', '2013', 'Undrafted', '2011', '2005', '2005', '2009', '1997', 'Undrafted', 'Undrafted', '2008', 'Undrafted', '2008', '2007', '2009', '2012', '2010', '2004', '2013', '2004', '2012', '2011', '2009', '2003', '2012', '2007', '2005', 'Undrafted', '2009', '2011', '2008', '2005', '2011', '2001', '2011', 'Undrafted', '2013', '2000', '2007', '1995', '2008', '2003', '2004', '2012', '2003', '2011', '1996', '2008', '2004', '2003', '2010', '2006', '2012', '2013', '2010', '2006', '2011', '2009', 'Undrafted', 'Undrafted', '2004', '2011', '2011', 'Undrafted', '2004', '2009', 'Undrafted', '2012', '2011', '2003', '2007', '2012', '2011', '2006', '2007', '2011', '2009', '2008', '2011', '2012', '2008', '2005', '2005', '2013', '2002', '2003', 'Undrafted', '2012', '2002', '2009', '2005', '2007', '2011', '1999', '2010', 'Undrafted', '2010', '2010', '2011', '2003', '2013', 'Undrafted', '2005', '2008', '2002', '2010', '2003', '2004', 'Undrafted', 'Undrafted', 'Undrafted', '2011', '2009', '2003', '2010', '2008', '2006', '2004', '2008', '2012', '2013', '2001', '2000', '2004', '2010', '2009', '2009', '2003', 'Undrafted', 'Undrafted', '2011', 'Undrafted', '2012', '2007', '2012', '2005', '2010', '2001', 'Undrafted', 'Undrafted', '2011', '2013', 'Undrafted', '2011', '2013', '2007', '2013', '2008', '2010', '2013', '2010', '2007', 'Undrafted', '2005', 'Undrafted', 'Undrafted', '2010', '2012', '2009', '2000', 'Undrafted', '2006', 'Undrafted', 'Undrafted', '2005', '2011', '2010', '2007', '1999', '2007', '2012', '2012', '2006', 'Undrafted', '2001', '2009', '2002', '2011', '2011', '2008', '2011', '1996', '2012', '2010', '2012', '2013', '2009', '2007', '2009', 'Undrafted', '2008', 'Undrafted', 'Undrafted', 'Undrafted', '2004', '2007', 'Undrafted', 'Undrafted', '2011', '2006', '2014', '2014', '2000', '2009', '2009', '2014', '2012', '2012', '2003', '2011', '1996', '2008', '2012', '2010', '2004', '2003', '2006', '2012', '2011', '2011', '2006', '2014', 'Undrafted', '2004', '1995', 'Undrafted', '2014', '2014', 'Undrafted', '2011', '2013', '2008', '2011', '2003', '2011', 'Undrafted', '2013', '2000', '2007', '2012', '2014', '2008', 'Undrafted', '2010', '2012', 'Undrafted', '2010', 'Undrafted', '2008', 'Undrafted', '2011', '2007', '2009', '2014', 'Undrafted', '2001', 'Undrafted', '2011', '2006', '2014', '2013', '2012', '2007', '2012', '2014', '2005', '2005', '2012', '2001', '1999', '2008', 'Undrafted', '2009', '2007', '2009', '2007', '2008', '2013', '2012', 'Undrafted', '2008', 'Undrafted', 'Undrafted', '2004', '2006', '2014', '2008', 'Undrafted', '2011', '2011', '2013', '2011', '2005', 'Undrafted', 'Undrafted', '2012', '2013', '2012', 'Undrafted', '2002', '2010', '2014', '2011', '2009', '2010', 'Undrafted', '2003', '2009', '2009', '2013', '2004', '2000', 'Undrafted', 'Undrafted', '2012', 'Undrafted', '2006', '2010', '2012', '2012', 'Undrafted', '2012', 'Undrafted', '1998', '2006', '2010', '2001', '2009', '2010', 'Undrafted', '2009', 'Undrafted', '2006', '2014', '2013', '2009', '2008', '2012', '2008', '2011', '2014', '2011', '2008', '2007', '2011', '2006', '2013', '2004', 'Undrafted', '2001', '2013', '2006', '2008', '2008', '2014', '2013', '2006', '2008', '2005', 'Undrafted', '2007', '2014', '2008', '2012', 'Undrafted', '2012', '2009', '2013', '2001', 'Undrafted', '2011', 'Undrafted', '2013', '2005', '2002', '2010', '2014', '2007', '2011', '2014', '2008', '2009', '2014', '2011', '2007', '2005', '2007', '1999', 'Undrafted', '2011', '2004', '2003', '2010', '2002', '2013', '2008', '2005', 'Undrafted', '2013', '2009', '2003', '2003', '2011', '2010', 'Undrafted', '2008', '2008', '2005', '2013', '2014', '2003', '2009', '2013', '2002', '1998', '2013', '2005', '2005', '2003', '2014', 'Undrafted', '2010', 'Undrafted', '2012', '2012', '2013', '2000', '2002', '2007', '2012', '2013', '2008', '2012', 'Undrafted', '2003', '2002', '2005', '2012', '2008', 'Undrafted', 'Undrafted', '2008', 'Undrafted', '2013', 'Undrafted', '2011', 'Undrafted', '2010', '2004', '2007', '2014', '2014', '2002', '2007', 'Undrafted', '2003', '2001', '2014', '2010', '2007', '2003', 'Undrafted', 'Undrafted', '2012', 'Undrafted', '2010', '2009', '2005', '2004', 'Undrafted', '2011', '2009', '2011', '2012', '2004', '2013', '2004', '2010', '2012', '2009', '2007', 'Undrafted', 'Undrafted', '2013', 'Undrafted', 'Undrafted', '2012', '2013', '2014', '2012', '2005', '1999', '2006', '2013', '1999', '2004', '2012', '2009', '1998', '2013', '2009', '2012', '2012', '2002', 'Undrafted', '2009', '2014', '2013', '2006', '2003', '2009', '2007', '2014', '2013', 'Undrafted', '2011', 'Undrafted', '2006', '1999', 'Undrafted', '2004', '2006', '2013', '2014', '2013', 'Undrafted', '2013', '2008', '2006', '2003', '2007', '2007', 'Undrafted', 'Undrafted', '2001', '2006', '2009', '2012', 'Undrafted', '2009', '2014', '2009', 'Undrafted', '2011', '2013', '2010', '2004', 'Undrafted', 'Undrafted', '2012', '2013', '2001', '2004', '2009', '2011', 'Undrafted', '2013', 'Undrafted', '1997', '2012', '2012', '2007', '2005', '2010', 'Undrafted', '2014', '2010', '2010', '2009', 'Undrafted', '2011', '2003', '2014', '2004', 'Undrafted', 'Undrafted', '2002', '2012', '2014', '2004', '2011', 'Undrafted', '1998', '2012', 'Undrafted', '2014', '2004', '2011', '2008', '2010', '1999', '2005', '2011', '2008', 'Undrafted', '2010', '2013', '2013', '2010', '2008', '2014', '2013', '2007', 'Undrafted', '2013', '2001', '2009', '2005', '2008', 'Undrafted', '2014', 'Undrafted', 'Undrafted', '2012', '2005', '2012', '2010', '2012', '2005', '2013', 'Undrafted', '2010', '2010', '2013', '2010', '2008', 'Undrafted', '2003', 'Undrafted', '2009', '2005', '2005', '2011', 'Undrafted', '2002', '2003', '2002', '2007', '2014', '2014', '2005', '2013', 'Undrafted', 'Undrafted', '2014', '2008', 'Undrafted', '2001', '2008', '2011', '2009', 'Undrafted', 'Undrafted', '2009', '2003', 'Undrafted', '2009', '2009', '2008', '2003', 'Undrafted', 'Undrafted', '2005', '2009', '2008', '2011', '2012', '2014', '2009', '2009', '2005', 'Undrafted', '2012', '2003', '2008', '2008', '2011', '2014', '2007', '2010', '2013', '2014', '2014', '2005', '2013', '2005', '2001', '2015', 'Undrafted', '2014', 'Undrafted', '2008', '2005', '2009', '2013', '2014', '2008', '2010', '2013', '2010', 'Undrafted', '2010', '2012', '2010', 'Undrafted', 'Undrafted', 'Undrafted', '2005', '2011', '2013', '2011', 'Undrafted', '2012', '2010', '2012', '2005', '2014', 'Undrafted', '2012', '1998', 'Undrafted', '2011', '2014', '2012', '2002', 'Undrafted', '2004', '2014', 'Undrafted', '2003', '2010', '2014', '2014', 'Undrafted', '2010', '1999', '2015', '2011', '2010', '2008', 'Undrafted', '2013', '2011', '2008', 'Undrafted', '2006', '2010', '2012', 'Undrafted', '2015', '2008', '2011', '2011', '2007', '2009', '2014', 'Undrafted', '2001', '2014', '2015', '2013', '2012', '2012', '2010', '2014', '2011', '2009', '2011', 'Undrafted', '2014', '2014', '2006', 'Undrafted', '2004', '2013', '2007', '2004', 'Undrafted', 'Undrafted', '2008', '2014', '2012', '2015', '2000', '2004', '2010', '2013', '2009', '2009', '2009', 'Undrafted', '2014', '2012', '2007', '2012', 'Undrafted', '2015', '2014', '2007', '1999', '2008', '2009', '2003', '2011', '2011', '2010', '2012', 'Undrafted', 'Undrafted', '2013', 'Undrafted', '2007', '2012', '2010', 'Undrafted', '2013', '2004', '2011', '2009', 'Undrafted', '2015', '2011', '2003', '2012', '2007', '2015', '2005', '2009', '2011', '2008', 'Undrafted', '2015', '2013', '2014', '2012', '2003', '2007', '2014', 'Undrafted', '2014', '2007', '2004', '2010', 'Undrafted', 'Undrafted', '2011', '2013', 'Undrafted', 'Undrafted', '2013', 'Undrafted', '2002', '2005', '2004', '2012', '2004', '1999', '2013', '2006', '2005', '2011', '2008', '2008', '2014', 'Undrafted', '2011', 'Undrafted', '2008', 'Undrafted', '2015', '2008', '2003', '2012', '2014', 'Undrafted', '2008', '2009', '2014', '2009', '2009', '2015', '2005', '2003', '2008', '2009', '2009', '2010', '2009', '2015', '2013', '2005', '2008', '2008', '2007', '2013', 'Undrafted', 'Undrafted', '2013', '2005', '2014', '2014', '2015', '2007', '2003', '2002', '2011', '2005', '2010', '2005', 'Undrafted', '2003', 'Undrafted', 'Undrafted', '2003', '2015', '2005', 'Undrafted', 'Undrafted', '2014', 'Undrafted', '2014', '2009', '2009', '2015', '2015', '2012', '2008', '2014', '2007', '2015', 'Undrafted', '2008', '2006', '2013', '2014', '2008', '2008', 'Undrafted', '2006', 'Undrafted', '2015', '2008', '2004', 'Undrafted', '2008', '2013', 'Undrafted', '2013', '2014', '2013', 'Undrafted', '2013', '2011', '2009', '2001', '2013', '2014', '2006', 'Undrafted', '2015', '2009', '2010', '2009', '2001', '2010', '2006', '1998', '2015', 'Undrafted', '2015', '2006', '2015', '2007', '2006', '2015', '2002', '2013', '2013', '2005', '2013', '2011', '2012', '2012', '2013', '2014', '2004', '2010', '2013', '2015', '2011', 'Undrafted', '2009', '2014', '2009', 'Undrafted', '2012', '2009', '2012', '2001', 'Undrafted', '2013', '1998', '2009', '2010', 'Undrafted', '2012', '2015', 'Undrafted', 'Undrafted', '2014', '2014', '2015', '2008', '2013', '2004', '2007', '2015', '2009', '2003', '2006', '2013', 'Undrafted', '2014', '2009', 'Undrafted', '2002', '2012', '2001', '2012', '2006', '2007', '2014', '2012', '2007', '2010', '1997', 'Undrafted', '2013', 'Undrafted', '2011', '2009', '2015', '2009', '2004', '2008', '2015', '2003', '2011', '1996', '2008', '2012', '2004', '2015', '2014', '2003', '2006', '2012', '2010', '2011', '2006', '2014', '2010', 'Undrafted', 'Undrafted', '2015', '2011', '2003', '2003', '2013', 'Undrafted', '2005', '2011', '2008', '2004', '1995', 'Undrafted', 'Undrafted', '2014', '2007', '2015', '2004', '2009', '2014', '2015', '2011', 'Undrafted', '2015', '2008', '2014', '2015', '2011', 'Undrafted', '2013', '2015', '2011', '2012', '2003', '2011', 'Undrafted', '2013', '2007', '2014', '2013', '2012', '2005', '2000', '2013', '2012', 'Undrafted', '2014', '2003', '2005', '2015', '2015', '1998', '2010', '2002', '2002', '2013', '2007', '2008', '2014', '2014', '2011', '2008', '2011', '2014', '2015', 'Undrafted', '2011', '2008', '2003', '2007', '2012', '2013', '2002', '2010', '2004', '1999', '2007', 'Undrafted', '2005', '2007', '2011', '2014', '2012', '2008', '2015', '2014', '2009', '2008', '2008', '2011', '1999', '2012', 'Undrafted', '2012', '2002', '2013', '2005', '2003', '2005', '2013', '2002', 'Undrafted', '2012', '1999', '2012', '2013', '2008', 'Undrafted', '2016', 'Undrafted', '2008', '2009', '2011', '2015', '2009', '2014', '2011', 'Undrafted', '2007', '2005', 'Undrafted', '2012', '2007', '1999', '2016', '2007', 'Undrafted', '2000', '2008', 'Undrafted', '2011', '2015', '2014', '2011', '2011', '2014', '2014', 'Undrafted', 'Undrafted', '2008', '2002', '2007', '2013', '2002', '2010', '2015', '2015', '2005', 'Undrafted', 'Undrafted', '2012', 'Undrafted', '2012', '2013', '2003', '2016', '2007', '2016', '2010', '2008', '2007', '2013', 'Undrafted', '2011', '2011', '2015', '2013', '2016', '2011', '2015', '2015', '2014', '2015', 'Undrafted', '2011', '2012', '2015', '2014', '2016', '2009', '2015', '2014', 'Undrafted', '2013', '2014', '2012', '2011', '2008', '2016', '2004', '2010', '2002', '2013', '2008', '2005', '2003', '2003', '2011', '2010', '2015', 'Undrafted', 'Undrafted', '2010', '2014', '2006', '2011', 'Undrafted', '2011', '2012', '2006', '2003', '2014', '2015', '2004', '2016', 'Undrafted', '2006', '2007', '2015', '2001', '2004', '2009', '2012', '2011', '2016', 'Undrafted', 'Undrafted', '2013', 'Undrafted', '2007', '2016', '2013', '2012', '2006', '2015', '2012', '2012', '2016', 'Undrafted', '2009', '2014', 'Undrafted', '2013', '2006', '2016', '2007', 'Undrafted', '2004', '2010', '2014', '2015', 'Undrafted', 'Undrafted', '2015', '2012', 'Undrafted', '2010', 'Undrafted', '2009', '2016', '1998', '2013', 'Undrafted', '2015', '2001', '2009', '2012', '2016', 'Undrafted', '2014', '2009', 'Undrafted', 'Undrafted', '2011', '2015', '2013', '2009', '2016', '2015', '2014', '2015', '2001', 'Undrafted', '2011', '2013', '2005', '2013', '2015', '2006', '2007', '2015', '2006', '2009', '2015', '2012', '2013', '2016', '1998', '2006', '2010', '2001', '2009', '2010', '2016', '2009', 'Undrafted', 'Undrafted', 'Undrafted', '2008', '2014', '2013', '2016', '2011', 'Undrafted', 'Undrafted', '2004', '2014', '2013', 'Undrafted', '2006', '2008', '2014', 'Undrafted', '2004', '2015', 'Undrafted', '2013', '2008', '2008', '2013', '2006', '2008', 'Undrafted', '2015', 'Undrafted', '2007', 'Undrafted', '2007', '2014', '2007', '2006', '2008', '2015', 'Undrafted', '2008', '2011', '2007', '2010', '2013', '2014', 'Undrafted', '2005', '2015', '2003', 'Undrafted', '2016', 'Undrafted', '2005', '2011', '2003', '2016', '2015', '2014', '2005', '2013', 'Undrafted', '2016', '2016', '2016', '2012', 'Undrafted', '2010', '2005', '2016', '2013', '2016', '2015', '2016', '2009', '2010', '2009', '2016', '2011', '2008', '2011', '2003', 'Undrafted', '2005', '2015', '2009', '2008', '2014', 'Undrafted', '2014', '2009', '2009', '2008', 'Undrafted', 'Undrafted', 'Undrafted', '2014', '2016', '2014', '2012', '2015', '2005', '2013', '2004', '2012', '2004', '2005', 'Undrafted', '2013', '2008', '2013', 'Undrafted', '2013', '2011', 'Undrafted', 'Undrafted', '2010', '2004', '2001', '2014', 'Undrafted', '2014', '2007', '2016', '2013', '2015', '2012', 'Undrafted', 'Undrafted', 'Undrafted', '2008', '2011', '2009', '2016', '2005', '2007', '2012', '2003', '2011', '2015', 'Undrafted', 'Undrafted', '2009', '2011', '2004', '2013', '2016', 'Undrafted', '2010', '2012', '2007', 'Undrafted', 'Undrafted', '2013', 'Undrafted', '2008', '2009', '2008', '2015', '2016', '1999', '2007', 'Undrafted', '2005', '2014', '2015', 'Undrafted', '2012', '2007', '2014', 'Undrafted', '2007', '2003', '2009', '2013', '2004', '2016', '2000', '2016', '2016', '2015', '2012', '2014', '2008', 'Undrafted', '2009', '2009', '2013', '2014', '2010', '2014', 'Undrafted', 'Undrafted', '2011', '2009', '2011', '2014', '2010', 'Undrafted', '2012', '2012', '2013', '2014', '2016', 'Undrafted', '2015', '2001', 'Undrafted', '2014', '2009', '2007', '2011', '2008', '2015', 'Undrafted', '2012', '2004', '2011', '2006', '2016', '2012', '2005', '2008', '2010', '2011', '2015', 'Undrafted', '2014', '2014', '2010', '2011', '2003', '2010', '2014', '2012', '2016', '2014', 'Undrafted', '2011', '2016', '1998', '2012', '2016', '2016', 'Undrafted', '2004', '2004', '2015', 'Undrafted', 'Undrafted', 'Undrafted', '2016', '2011', 'Undrafted', '2013', '2011', '2005', 'Undrafted', 'Undrafted', '2016', '2010', '2012', '2010', '2010', '2013', '2010', '2008', '2014', '2013', '2009', '2005', '2016', '2016', '2008', 'Undrafted', 'Undrafted', '2014', 'Undrafted', '2003', '2015', 'Undrafted', '2012', '2015', '2016', '2016', 'Undrafted', '2000', '2016', 'Undrafted', '2004', 'Undrafted', '2013', '2009', 'Undrafted', '2009', 'Undrafted', '2014', 'Undrafted', '2007', 'Undrafted', '2015', '2017', '2005', '2007', '1999', '2017', '2016', 'Undrafted', 'Undrafted', 'Undrafted', '2014', '2010', '2013', '2010', '2016', '2012', '2010', '2016', 'Undrafted', '2005', '2017', '2011', '2013', 'Undrafted', 'Undrafted', '2011', '2016', 'Undrafted', '2017', '2016', 'Undrafted', '2006', '2004', 'Undrafted', 'Undrafted', '2008', '2017', 'Undrafted', '2017', '2007', '2009', '2010', '2015', 'Undrafted', 'Undrafted', '2017', '2014', '2017', 'Undrafted', '2007', '2015', '2004', '2009', '2014', '2016', 'Undrafted', '2015', 'Undrafted', '2017', '2017', '2015', '2014', '2017', '2015', '2011', '2016', '2013', '2014', '2017', 'Undrafted', '2017', '2013', '2014', '2010', '2012', 'Undrafted', '2015', '2008', '2011', '2007', '2009', '2014', 'Undrafted', '2001', '2015', '2016', '2014', '2013', '2017', '2012', 'Undrafted', '2010', 'Undrafted', '2014', '2011', '2009', '2011', 'Undrafted', '2008', '2014', '2013', 'Undrafted', '2016', '2012', 'Undrafted', 'Undrafted', '2017', '2016', 'Undrafted', '2008', '2009', '2009', '2014', 'Undrafted', '2015', '2014', '2008', '2009', '2015', 'Undrafted', 'Undrafted', '2003', '2011', '2017', '2017', '2008', '2011', '2016', '2012', '2017', '2008', '2015', '2008', '2014', 'Undrafted', '2016', 'Undrafted', '2013', '2005', '2017', '2015', '2016', '2003', '2015', '2011', '2005', 'Undrafted', '2016', '2016', 'Undrafted', '2015', '2005', '2014', '2013', '2010', '2007', '2011', '2008', 'Undrafted', '2009', '2015', '2010', '2016', '2010', '2017', '2010', '2014', '2004', '2015', '2011', '2010', '2008', 'Undrafted', 'Undrafted', '2005', '2011', '2012', '2015', '2017', '2017', 'Undrafted', '2016', 'Undrafted', '2014', 'Undrafted', '2008', '2016', '2016', '2005', '2010', '2003', '2014', '2004', '2015', '2016', '2013', '2017', '2016', '2010', 'Undrafted', '2008', 'Undrafted', '2017', '2011', '2015', '2004', 'Undrafted', 'Undrafted', '2016', '2017', '2012', '1998', '2016', '2017', 'Undrafted', '2014', '2016', '2012', '2017', 'Undrafted', '2009', '2011', '2003', '2011', 'Undrafted', '2017', '2008', '2013', '2014', '2013', 'Undrafted', '2004', '2011', '2017', '2016', '2013', 'Undrafted', '2014', '2009', '2017', '2013', 'Undrafted', '2014', '2017', '2009', 'Undrafted', '2016', '2017', '2012', '2015', '2015', '2015', 'Undrafted', 'Undrafted', '2012', '2010', 'Undrafted', '2015', '2006', '2007', '2015', '2013', '2005', '2013', 'Undrafted', '2011', '2001', '2015', '2009', 'Undrafted', '2008', '2014', 'Undrafted', 'Undrafted', 'Undrafted', '2015', 'Undrafted', '2006', '2013', '2008', '2008', '2006', '2016', '2007', '2016', '2001', '2015', 'Undrafted', 'Undrafted', '2013', '1998', 'Undrafted', '2016', 'Undrafted', '2009', 'Undrafted', '2017', '2016', '2010', '2012', '2015', 'Undrafted', '2015', '2007', 'Undrafted', 'Undrafted', '2011', 'Undrafted', '2017', '2014', '2001', 'Undrafted', '2009', '2012', '2016', 'Undrafted', '2013', 'Undrafted', 'Undrafted', '2016', '2011', '2012', '2004', '2017', '2001', '2013', 'Undrafted', 'Undrafted', 'Undrafted', '2004', '2010', '2013', '2015', 'Undrafted', '2011', 'Undrafted', 'Undrafted', 'Undrafted', '2017', '2014', 'Undrafted', '2017', '2017', '2016', '2006', '2001', '2005', '2008', '2013', 'Undrafted', '2010', '2017', 'Undrafted', '2004', '2016', '2016', 'Undrafted', 'Undrafted', '2013', '2016', 'Undrafted', '1999', '2011', '2007', '2005', '2007', 'Undrafted', '2011', '2016', '2014', '2008', '2015', '2017', '2017', 'Undrafted', '2003', 'Undrafted', '2013', '2007', '2008', '2015', 'Undrafted', '2012', '2011', 'Undrafted', '2008', '2016', '2015', '2014', 'Undrafted', '2003', '2017', '2006', '2012', '2011', '2011', '2006', '2010', 'Undrafted', 'Undrafted', 'Undrafted', '2015', '2017', '2014', '2010', '2017', '2016', '2013', '2003', '2007', '2008', 'Undrafted', 'Undrafted', '2014', '2014', '2011', '2011', '2014', '2015', '2002', '2017', 'Undrafted', '2008', '2009', '2013', '2006', 'Undrafted', '2016', '2015', '2009', '2016', '2010', '2009', 'Undrafted', '2010', 'Undrafted', '2013', '2012', '2008', 'Undrafted', '2005', '2013', 'Undrafted', 'Undrafted', 'Undrafted', '2012', 'Undrafted', '2012', '2008', '2013', '2012', '2007', 'Undrafted', '2013', '2012', '2012', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', '2017', '2015', 'Undrafted', '2015', '2011', '2003', '2016', 'Undrafted', '2015', '2005', '2013', '2004', '2012', '2005', '2013', 'Undrafted', 'Undrafted', '2013', '2013', '2017', '2011', 'Undrafted', '2010', '2004', '2007', '2014', '2016', 'Undrafted', 'Undrafted', '2014', '2007', 'Undrafted', '2014', 'Undrafted', '2015', 'Undrafted', '2009', '2016', '2007', '2012', '2011', '2014', '2016', 'Undrafted', 'Undrafted', '2009', '2011', '2016', '2015', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', '2012', '2012', '2010', '2017', '2013', '2007', '2007', '2013', '2012', '2018', '2012', '2018', 'Undrafted', '2018', '2017', '2015', '2018', 'Undrafted', '2012', '2009', '2008', '2012', '2018', 'Undrafted', '2012', 'Undrafted', '2013', '2005', '2018', '2016', '2011', '2018', '2013', '2015', '2010', '2010', '2006', '2017', '2001', '2009', '2010', '2016', '2009', '2015', '2016', 'Undrafted', '2006', '2013', 'Undrafted', '2009', 'Undrafted', '2017', '2015', '2014', '2011', '2011', '2014', '2014', '2008', '2007', '2013', '2002', '2018', '2015', 'Undrafted', '2011', '2017', '2003', '2014', '2016', '2018', '2008', 'Undrafted', '2011', '2018', '2012', 'Undrafted', '2015', '2006', '2008', '2018', '2007', '2013', 'Undrafted', 'Undrafted', '2011', '2011', '2015', '2013', '2018', '2011', '2015', '2018', '2014', '2012', '2006', 'Undrafted', '2007', '2005', '2007', '2011', '2017', '2016', 'Undrafted', '2016', '2016', '2004', 'Undrafted', '2011', '2017', '2008', '2005', '2013', '2017', '2018', '2003', '2017', '2015', 'Undrafted', '2018', '2012', '2010', '2018', '2010', 'Undrafted', '2015', 'Undrafted', '2017', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', '2018', '2011', '2015', '2013', '2004', 'Undrafted', 'Undrafted', '2018', 'Undrafted', '2013', '2001', '2017', '2012', '2011', '2016', '2013', 'Undrafted', '2016', '2017', '2016', '2012', '2009', '2003', 'Undrafted', '2014', '2017', 'Undrafted', 'Undrafted', 'Undrafted', '2007', '2015', '2015', '2012', 'Undrafted', '2018', '2010', '2018', 'Undrafted', '2009', 'Undrafted', '2016', '2018', '1998', '2013', 'Undrafted', '2015', '2001', '2016', '2017', '2017', 'Undrafted', '2007', '2015', 'Undrafted', 'Undrafted', 'Undrafted', '2008', '2008', '2013', '2006', 'Undrafted', '2015', 'Undrafted', 'Undrafted', 'Undrafted', '2014', '2008', '2018', 'Undrafted', '2009', '2015', '2011', '2013', '2005', '2018', 'Undrafted', '2013', '2006', '2018', 'Undrafted', '2017', 'Undrafted', '2006', '2015', '2012', '2012', '2017', '2016', '2009', 'Undrafted', '2017', '2014', 'Undrafted', '2018', '2008', '2013', '2009', '2015', '2014', '2013', '2016', '2017', '2011', '2004', 'Undrafted', '2018', '2018', '2014', '2017', '2014', '2017', '2015', '2016', '2018', '2014', '2014', '2012', '2009', '2007', 'Undrafted', '2011', '2017', '2017', '2018', '2008', '2016', 'Undrafted', '2009', '2010', '2011', '2005', 'Undrafted', '2004', '2012', '2005', 'Undrafted', 'Undrafted', 'Undrafted', '2018', 'Undrafted', 'Undrafted', '2013', 'Undrafted', '2013', '2013', '2015', '2010', 'Undrafted', '2012', '1998', '2016', '2011', '2017', '2018', '2017', 'Undrafted', '2014', '2016', '2012', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', '2009', '2016', '2018', '2018', '2015', '2016', 'Undrafted', '2013', '2017', 'Undrafted', 'Undrafted', 'Undrafted', '2008', '2017', '2015', '2004', 'Undrafted', '2010', '2014', '2018', 'Undrafted', '2013', '2014', 'Undrafted', '2005', 'Undrafted', 'Undrafted', '2018', '2018', 'Undrafted', '2005', '2011', '2018', '2015', '2003', '2016', '2016', '2007', '2008', '2009', '2008', 'Undrafted', '2016', '2017', 'Undrafted', '2011', '2012', 'Undrafted', 'Undrafted', '2017', '2008', '2015', 'Undrafted', '2016', 'Undrafted', '2015', '2017', '2009', '2011', 'Undrafted', '2016', '2013', '2017', 'Undrafted', 'Undrafted', '2012', 'Undrafted', 'Undrafted', 'Undrafted', '2012', '2016', '2010', '2015', '2014', '2011', '2005', '2013', 'Undrafted', '2016', 'Undrafted', '2014', '2018', '2008', 'Undrafted', 'Undrafted', '2016', 'Undrafted', '2012', 'Undrafted', 'Undrafted', '2017', '2004', '2014', '2003', 'Undrafted', '2017', '2007', '2009', 'Undrafted', '2014', '2016', '2012', '2015', '2018', '2008', '2018', '2011', '2011', 'Undrafted', '2007', 'Undrafted', '2007', 'Undrafted', '2000', '2016', '2013', '2009', '2009', '2017', 'Undrafted', 'Undrafted', '2015', '2018', '2017', '2018', '2017', '2007', '2009', 'Undrafted', '2014', 'Undrafted', 'Undrafted', '2017', '2017', '2018', '2015', '2014', '2009', 'Undrafted', '2014', '2015', 'Undrafted', '2017', '2017', '2016', 'Undrafted', '2014', '2017', 'Undrafted', '2014', '2017', '2012', 'Undrafted', '2012', '2010', 'Undrafted', 'Undrafted', '2011', '2017', '2009', '2011', '2017', 'Undrafted', '2018', '2014', '2016', '2015', '2017', '2017', 'Undrafted', '2016', 'Undrafted', 'Undrafted', '2015', '2014', 'Undrafted', '2008', '2018', '2016', '2005', '2013', 'Undrafted', '2014', '2017', '2012', '2018', '2011', '2010', '2017', '2010', '2014', '2010', '2018', '2015', '2011', '2010', '2008', 'Undrafted', '2005', 'Undrafted', '2008', '2010', '2013', '2011', 'Undrafted', '2017', '2016', 'Undrafted', 'Undrafted', 'Undrafted', '2006', 'Undrafted', 'Undrafted', '2008', '2014', '2018', '2012', '2004', '2017', '2013', 'Undrafted', '2018', '2010', '2016', '2018', '2012', '2017', '2010', 'Undrafted', '2016', 'Undrafted', '2005', '2017', '2011', '2018', 'Undrafted', '2016', '2011', '2016', '2018', 'Undrafted', '2016', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', '2014', 'Undrafted', '2018', '2008', '2016', '2013', '2014', '2019', '2008', '2010', '2013', '2019', '2018', '2018', '2012', '2017', '2010', '2016', '2005', '2019', '2011', '2018', '2017', '2017', '2019', '2017', '2017', '2016', '2017', 'Undrafted', '2018', 'Undrafted', '2014', '2016', '2012', 'Undrafted', 'Undrafted', '2017', '2017', '2004', '2014', '2018', '2011', '2010', '2017', '2014', '2018', '2015', '2011', '2010', '2008', '2017', '2019', '2005', '2012', '2010', '2015', '2012', '2011', '2016', 'Undrafted', '2019', '2017', '2007', '2009', '2014', 'Undrafted', '2012', '2009', '2015', '2018', '2018', '2011', '2007', '2018', '2014', 'Undrafted', '2014', '2017', '2012', 'Undrafted', 'Undrafted', 'Undrafted', '2017', '2011', '2017', '2017', '2019', '2014', '2016', 'Undrafted', '2019', 'Undrafted', 'Undrafted', '2018', '2006', '2004', '2019', 'Undrafted', 'Undrafted', '2008', '2014', '2018', '2012', '2015', '2016', '2016', '2018', 'Undrafted', '2019', '2000', '2016', '2013', '2009', '2009', '2007', 'Undrafted', '2018', '2018', '2019', '2017', '2019', 'Undrafted', '2017', 'Undrafted', '2019', '2011', '2009', 'Undrafted', '2015', '2014', '2011', '2019', 'Undrafted', '2012', '2019', 'Undrafted', '2016', '2011', 'Undrafted', '2008', '2018', '2014', '2019', 'Undrafted', '2016', '2013', '2005', 'Undrafted', '2017', '2019', '2019', '2015', '2016', '2003', '2016', '2019', '2013', 'Undrafted', '2018', '2016', 'Undrafted', '2019', '2007', '2010', '2011', '2019', 'Undrafted', '2013', 'Undrafted', '2018', '2013', '2018', '2017', 'Undrafted', '2012', '2004', '2013', '2014', '2018', '2016', '2012', 'Undrafted', 'Undrafted', '2017', 'Undrafted', '2012', '2010', '2017', '2015', '2018', '2011', '2019', '2019', '2018', 'Undrafted', '2011', '2017', '2019', '2018', '2008', '2016', '2009', '2009', 'Undrafted', '2018', '2016', '2015', '2013', '2017', '2016', 'Undrafted', '2010', 'Undrafted', '2008', 'Undrafted', '2017', '2015', '2018', 'Undrafted', '2018', '2014', 'Undrafted', '2014', '2009', 'Undrafted', '2015', '2016', '2018', 'Undrafted', 'Undrafted', 'Undrafted', '2005', 'Undrafted', 'Undrafted', '2014', '2019', '2019', '2013', '2014', '2018', '2011', '2008', 'Undrafted', '2015', '2008', '2017', 'Undrafted', '2016', '2012', 'Undrafted', '2017', '2019', 'Undrafted', '2008', '2007', '2019', '2014', '2017', '2015', 'Undrafted', '2006', '2013', '2019', '2008', '2008', 'Undrafted', 'Undrafted', '2019', '2017', '2008', 'Undrafted', '2014', '2018', '2018', 'Undrafted', 'Undrafted', '2017', '2016', '2013', '2014', '2015', 'Undrafted', '2009', '2017', '2013', '2018', '2017', '2019', '2015', 'Undrafted', '2018', '2017', '2018', 'Undrafted', '2013', '2006', '2019', 'Undrafted', '2016', '2015', '2009', '2016', '2010', '2009', '2010', '2006', 'Undrafted', 'Undrafted', '2019', '2019', '2006', '2013', 'Undrafted', '2013', '2011', '2015', '2009', 'Undrafted', '2018', '2008', '2014', '2014', 'Undrafted', '2009', 'Undrafted', '2012', '2016', '2001', '2015', 'Undrafted', 'Undrafted', '2013', '1998', 'Undrafted', '2017', '2009', '2018', 'Undrafted', '2017', 'Undrafted', '2012', 'Undrafted', '2015', '2015', '2007', 'Undrafted', 'Undrafted', '2017', '2014', '2019', '2018', '2019', 'Undrafted', '2019', 'Undrafted', '2019', 'Undrafted', '2019', 'Undrafted', '2016', '2019', 'Undrafted', '2017', '2012', '2015', '2006', '2007', '2014', 'Undrafted', '2017', '2016', 'Undrafted', 'Undrafted', '2016', '2011', '2012', '2017', '2013', 'Undrafted', '2018', '2019', 'Undrafted', '2004', '2013', '2015', '2011', '2018', '2013', '2015', 'Undrafted', '2011', '2018', '2018', '2018', '2008', '2019', '2015', 'Undrafted', '2012', '2018', '2018', '2016', '2015', 'Undrafted', 'Undrafted', '2014', '2019', '2003', '2017', '2006', '2012', '2011', '2006', 'Undrafted', '2018', 'Undrafted', '2015', '2017', '2003', '2014', '2013', '2017', 'Undrafted', 'Undrafted', '2017', '2012', '2018', 'Undrafted', '2015', '2009', 'Undrafted', '2016', '2014', '2015', 'Undrafted', '2017', '2019', '2017', '2019', '2019', '2015', '2014', 'Undrafted', '2019', '2017', '2015', '2011', '2018', 'Undrafted', '2019', '2013', '2015', '2011', 'Undrafted', '2005', '2018', '2008', 'Undrafted', 'Undrafted', '2012', '2018', '2018', '2007', '2013', '2012', '2018', '2018', '2019', '2018', '2017', '2015', '2018', 'Undrafted', 'Undrafted', '2015', '2019', 'Undrafted', 'Undrafted', '2010', '2013', '2019', '2008', '2019', 'Undrafted', '2017', '2014', '2019', '2012', '2013', 'Undrafted', 'Undrafted', '2018', '2019', '2017', 'Undrafted', '2016', 'Undrafted', '2016', '2017', '2018', '2007', '2007', '2018', '2014', '2019', '2011', '2017', 'Undrafted', '2012', 'Undrafted', '2015', 'Undrafted', '2019', 'Undrafted', '2005', '2018', '2016', '2019', 'Undrafted', '2011', '2013', 'Undrafted', 'Undrafted', '2019', '2020', '2014', 'Undrafted', '2020', '2015', '2015', '2017', '2017', 'Undrafted', 'Undrafted', '2016', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', '2014', 'Undrafted', '2018', '2008', '2016', '2013', '2014', '2019', '2008', '2010', '2013', '2020', '2019', '2018', 'Undrafted', '2018', '2012', '2017', '2017', '2010', 'Undrafted', '2005', 'Undrafted', 'Undrafted', '2018', 'Undrafted', '2019', '2017', '2016', '2017', 'Undrafted', '2018', 'Undrafted', '2014', '2012', 'Undrafted', 'Undrafted', '2017', '2004', '2014', '2019', '2011', '2010', '2017', '2014', 'Undrafted', '2020', '2011', '2010', '2008', '2019', '2012', '2016', '2019', '2011', '2007', 'Undrafted', '2018', '2018', '2019', '2017', '2019', 'Undrafted', '2019', '2020', 'Undrafted', '2016', 'Undrafted', '2019', '2017', '2007', '2009', '2014', 'Undrafted', '2012', '2018', '2018', '2011', '2014', 'Undrafted', '2014', '2017', 'Undrafted', '2010', '2020', 'Undrafted', '2009', '2009', '2020', '2018', '2020', '2017', '2020', '2019', '2020', '2011', 'Undrafted', '2016', '2006', '2019', 'Undrafted', 'Undrafted', '2020', '2008', '2020', '2012', 'Undrafted', '2015', '2020', '2016', '2016', '2018', '2020', 'Undrafted', '2019', '2020', '2016', '2013', '2014', '2019', '2015', '2020', '2013', '2016', '2011', '2009', 'Undrafted', '2015', '2014', '2011', '2019', 'Undrafted', '2012', '2019', 'Undrafted', '2016', 'Undrafted', 'Undrafted', '2008', '2018', '2014', '2019', 'Undrafted', '2016', '2020', '2013', 'Undrafted', '2019', '2019', 'Undrafted', '2015', '2017', 'Undrafted', '2010', '2012', '2018', '2020', '2016', 'Undrafted', '2007', '2010', '2011', '2020', '2019', 'Undrafted', '2013', 'Undrafted', '2018', 'Undrafted', 'Undrafted', 'Undrafted', '2012', '2004', '2013', '2014', '2018', '2012', '2020', 'Undrafted', 'Undrafted', 'Undrafted', '2017', '2019', 'Undrafted', 'Undrafted', '2004', '2011', '2016', '2019', '2009', '2014', 'Undrafted', '2014', '2019', '2019', '2012', 'Undrafted', '2011', '2017', '2019', '2018', '2008', '2016', '2009', '2010', 'Undrafted', '2018', '2019', '2016', '2015', '2020', '2013', '2017', '2016', '2010', 'Undrafted', '2008', '2017', '2008', 'Undrafted', '2020', '2019', '2020', '2020', '2015', '2018', 'Undrafted', '2015', '2018', 'Undrafted', 'Undrafted', '2005', 'Undrafted', 'Undrafted', '2019', '2014', '2003', '2019', '2013', '2020', '2018', '2011', 'Undrafted', '2015', '2008', '2017', 'Undrafted', '2018', '2016', '2012', 'Undrafted', '2017', '2019', 'Undrafted', '2017', '2019', '2015', '2009', 'Undrafted', '2019', '2018', '2020', '2008', '2018', '2014', 'Undrafted', '2019', '2015', 'Undrafted', '2006', '2013', '2019', '2008', 'Undrafted', '2020', '2020', '2020', 'Undrafted', '2019', '2017', '2008', 'Undrafted', '2018', '2018', 'Undrafted', '2020', '2011', '2013', 'Undrafted', '2015', 'Undrafted', '2017', '2020', '2020', 'Undrafted', '2013', '2006', '2019', 'Undrafted', '2016', '2015', '2009', '2016', '2017', '2010', '2009', '2010', '2006', '2020', 'Undrafted', '2020', '2020', 'Undrafted', '2019', '2020', '2019', '2006', '2013', '2018', '2020', '2014', '2020', '2014', 'Undrafted', '2004', '2013', '2015', '2011', '2018', '2019', 'Undrafted', '2020', 'Undrafted', '2019', 'Undrafted', '2020', '2020', '2020', '2015', '2020', 'Undrafted', '2020', '2013', 'Undrafted', '2017', '2009', '2018', 'Undrafted', '2017', 'Undrafted', '2012', 'Undrafted', '2019', '2020', '2018', 'Undrafted', '2015', '2009', '2017', '2013', '2018', '2017', '2015', '2014', 'Undrafted', '2009', '2019', '2016', '2019', 'Undrafted', '2013', '2017', '2015', '2007', '2014', '2020', 'Undrafted', '2017', '2016', 'Undrafted', '2013', '2016', '2011', '2012', '2017', '2013', '2012', '2015', '2011', 'Undrafted', 'Undrafted', 'Undrafted', '2013', '2020', '2007', '2018', '2018', '2008', '2019', '2015', 'Undrafted', '2012', '2018', '2020', 'Undrafted', '2020', '2018', '2016', '2015', '2014', '2019', '2017', '2006', '2011', '2006', '2020', 'Undrafted', '2018', 'Undrafted', 'Undrafted', '2011', '2015', '2013', '2014', 'Undrafted', '2020', '2019', '2020', 'Undrafted', '2017', '2017', '2018', '2015', '2009', 'Undrafted', '2016', '2014', '2015', 'Undrafted', '2019', '2017', '2019', '2015', '2014', 'Undrafted', '2019', 'Undrafted', '2015', '2011', '2018', 'Undrafted', '2019', '2019', '2017', '2014', '2017', '2018', '2018', '2018', '2007', 'Undrafted', '2013', '2012', '2018', '2018', '2019', '2018', '2017', '2015', '2018', 'Undrafted', 'Undrafted', '2015', 'Undrafted', '2019', 'Undrafted', 'Undrafted', 'Undrafted', 'Undrafted', '2010', '2013', '2020', '2019', '2020', '2008', '2019', '2013', '2019', '2012', 'Undrafted', '2017', '2005', 'Undrafted', 'Undrafted', '2018', '2019', '2017', 'Undrafted', '2020', '2016', '2016', 'Undrafted', '2017', 'Undrafted', '2003', '2007', '2014', '2017', '2011', 'Undrafted', '2019', '2016', '2018', 'Undrafted', '2013', '2019', 'Undrafted', 'Undrafted', '2012', 'Undrafted', '2011', '2014'] ['1996-97', '1997-98', '1998-99', '1999-00', '2000-01', '2001-02', '2002-03', '2003-04', '2004-05', '2005-06', '2006-07', '2007-08', '2008-09', '2009-10', '2010-11', '2011-12', '2012-13', '2013-14', '2014-15', '2015-16', '2016-17', '2017-18', '2018-19', '2019-20', '2020-21'] ['LAL', 'MIA', 'HOU', 'BOS', 'DAL', 'TOR', 'CLE', 'GSW', 'IND', 'PHI', 'PHX', 'POR', 'LAC', 'CHH', 'SAC', 'CHI', 'NYK', 'DET', 'ORL', 'SEA', 'WAS', 'VAN', 'SAS', 'ATL', 'NJN', 'UTA', 'MIL', 'DEN', 'MIN', 'MEM', 'NOH', 'CHA', 'NOK', 'OKC', 'BKN', 'NOP']\n"
     ]
    }
   ],
   "source": [
    "colleges = []\n",
    "countries = []\n",
    "years = []\n",
    "seasons = []\n",
    "teams = []\n",
    "for i in range(0, total_players):\n",
    "  if(csv[\"college\"][i] not in colleges):\n",
    "    colleges.append(csv[\"college\"][i])\n",
    "  if(csv[\"country\"][i] not in countries):\n",
    "    countries.append(csv[\"country\"][i])\n",
    "  if(csv[\"draft_year\"][i] not in colleges):\n",
    "    years.append(csv[\"draft_year\"][i])\n",
    "  if(csv[\"season\"][i] not in seasons):\n",
    "    seasons.append(csv[\"season\"][i])\n",
    "  if(csv[\"team_abbreviation\"][i] not in teams):\n",
    "    teams.append(csv[\"team_abbreviation\"][i])\n",
    "print(colleges, countries, years, seasons, teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "843UJCmywpw5",
    "outputId": "0fc70c2e-988d-4b39-9f5d-cccf808995ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3026233603997501 21.783007080383175\n"
     ]
    }
   ],
   "source": [
    "sum_round = 0\n",
    "count_round = 0\n",
    "sum_number = 0\n",
    "count_number = 0\n",
    "for i in range(0, total_players):\n",
    "  if(csv[\"draft_number\"][i] != \"Undrafted\"):\n",
    "    sum_number += int(csv[\"draft_number\"][i])\n",
    "    count_number += 1\n",
    "  if(csv[\"draft_round\"][i] != \"Undrafted\"):\n",
    "    sum_round += int(csv[\"draft_round\"][i])\n",
    "    count_round += 1\n",
    "\n",
    "mean_round = sum_round / count_round\n",
    "mean_number = sum_number / count_number\n",
    "print(mean_round, mean_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RvnWL8J0yCJq",
    "outputId": "9ace3917-5648-4a74-f410-2d492979233e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3337/2124559815.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  csv[\"draft_number\"][i] = mean_number\n",
      "/tmp/ipykernel_3337/2124559815.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  csv[\"draft_round\"][i] = mean_round\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, total_players):\n",
    "  if(csv[\"draft_number\"][i] == \"Undrafted\"):\n",
    "    csv[\"draft_number\"][i] = mean_number\n",
    "  if(csv[\"draft_round\"][i] == \"Undrafted\"):\n",
    "    csv[\"draft_round\"][i] = mean_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rXek9_aJpQ3f"
   },
   "outputs": [],
   "source": [
    "def one_hot(value, array):\n",
    "  v = [0 for i in range(len(array))]\n",
    "  v[array.index(value)] = 1\n",
    "  return torch.Tensor(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB2uQ3dqZBfV"
   },
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EcBguZmQVkO0"
   },
   "outputs": [],
   "source": [
    "def random_split(test_percent):\n",
    "  test_amount = int(total_players * test_percent)\n",
    "  test_sample = random.sample(range(0,total_players), test_amount)\n",
    "  train_sample = [i for i in range(0, total_players)]\n",
    "  test_x = []\n",
    "  test_y = []\n",
    "  train_x = []\n",
    "  train_y = []\n",
    "  for idx in test_sample:\n",
    "    train_sample.remove(idx)\n",
    "    test_x.append([one_hot(csv[\"team_abbreviation\"][idx], teams), one_hot(csv[\"college\"][idx], colleges), one_hot(csv[\"country\"][idx], countries),\n",
    "                   one_hot(csv[\"draft_year\"][idx], years), one_hot(csv[\"season\"][idx], seasons), float(csv[\"age\"][idx]),\n",
    "                   float(csv[\"player_height\"][idx]), float(csv[\"player_weight\"][idx]), float(csv[\"draft_round\"][idx]), float(csv[\"draft_number\"][idx]),\n",
    "                   float(csv[\"gp\"][idx]), float(csv[\"net_rating\"][idx]), float(csv[\"usg_pct\"][idx])])\n",
    "    test_y.append(torch.Tensor([float(csv[\"pts\"][idx]), float(csv[\"reb\"][idx]),\n",
    "                   float(csv[\"ast\"][idx]), float(csv[\"oreb_pct\"][idx]), float(csv[\"dreb_pct\"][idx]), float(csv[\"ts_pct\"][idx]), float(csv[\"ast_pct\"][idx])]))\n",
    "  for idx in train_sample:\n",
    "    train_x.append([one_hot(csv[\"team_abbreviation\"][idx], teams), one_hot(csv[\"college\"][idx], colleges), one_hot(csv[\"country\"][idx], countries),\n",
    "                   one_hot(csv[\"draft_year\"][idx], years), one_hot(csv[\"season\"][idx], seasons), float(csv[\"age\"][idx]),\n",
    "                   float(csv[\"player_height\"][idx]), float(csv[\"player_weight\"][idx]), float(csv[\"draft_round\"][idx]), float(csv[\"draft_number\"][idx]),\n",
    "                   float(csv[\"gp\"][idx]), float(csv[\"net_rating\"][idx]), float(csv[\"usg_pct\"][idx])])\n",
    "    train_y.append(torch.Tensor([float(csv[\"pts\"][idx]), float(csv[\"reb\"][idx]),\n",
    "                   float(csv[\"ast\"][idx]), float(csv[\"oreb_pct\"][idx]), float(csv[\"dreb_pct\"][idx]), float(csv[\"ts_pct\"][idx]), float(csv[\"ast_pct\"][idx])]))\n",
    "  return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETNQDbVTtHJR",
    "outputId": "a4b9b9ee-4ac3-4de3-b95e-d7be20b239cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.63 s, sys: 264 ms, total: 9.89 s\n",
      "Wall time: 9.83 s\n"
     ]
    }
   ],
   "source": [
    "%time train_x, train_y, test_x, test_y = random_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9360 9360\n",
      "2340 2340\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x), len(train_y))\n",
    "print(len(test_x), len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXJsCq65ZWKm"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wUFwyj82WDeB"
   },
   "outputs": [],
   "source": [
    "def train(neural_net, optimizer, loss, scheduler, train_features, train_labels, epochs, batch_size, dropout=False):\n",
    "  xs = [[] for i in range(epochs)]\n",
    "  ys = [[] for i in range(epochs)]\n",
    "  neural_net.train()\n",
    "  for epoch in range(epochs):\n",
    "    count = 0\n",
    "    rl = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for i in range(len(train_labels)):\n",
    "      train_y = train_labels[i]\n",
    "      train_x = train_features[i]\n",
    "      output = neural_net(train_x)\n",
    "      out_loss = loss(output, train_y)\n",
    "      out_loss.backward()\n",
    "      optimizer.step()\n",
    "      count += 1\n",
    "      rl += out_loss.item()\n",
    "      if count % batch_size == 0:\n",
    "        optimizer.zero_grad()\n",
    "        print(str(count) + \" completed. Loss: \" + str(rl/batch_size))\n",
    "        xs[epoch].append(count)\n",
    "        ys[epoch].append(rl/batch_size)\n",
    "        rl = 0.0\n",
    "    scheduler.step()\n",
    "  return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "o3lRDT7abczZ"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.teamLin = nn.Linear(len(teams), 1)\n",
    "    self.collegeLin = nn.Linear(len(colleges), 1)\n",
    "    self.countryLin = nn.Linear(len(countries), 1)\n",
    "    self.draftLin = nn.Linear(len(years), 1)\n",
    "    self.seasonLin = nn.Linear(len(seasons), 1)\n",
    "    self.sequential = nn.Sequential(\n",
    "        nn.Linear(13, 20),\n",
    "        #nn.Dropout(p=0.2),\n",
    "        nn.Linear(20, 40),\n",
    "        #nn.Dropout(p=0.2),\n",
    "        nn.Linear(40, 40),\n",
    "        #nn.Dropout(p=0.2),\n",
    "        nn.Linear(40, 40),\n",
    "        #nn.Dropout(p=0.2),\n",
    "        nn.Linear(40, 20),\n",
    "        #nn.Dropout(p=0.2),\n",
    "        nn.Linear(20, 7),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.Tensor([self.teamLin(x[0]), self.collegeLin(x[1]), self.countryLin(x[2]), self.draftLin(x[3]), self.seasonLin(x[4])] + x[5:])\n",
    "    return self.sequential(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5iMAYklzpqVI"
   },
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.teamLin = nn.Linear(len(teams), 1)\n",
    "    self.collegeLin = nn.Linear(len(colleges), 1)\n",
    "    self.countryLin = nn.Linear(len(countries), 1)\n",
    "    self.draftLin = nn.Linear(len(years), 1)\n",
    "    self.seasonLin = nn.Linear(len(seasons), 1)\n",
    "    self.sequential = nn.Sequential(\n",
    "        nn.Linear(13, 7)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.Tensor([self.teamLin(x[0]), self.collegeLin(x[1]), self.countryLin(x[2]), self.draftLin(x[3]), self.seasonLin(x[4])] + x[5:])\n",
    "    return self.sequential(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iXGBUuFtbeyq"
   },
   "outputs": [],
   "source": [
    "neural = NeuralNet()\n",
    "linnet = LinearNet()\n",
    "loss = nn.MSELoss()\n",
    "lin_loss = nn.MSELoss()\n",
    "lin_optim = optim.Adam(linnet.parameters())\n",
    "optimizer = optim.Adam(neural.parameters())\n",
    "n_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 completed. Loss: 4.9774973830673845\n",
      "400 completed. Loss: 3.784033372104168\n",
      "600 completed. Loss: 3.7344077716208997\n",
      "800 completed. Loss: 3.8051443555706648\n",
      "1000 completed. Loss: 3.59155953691341\n",
      "1200 completed. Loss: 3.5686585773155093\n",
      "1400 completed. Loss: 3.8229046947788445\n",
      "1600 completed. Loss: 3.519837479745038\n",
      "1800 completed. Loss: 3.9213624075008555\n",
      "2000 completed. Loss: 3.4714549604011697\n",
      "2200 completed. Loss: 3.584048205576837\n",
      "2400 completed. Loss: 3.465184519756585\n",
      "2600 completed. Loss: 3.667901323568076\n",
      "2800 completed. Loss: 3.584350655162707\n",
      "3000 completed. Loss: 4.94993894668296\n",
      "3200 completed. Loss: 3.9165408366988412\n",
      "3400 completed. Loss: 3.656200190107338\n",
      "3600 completed. Loss: 4.03343846431002\n",
      "3800 completed. Loss: 4.281352837802842\n",
      "4000 completed. Loss: 4.2867395366728305\n",
      "4200 completed. Loss: 3.5475336947664617\n",
      "4400 completed. Loss: 4.331654513720423\n",
      "4600 completed. Loss: 4.36427830491215\n",
      "4800 completed. Loss: 4.216712399832904\n",
      "5000 completed. Loss: 3.810983950383961\n",
      "5200 completed. Loss: 3.5124181903898717\n",
      "5400 completed. Loss: 3.629015153031796\n",
      "5600 completed. Loss: 3.1262197976373134\n",
      "5800 completed. Loss: 3.707521667322144\n",
      "6000 completed. Loss: 3.0390318882931022\n",
      "6200 completed. Loss: 3.303505947748199\n",
      "6400 completed. Loss: 3.791878404079471\n",
      "6600 completed. Loss: 3.36507452532649\n",
      "6800 completed. Loss: 3.154483791794628\n",
      "7000 completed. Loss: 4.02438840651419\n",
      "7200 completed. Loss: 4.0177626808546485\n",
      "7400 completed. Loss: 4.116913378247991\n",
      "7600 completed. Loss: 3.751063322145492\n",
      "7800 completed. Loss: 3.785151900285855\n",
      "8000 completed. Loss: 3.9582503827475013\n",
      "8200 completed. Loss: 4.063377123803948\n",
      "8400 completed. Loss: 3.6749241583887486\n",
      "8600 completed. Loss: 3.8535713836271315\n",
      "8800 completed. Loss: 4.10935673086904\n",
      "9000 completed. Loss: 4.944745774827897\n",
      "9200 completed. Loss: 3.8847023035702297\n",
      "200 completed. Loss: 5.123928817528067\n",
      "400 completed. Loss: 3.7682537215389313\n",
      "600 completed. Loss: 3.703037516735494\n",
      "800 completed. Loss: 3.784448582136538\n",
      "1000 completed. Loss: 3.5358474545460195\n",
      "1200 completed. Loss: 3.526007535457611\n",
      "1400 completed. Loss: 3.783911301949993\n",
      "1600 completed. Loss: 3.458729286594316\n",
      "1800 completed. Loss: 3.8500966286705807\n",
      "2000 completed. Loss: 3.4107272637588903\n",
      "2200 completed. Loss: 3.5344832498487087\n",
      "2400 completed. Loss: 3.4028483764454722\n",
      "2600 completed. Loss: 3.47777867782861\n",
      "2800 completed. Loss: 3.4442538015544413\n",
      "3000 completed. Loss: 4.7525374485366045\n",
      "3200 completed. Loss: 3.7694907570816576\n",
      "3400 completed. Loss: 3.632896758634597\n",
      "3600 completed. Loss: 3.898298036959022\n",
      "3800 completed. Loss: 4.238437239397317\n",
      "4000 completed. Loss: 4.113566284920089\n",
      "4200 completed. Loss: 3.7064807659387586\n",
      "4400 completed. Loss: 3.906172154340893\n",
      "4600 completed. Loss: 3.9274332931358367\n",
      "4800 completed. Loss: 3.8450426992960276\n",
      "5000 completed. Loss: 3.6771454038098454\n",
      "5200 completed. Loss: 3.561819477993995\n",
      "5400 completed. Loss: 4.055559171289206\n",
      "5600 completed. Loss: 3.288136329036206\n",
      "5800 completed. Loss: 3.922817035331391\n",
      "6000 completed. Loss: 3.087519013546407\n",
      "6200 completed. Loss: 3.242923203725368\n",
      "6400 completed. Loss: 3.6768850379111244\n",
      "6600 completed. Loss: 3.272085878653452\n",
      "6800 completed. Loss: 3.0815142082609235\n",
      "7000 completed. Loss: 4.022476190887391\n",
      "7200 completed. Loss: 3.881860367404297\n",
      "7400 completed. Loss: 4.1652315904200075\n",
      "7600 completed. Loss: 3.816112539153546\n",
      "7800 completed. Loss: 3.7193523869989438\n",
      "8000 completed. Loss: 3.8462976848706605\n",
      "8200 completed. Loss: 3.9573858657106755\n",
      "8400 completed. Loss: 3.6074706536252052\n",
      "8600 completed. Loss: 3.7422120133973658\n",
      "8800 completed. Loss: 3.930668523083441\n",
      "9000 completed. Loss: 4.857529922295361\n",
      "9200 completed. Loss: 3.856771269617602\n",
      "200 completed. Loss: 5.0767170988209545\n",
      "400 completed. Loss: 3.851901564914733\n",
      "600 completed. Loss: 3.6402381692267953\n",
      "800 completed. Loss: 3.6769890203059186\n",
      "1000 completed. Loss: 3.448632401963696\n",
      "1200 completed. Loss: 3.4487531609088182\n",
      "1400 completed. Loss: 3.6911507477145644\n",
      "1600 completed. Loss: 3.378961602258496\n",
      "1800 completed. Loss: 3.7497926869336515\n",
      "2000 completed. Loss: 3.319091853364371\n",
      "2200 completed. Loss: 3.4368313682451843\n",
      "2400 completed. Loss: 3.2844408769253643\n",
      "2600 completed. Loss: 3.5061884134076537\n",
      "2800 completed. Loss: 3.4350728216348214\n",
      "3000 completed. Loss: 4.787036926308647\n",
      "3200 completed. Loss: 3.899216498127207\n",
      "3400 completed. Loss: 3.5990996818523855\n",
      "3600 completed. Loss: 3.871082607153803\n",
      "3800 completed. Loss: 4.095261943368241\n",
      "4000 completed. Loss: 4.069935455471278\n",
      "4200 completed. Loss: 3.395064018573612\n",
      "4400 completed. Loss: 3.9729221371852326\n",
      "4600 completed. Loss: 4.075274947304279\n",
      "4800 completed. Loss: 4.059331726673991\n",
      "5000 completed. Loss: 3.8081153370812535\n",
      "5200 completed. Loss: 3.489442562526092\n",
      "5400 completed. Loss: 3.5835473794303834\n",
      "5600 completed. Loss: 3.0402010120032354\n",
      "5800 completed. Loss: 3.5988481418043374\n",
      "6000 completed. Loss: 2.915621180580929\n",
      "6200 completed. Loss: 3.170563703584485\n",
      "6400 completed. Loss: 3.6668835385027343\n",
      "6600 completed. Loss: 3.2465600985521452\n",
      "6800 completed. Loss: 3.0449511423707007\n",
      "7000 completed. Loss: 4.107884055175819\n",
      "7200 completed. Loss: 3.9998854137025774\n",
      "7400 completed. Loss: 4.040244102063589\n",
      "7600 completed. Loss: 3.596520821414888\n",
      "7800 completed. Loss: 3.6505026711709796\n",
      "8000 completed. Loss: 3.8065954782813787\n",
      "8200 completed. Loss: 3.915485389556852\n",
      "8400 completed. Loss: 3.4831024651695044\n",
      "8600 completed. Loss: 3.668585620075464\n",
      "8800 completed. Loss: 3.845168676264584\n",
      "9000 completed. Loss: 4.73432433526963\n",
      "9200 completed. Loss: 3.730088025247678\n",
      "200 completed. Loss: 5.001817001011222\n",
      "400 completed. Loss: 4.043748770542443\n",
      "600 completed. Loss: 3.644430519081652\n",
      "800 completed. Loss: 3.625217419251567\n",
      "1000 completed. Loss: 3.406499150265008\n",
      "1200 completed. Loss: 3.3891636251099406\n",
      "1400 completed. Loss: 3.626074355971068\n",
      "1600 completed. Loss: 3.2988468044856565\n",
      "1800 completed. Loss: 3.6427851841319354\n",
      "2000 completed. Loss: 3.2515903515554965\n",
      "2200 completed. Loss: 3.367586855487898\n",
      "2400 completed. Loss: 3.186539911576547\n",
      "2600 completed. Loss: 3.3894580293726175\n",
      "2800 completed. Loss: 3.323652913435362\n",
      "3000 completed. Loss: 4.377221290869638\n",
      "3200 completed. Loss: 3.4906329663423823\n",
      "3400 completed. Loss: 3.4467922729835845\n",
      "3600 completed. Loss: 3.6494709649216386\n",
      "3800 completed. Loss: 4.25707885642536\n",
      "4000 completed. Loss: 3.9440931009221822\n",
      "4200 completed. Loss: 4.1453767521958795\n",
      "4400 completed. Loss: 3.8137329747341573\n",
      "4600 completed. Loss: 3.6894110521581025\n",
      "4800 completed. Loss: 3.8213238050881775\n",
      "5000 completed. Loss: 3.4530216422490776\n",
      "5200 completed. Loss: 3.2121277819760143\n",
      "5400 completed. Loss: 3.6195873988140375\n",
      "5600 completed. Loss: 2.751767304968089\n",
      "5800 completed. Loss: 3.4121107850037515\n",
      "6000 completed. Loss: 2.8842330475337805\n",
      "6200 completed. Loss: 3.127452907562256\n",
      "6400 completed. Loss: 3.1977318981289864\n",
      "6600 completed. Loss: 3.202503569340333\n",
      "6800 completed. Loss: 2.9636041226313683\n",
      "7000 completed. Loss: 4.137121610725298\n",
      "7200 completed. Loss: 3.684677013959736\n",
      "7400 completed. Loss: 4.146751996185631\n",
      "7600 completed. Loss: 3.8222066258080303\n",
      "7800 completed. Loss: 3.721337115727365\n",
      "8000 completed. Loss: 3.721934980079532\n",
      "8200 completed. Loss: 3.832847696268\n",
      "8400 completed. Loss: 3.3737588576227426\n",
      "8600 completed. Loss: 3.54020627014339\n",
      "8800 completed. Loss: 3.6615581756597386\n",
      "9000 completed. Loss: 4.387289525792003\n",
      "9200 completed. Loss: 3.5615307567315178\n",
      "200 completed. Loss: 5.03117184096016\n",
      "400 completed. Loss: 3.9721851858776063\n",
      "600 completed. Loss: 3.737754246755503\n",
      "800 completed. Loss: 3.5503155514760874\n",
      "1000 completed. Loss: 3.2749876180291175\n",
      "1200 completed. Loss: 3.266150718219578\n",
      "1400 completed. Loss: 3.5029649455845355\n",
      "1600 completed. Loss: 3.174173604566604\n",
      "1800 completed. Loss: 3.4801426427066326\n",
      "2000 completed. Loss: 3.1176521812193094\n",
      "2200 completed. Loss: 3.239168779645115\n",
      "2400 completed. Loss: 3.0038204469438643\n",
      "2600 completed. Loss: 3.4097522624372503\n",
      "2800 completed. Loss: 3.2729863750655204\n",
      "3000 completed. Loss: 4.273993988623842\n",
      "3200 completed. Loss: 3.4606469270586966\n",
      "3400 completed. Loss: 3.3859921825421044\n",
      "3600 completed. Loss: 3.5634783609211444\n",
      "3800 completed. Loss: 4.192708074245602\n",
      "4000 completed. Loss: 3.9256557457707824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200 completed. Loss: 4.126460027815774\n",
      "4400 completed. Loss: 3.8787734283320607\n",
      "4600 completed. Loss: 3.750051874599885\n",
      "4800 completed. Loss: 4.002311382647604\n",
      "5000 completed. Loss: 3.5561816980643197\n",
      "5200 completed. Loss: 3.44495613518171\n",
      "5400 completed. Loss: 3.15101816708222\n",
      "5600 completed. Loss: 2.763176950532943\n",
      "5800 completed. Loss: 3.4896498231356965\n",
      "6000 completed. Loss: 3.157082067336887\n",
      "6200 completed. Loss: 3.145166535368189\n",
      "6400 completed. Loss: 3.8718348454125224\n",
      "6600 completed. Loss: 3.3718437572568654\n",
      "6800 completed. Loss: 3.137181056048721\n",
      "7000 completed. Loss: 3.7532712786644695\n",
      "7200 completed. Loss: 3.5668092641234397\n",
      "7400 completed. Loss: 3.560485175549984\n",
      "7600 completed. Loss: 3.267474735341966\n",
      "7800 completed. Loss: 3.49616450981237\n",
      "8000 completed. Loss: 3.856033023223281\n",
      "8200 completed. Loss: 3.834920377098024\n",
      "8400 completed. Loss: 3.491783110466786\n",
      "8600 completed. Loss: 3.5636717633064836\n",
      "8800 completed. Loss: 3.630671450421214\n",
      "9000 completed. Loss: 4.238497195681557\n",
      "9200 completed. Loss: 3.620819384707138\n",
      "200 completed. Loss: 5.036706984620541\n",
      "400 completed. Loss: 4.047624230328947\n",
      "600 completed. Loss: 3.3896859838906677\n",
      "800 completed. Loss: 3.4083088340214456\n",
      "1000 completed. Loss: 3.256594817843288\n",
      "1200 completed. Loss: 3.216480402313173\n",
      "1400 completed. Loss: 3.4593893597368153\n",
      "1600 completed. Loss: 3.1226645384635776\n",
      "1800 completed. Loss: 3.416218492318876\n",
      "2000 completed. Loss: 3.0897278881072996\n",
      "2200 completed. Loss: 3.194023457933217\n",
      "2400 completed. Loss: 2.937936395024881\n",
      "2600 completed. Loss: 3.2411083578760738\n",
      "2800 completed. Loss: 3.111297886171378\n",
      "3000 completed. Loss: 3.9596268559247254\n",
      "3200 completed. Loss: 3.1694746731081977\n",
      "3400 completed. Loss: 3.1841116479877383\n",
      "3600 completed. Loss: 3.8231167210265995\n",
      "3800 completed. Loss: 4.437661510184407\n",
      "4000 completed. Loss: 3.9549310849327592\n",
      "4200 completed. Loss: 3.945495459083468\n",
      "4400 completed. Loss: 3.8254591815918686\n",
      "4600 completed. Loss: 3.6502954068174587\n",
      "4800 completed. Loss: 3.8366309821978213\n",
      "5000 completed. Loss: 3.363440177482553\n",
      "5200 completed. Loss: 3.252747820969671\n",
      "5400 completed. Loss: 2.99123560001608\n",
      "5600 completed. Loss: 2.6244214829802512\n",
      "5800 completed. Loss: 3.3628835377423094\n",
      "6000 completed. Loss: 3.030215795543045\n",
      "6200 completed. Loss: 2.8932303719222547\n",
      "6400 completed. Loss: 3.557753319628537\n",
      "6600 completed. Loss: 2.924517067661509\n",
      "6800 completed. Loss: 2.733572399234399\n",
      "7000 completed. Loss: 3.8872442399617286\n",
      "7200 completed. Loss: 3.4548808437213303\n",
      "7400 completed. Loss: 4.1192673527728765\n",
      "7600 completed. Loss: 3.8578729239106178\n",
      "7800 completed. Loss: 3.714476643009111\n",
      "8000 completed. Loss: 3.623418333809823\n",
      "8200 completed. Loss: 3.7079757470590993\n",
      "8400 completed. Loss: 3.2231451970525087\n",
      "8600 completed. Loss: 3.421053724568337\n",
      "8800 completed. Loss: 3.519946304121986\n",
      "9000 completed. Loss: 4.016340692788362\n",
      "9200 completed. Loss: 3.329157515941188\n",
      "200 completed. Loss: 4.78098616217263\n",
      "400 completed. Loss: 3.8979850097559394\n",
      "600 completed. Loss: 3.7245762237813325\n",
      "800 completed. Loss: 3.518495995022822\n",
      "1000 completed. Loss: 3.1398408827371895\n",
      "1200 completed. Loss: 3.116337958201766\n",
      "1400 completed. Loss: 3.369410219588317\n",
      "1600 completed. Loss: 3.019195262864232\n",
      "1800 completed. Loss: 3.2902363448310643\n",
      "2000 completed. Loss: 2.9853230188693853\n",
      "2200 completed. Loss: 3.1133821765612812\n",
      "2400 completed. Loss: 2.819475823792163\n",
      "2600 completed. Loss: 3.2252551687543747\n",
      "2800 completed. Loss: 3.04130753508769\n",
      "3000 completed. Loss: 3.8827338307909667\n",
      "3200 completed. Loss: 3.16164645077195\n",
      "3400 completed. Loss: 3.119067388579715\n",
      "3600 completed. Loss: 3.723841520138085\n",
      "3800 completed. Loss: 3.8912454660981894\n",
      "4000 completed. Loss: 3.6145105102611708\n",
      "4200 completed. Loss: 3.443585071749985\n",
      "4400 completed. Loss: 3.274211174855009\n",
      "4600 completed. Loss: 3.3723904717480764\n",
      "4800 completed. Loss: 3.566803208431229\n",
      "5000 completed. Loss: 3.1408285664208235\n",
      "5200 completed. Loss: 3.017407270523254\n",
      "5400 completed. Loss: 3.0500377540057526\n",
      "5600 completed. Loss: 2.4489616869576274\n",
      "5800 completed. Loss: 3.364788540985901\n",
      "6000 completed. Loss: 2.8120419168006627\n",
      "6200 completed. Loss: 2.9494871393963695\n",
      "6400 completed. Loss: 2.907841861611232\n",
      "6600 completed. Loss: 2.886595537196845\n",
      "6800 completed. Loss: 2.8703735139220954\n",
      "7000 completed. Loss: 3.8812558237463235\n",
      "7200 completed. Loss: 3.62694537143223\n",
      "7400 completed. Loss: 4.0790857532079094\n",
      "7600 completed. Loss: 3.6659015877591448\n",
      "7800 completed. Loss: 3.3924959039874376\n",
      "8000 completed. Loss: 3.542354830279946\n",
      "8200 completed. Loss: 3.6029818982229336\n",
      "8400 completed. Loss: 3.130349323377013\n",
      "8600 completed. Loss: 3.3069834145344794\n",
      "8800 completed. Loss: 3.3136933973664418\n",
      "9000 completed. Loss: 3.818171905297786\n",
      "9200 completed. Loss: 3.225075986580923\n",
      "200 completed. Loss: 4.756770028215833\n",
      "400 completed. Loss: 3.9444394414126873\n",
      "600 completed. Loss: 3.7167544976249336\n",
      "800 completed. Loss: 3.4141911249398254\n",
      "1000 completed. Loss: 3.076138613373041\n",
      "1200 completed. Loss: 3.040413362234831\n",
      "1400 completed. Loss: 3.284815318781184\n",
      "1600 completed. Loss: 2.9346036457852462\n",
      "1800 completed. Loss: 3.1889166064257735\n",
      "2000 completed. Loss: 2.9170747134974224\n",
      "2200 completed. Loss: 3.0475999370682985\n",
      "2400 completed. Loss: 2.7280025905685035\n",
      "2600 completed. Loss: 3.1028367447294296\n",
      "2800 completed. Loss: 2.9555301758972927\n",
      "3000 completed. Loss: 3.825211939336732\n",
      "3200 completed. Loss: 3.1674178836494686\n",
      "3400 completed. Loss: 3.0670030857983512\n",
      "3600 completed. Loss: 3.4449562295246867\n",
      "3800 completed. Loss: 3.642017595805228\n",
      "4000 completed. Loss: 3.513978000837378\n",
      "4200 completed. Loss: 2.868709860118106\n",
      "4400 completed. Loss: 3.8272580226324497\n",
      "4600 completed. Loss: 3.286434611286968\n",
      "4800 completed. Loss: 3.419337451718748\n",
      "5000 completed. Loss: 3.0725787666626276\n",
      "5200 completed. Loss: 2.930455999118276\n",
      "5400 completed. Loss: 3.0409706484759225\n",
      "5600 completed. Loss: 2.4365787522587925\n",
      "5800 completed. Loss: 3.255537901716307\n",
      "6000 completed. Loss: 2.858210511300713\n",
      "6200 completed. Loss: 2.763726261490956\n",
      "6400 completed. Loss: 2.8706018928764387\n",
      "6600 completed. Loss: 2.7843342119641603\n",
      "6800 completed. Loss: 2.8475220538070425\n",
      "7000 completed. Loss: 3.888259798549116\n",
      "7200 completed. Loss: 3.5951356984395533\n",
      "7400 completed. Loss: 4.030719207632355\n",
      "7600 completed. Loss: 3.5553185573779045\n",
      "7800 completed. Loss: 3.3056476258393377\n",
      "8000 completed. Loss: 3.48266072222963\n",
      "8200 completed. Loss: 3.544084347584867\n",
      "8400 completed. Loss: 3.056186681385152\n",
      "8600 completed. Loss: 3.2430138538219033\n",
      "8800 completed. Loss: 3.2483271780004723\n",
      "9000 completed. Loss: 3.691896376782097\n",
      "9200 completed. Loss: 3.1646735525503753\n",
      "200 completed. Loss: 4.584208434834145\n",
      "400 completed. Loss: 3.9884424725174905\n",
      "600 completed. Loss: 3.7116231537703426\n",
      "800 completed. Loss: 3.369645939306356\n",
      "1000 completed. Loss: 3.0294405737053602\n",
      "1200 completed. Loss: 2.980262674540281\n",
      "1400 completed. Loss: 3.230843029797543\n",
      "1600 completed. Loss: 2.8662363915890454\n",
      "1800 completed. Loss: 3.1164631449617444\n",
      "2000 completed. Loss: 2.879676603162661\n",
      "2200 completed. Loss: 3.0020033749728463\n",
      "2400 completed. Loss: 2.674428994759219\n",
      "2600 completed. Loss: 2.957872906876728\n",
      "2800 completed. Loss: 2.7632031736383214\n",
      "3000 completed. Loss: 3.707911075819284\n",
      "3200 completed. Loss: 3.098681900333613\n",
      "3400 completed. Loss: 3.018368722652085\n",
      "3600 completed. Loss: 3.4862471971753983\n",
      "3800 completed. Loss: 3.545184821924195\n",
      "4000 completed. Loss: 3.4645536845363676\n",
      "4200 completed. Loss: 2.857243018075824\n",
      "4400 completed. Loss: 3.539194544237107\n",
      "4600 completed. Loss: 3.2030884301662446\n",
      "4800 completed. Loss: 3.379450864875689\n",
      "5000 completed. Loss: 3.022838102709502\n",
      "5200 completed. Loss: 2.8659642096422613\n",
      "5400 completed. Loss: 2.9493173041427507\n",
      "5600 completed. Loss: 2.380189510025084\n",
      "5800 completed. Loss: 3.262393539156765\n",
      "6000 completed. Loss: 2.834656540658325\n",
      "6200 completed. Loss: 2.724170256303623\n",
      "6400 completed. Loss: 2.8327705871127544\n",
      "6600 completed. Loss: 2.7346609020791948\n",
      "6800 completed. Loss: 2.82056098992005\n",
      "7000 completed. Loss: 3.6963340343907474\n",
      "7200 completed. Loss: 3.491015734784305\n",
      "7400 completed. Loss: 3.9843497393885627\n",
      "7600 completed. Loss: 3.569947067340836\n",
      "7800 completed. Loss: 3.3217909673042594\n",
      "8000 completed. Loss: 3.437581218276173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8200 completed. Loss: 3.5006874678848545\n",
      "8400 completed. Loss: 2.992058396865614\n",
      "8600 completed. Loss: 3.1939380041323604\n",
      "8800 completed. Loss: 3.200878359666094\n",
      "9000 completed. Loss: 3.6093006876762956\n",
      "9200 completed. Loss: 3.121409763228148\n",
      "200 completed. Loss: 4.352761319582351\n",
      "400 completed. Loss: 3.9777548789046704\n",
      "600 completed. Loss: 3.599147471068427\n",
      "800 completed. Loss: 3.2866042795963586\n",
      "1000 completed. Loss: 2.9905560490861536\n",
      "1200 completed. Loss: 2.927293119505048\n",
      "1400 completed. Loss: 3.185383331924677\n",
      "1600 completed. Loss: 2.8100661590881644\n",
      "1800 completed. Loss: 3.0643645785422997\n",
      "2000 completed. Loss: 2.845115140359849\n",
      "2200 completed. Loss: 2.9682305179326796\n",
      "2400 completed. Loss: 2.635009273844771\n",
      "2600 completed. Loss: 2.894620020161383\n",
      "2800 completed. Loss: 2.6139055352518334\n",
      "3000 completed. Loss: 3.6419708914123476\n",
      "3200 completed. Loss: 3.086799594014883\n",
      "3400 completed. Loss: 2.984577136905864\n",
      "3600 completed. Loss: 3.3665347717609255\n",
      "3800 completed. Loss: 3.4649235292710365\n",
      "4000 completed. Loss: 3.4396905008517207\n",
      "4200 completed. Loss: 2.7935291936947033\n",
      "4400 completed. Loss: 3.416873183250427\n",
      "4600 completed. Loss: 3.157996331974864\n",
      "4800 completed. Loss: 3.364484222084284\n",
      "5000 completed. Loss: 2.9909294224716723\n",
      "5200 completed. Loss: 2.8246422802703455\n",
      "5400 completed. Loss: 2.9392569036828355\n",
      "5600 completed. Loss: 2.364091439507902\n",
      "5800 completed. Loss: 3.20209334035404\n",
      "6000 completed. Loss: 2.8385456227511168\n",
      "6200 completed. Loss: 2.6596767937764527\n",
      "6400 completed. Loss: 2.8353024652402383\n",
      "6600 completed. Loss: 2.6775868108775467\n",
      "6800 completed. Loss: 2.793101575598121\n",
      "7000 completed. Loss: 3.5164147281087934\n",
      "7200 completed. Loss: 3.3611678458424286\n",
      "7400 completed. Loss: 3.912093194150366\n",
      "7600 completed. Loss: 3.544212863976136\n",
      "7800 completed. Loss: 3.3157845747284593\n",
      "8000 completed. Loss: 3.3961853316798805\n",
      "8200 completed. Loss: 3.4542094234417893\n",
      "8400 completed. Loss: 2.9279121933877468\n",
      "8600 completed. Loss: 3.1397474084142596\n",
      "8800 completed. Loss: 3.1393395254435017\n",
      "9000 completed. Loss: 3.5130008951260243\n",
      "9200 completed. Loss: 3.0864235436264424\n",
      "200 completed. Loss: 4.1673560420470315\n",
      "400 completed. Loss: 3.927576215732843\n",
      "600 completed. Loss: 3.4744859833689405\n",
      "800 completed. Loss: 3.232842286936939\n",
      "1000 completed. Loss: 2.9464611527416853\n",
      "1200 completed. Loss: 2.8769937874749303\n",
      "1400 completed. Loss: 3.144628840261139\n",
      "1600 completed. Loss: 2.754488375876099\n",
      "1800 completed. Loss: 3.0122070156759584\n",
      "2000 completed. Loss: 2.809000316048041\n",
      "2200 completed. Loss: 2.93878637725953\n",
      "2400 completed. Loss: 2.5942155870376156\n",
      "2600 completed. Loss: 2.86143697605934\n",
      "2800 completed. Loss: 2.515482386602089\n",
      "3000 completed. Loss: 3.5716210448741914\n",
      "3200 completed. Loss: 3.067088279612362\n",
      "3400 completed. Loss: 2.9513508346490562\n",
      "3600 completed. Loss: 3.263380078282207\n",
      "3800 completed. Loss: 3.4195177603699265\n",
      "4000 completed. Loss: 3.4092896418459713\n",
      "4200 completed. Loss: 2.7759640384605153\n",
      "4400 completed. Loss: 3.2656164616160095\n",
      "4600 completed. Loss: 3.1666184405982496\n",
      "4800 completed. Loss: 3.355267089381814\n",
      "5000 completed. Loss: 2.9633229995524744\n",
      "5200 completed. Loss: 2.7885487655876204\n",
      "5400 completed. Loss: 2.942387727017049\n",
      "5600 completed. Loss: 2.37375289763324\n",
      "5800 completed. Loss: 3.123098920173943\n",
      "6000 completed. Loss: 2.779246615981683\n",
      "6200 completed. Loss: 2.6250001170672475\n",
      "6400 completed. Loss: 2.8620441324822603\n",
      "6600 completed. Loss: 2.630124331554398\n",
      "6800 completed. Loss: 2.749227881412953\n",
      "7000 completed. Loss: 3.3688190820626915\n",
      "7200 completed. Loss: 3.234305020510219\n",
      "7400 completed. Loss: 3.8214709225064145\n",
      "7600 completed. Loss: 3.47897034952417\n",
      "7800 completed. Loss: 3.3318176369555292\n",
      "8000 completed. Loss: 3.3619864518940448\n",
      "8200 completed. Loss: 3.4148470213543622\n",
      "8400 completed. Loss: 2.869191641882062\n",
      "8600 completed. Loss: 3.0854949149582533\n",
      "8800 completed. Loss: 3.0754948055231943\n",
      "9000 completed. Loss: 3.4032980658952146\n",
      "9200 completed. Loss: 3.0458684850391\n",
      "200 completed. Loss: 4.031911299219355\n",
      "400 completed. Loss: 3.846285952590406\n",
      "600 completed. Loss: 3.373625474022701\n",
      "800 completed. Loss: 3.2093097332306204\n",
      "1000 completed. Loss: 2.9102024852577597\n",
      "1200 completed. Loss: 2.833285238146782\n",
      "1400 completed. Loss: 3.101047713109292\n",
      "1600 completed. Loss: 2.699462997438386\n",
      "1800 completed. Loss: 2.95961667987518\n",
      "2000 completed. Loss: 2.7746883288398383\n",
      "2200 completed. Loss: 2.911512178923003\n",
      "2400 completed. Loss: 2.555416356776841\n",
      "2600 completed. Loss: 2.8433935654722156\n",
      "2800 completed. Loss: 2.456345499642193\n",
      "3000 completed. Loss: 3.5048333197273314\n",
      "3200 completed. Loss: 3.038817600393668\n",
      "3400 completed. Loss: 2.9245179389975964\n",
      "3600 completed. Loss: 3.193396624377929\n",
      "3800 completed. Loss: 3.4084940008074045\n",
      "4000 completed. Loss: 3.376569901946932\n",
      "4200 completed. Loss: 2.8235759696154856\n",
      "4400 completed. Loss: 3.0877293262351304\n",
      "4600 completed. Loss: 3.183957959935069\n",
      "4800 completed. Loss: 3.3433355349488556\n",
      "5000 completed. Loss: 2.9413833742123097\n",
      "5200 completed. Loss: 2.7554435042338445\n",
      "5400 completed. Loss: 2.944919841438532\n",
      "5600 completed. Loss: 2.4068499933183194\n",
      "5800 completed. Loss: 3.0590929627325387\n",
      "6000 completed. Loss: 2.6153905696142465\n",
      "6200 completed. Loss: 2.606925504403189\n",
      "6400 completed. Loss: 2.851631400610786\n",
      "6600 completed. Loss: 2.6013723494671286\n",
      "6800 completed. Loss: 2.7156845656596125\n",
      "7000 completed. Loss: 3.2683196917176245\n",
      "7200 completed. Loss: 3.13992447483819\n",
      "7400 completed. Loss: 3.7236332143750044\n",
      "7600 completed. Loss: 3.3837140009738507\n",
      "7800 completed. Loss: 3.285073865093291\n",
      "8000 completed. Loss: 3.333163652140647\n",
      "8200 completed. Loss: 3.379165195239475\n",
      "8400 completed. Loss: 2.8183970613079143\n",
      "8600 completed. Loss: 3.04290564998053\n",
      "8800 completed. Loss: 3.0238770920457316\n",
      "9000 completed. Loss: 3.328447989949491\n",
      "9200 completed. Loss: 3.0217696698755026\n",
      "200 completed. Loss: 3.9398185963463037\n",
      "400 completed. Loss: 3.7658296678215266\n",
      "600 completed. Loss: 3.2491949228290467\n",
      "800 completed. Loss: 3.1950640604831277\n",
      "1000 completed. Loss: 2.8846723243873567\n",
      "1200 completed. Loss: 2.799025558941066\n",
      "1400 completed. Loss: 3.070240827174857\n",
      "1600 completed. Loss: 2.6565991236735136\n",
      "1800 completed. Loss: 2.9253368342947215\n",
      "2000 completed. Loss: 2.7526567939296367\n",
      "2200 completed. Loss: 2.8904270003549755\n",
      "2400 completed. Loss: 2.527002393323928\n",
      "2600 completed. Loss: 2.828763164561242\n",
      "2800 completed. Loss: 2.4073129154881463\n",
      "3000 completed. Loss: 3.4408647424727676\n",
      "3200 completed. Loss: 2.982595584737137\n",
      "3400 completed. Loss: 2.8861090652644634\n",
      "3600 completed. Loss: 3.163246143939905\n",
      "3800 completed. Loss: 3.379046539817937\n",
      "4000 completed. Loss: 3.3533576344605533\n",
      "4200 completed. Loss: 2.8352520579006524\n",
      "4400 completed. Loss: 2.985028591649607\n",
      "4600 completed. Loss: 3.1595770068466664\n",
      "4800 completed. Loss: 3.3271146849915385\n",
      "5000 completed. Loss: 2.9227131819352508\n",
      "5200 completed. Loss: 2.727882082615979\n",
      "5400 completed. Loss: 2.929391782486346\n",
      "5600 completed. Loss: 2.426338011668995\n",
      "5800 completed. Loss: 3.0343858476076275\n",
      "6000 completed. Loss: 2.4930206295661628\n",
      "6200 completed. Loss: 2.595294736633077\n",
      "6400 completed. Loss: 2.7931744472577704\n",
      "6600 completed. Loss: 2.5814966282434764\n",
      "6800 completed. Loss: 2.679792221188545\n",
      "7000 completed. Loss: 3.163724164608866\n",
      "7200 completed. Loss: 3.0347305255383255\n",
      "7400 completed. Loss: 3.629372450262308\n",
      "7600 completed. Loss: 3.3027438147179784\n",
      "7800 completed. Loss: 3.2738059022836388\n",
      "8000 completed. Loss: 3.3199454908259214\n",
      "8200 completed. Loss: 3.3475805752875747\n",
      "8400 completed. Loss: 2.7735991631704384\n",
      "8600 completed. Loss: 3.0051743997726588\n",
      "8800 completed. Loss: 2.9841849394654854\n",
      "9000 completed. Loss: 3.2622633402748034\n",
      "9200 completed. Loss: 3.0085689994227143\n",
      "200 completed. Loss: 3.8610337537992745\n",
      "400 completed. Loss: 3.631729434137233\n",
      "600 completed. Loss: 3.1063655336387455\n",
      "800 completed. Loss: 3.1892515497282146\n",
      "1000 completed. Loss: 2.862711012365762\n",
      "1200 completed. Loss: 2.769050239305943\n",
      "1400 completed. Loss: 3.0417481287103145\n",
      "1600 completed. Loss: 2.6174194873962553\n",
      "1800 completed. Loss: 2.8910707183694466\n",
      "2000 completed. Loss: 2.7310975307412444\n",
      "2200 completed. Loss: 2.8714865596964954\n",
      "2400 completed. Loss: 2.495702425176278\n",
      "2600 completed. Loss: 2.8182803065143527\n",
      "2800 completed. Loss: 2.382025234771427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 completed. Loss: 3.403111710175872\n",
      "3200 completed. Loss: 2.9482610855204983\n",
      "3400 completed. Loss: 2.847411224339157\n",
      "3600 completed. Loss: 3.1340811115549876\n",
      "3800 completed. Loss: 3.3470364409917965\n",
      "4000 completed. Loss: 3.334179737335071\n",
      "4200 completed. Loss: 2.854952356686117\n",
      "4400 completed. Loss: 2.9098367112874985\n",
      "4600 completed. Loss: 3.0866112653166056\n",
      "4800 completed. Loss: 3.3182288779318334\n",
      "5000 completed. Loss: 2.9087535395193846\n",
      "5200 completed. Loss: 2.704358902005479\n",
      "5400 completed. Loss: 2.9151013012090696\n",
      "5600 completed. Loss: 2.4320219538733365\n",
      "5800 completed. Loss: 3.0216051543317737\n",
      "6000 completed. Loss: 2.429975641099736\n",
      "6200 completed. Loss: 2.596383170299232\n",
      "6400 completed. Loss: 2.71364755803952\n",
      "6600 completed. Loss: 2.570519579378888\n",
      "6800 completed. Loss: 2.654107819898054\n",
      "7000 completed. Loss: 3.0144056958239527\n",
      "7200 completed. Loss: 2.8848910283623264\n",
      "7400 completed. Loss: 3.5110609796317296\n",
      "7600 completed. Loss: 3.202046747957356\n",
      "7800 completed. Loss: 3.349667370216921\n",
      "8000 completed. Loss: 3.331713815331459\n",
      "8200 completed. Loss: 3.320965661481023\n",
      "8400 completed. Loss: 2.73619520048378\n",
      "8600 completed. Loss: 2.9738231576862746\n",
      "8800 completed. Loss: 2.9552100282302125\n",
      "9000 completed. Loss: 3.213298548129387\n",
      "9200 completed. Loss: 3.0032742153294385\n",
      "200 completed. Loss: 3.790935046887025\n",
      "400 completed. Loss: 3.443717340212315\n",
      "600 completed. Loss: 2.985044908830896\n",
      "800 completed. Loss: 3.185808900576085\n",
      "1000 completed. Loss: 2.841208018611651\n",
      "1200 completed. Loss: 2.747424291353673\n",
      "1400 completed. Loss: 3.022541699167341\n",
      "1600 completed. Loss: 2.5838927964959293\n",
      "1800 completed. Loss: 2.8643665092159063\n",
      "2000 completed. Loss: 2.711110517308116\n",
      "2200 completed. Loss: 2.857563893273473\n",
      "2400 completed. Loss: 2.4698063824139536\n",
      "2600 completed. Loss: 2.810424317494035\n",
      "2800 completed. Loss: 2.365682589376811\n",
      "3000 completed. Loss: 3.3722598695196213\n",
      "3200 completed. Loss: 2.9317951973807066\n",
      "3400 completed. Loss: 2.818680252339691\n",
      "3600 completed. Loss: 3.1104017322417348\n",
      "3800 completed. Loss: 3.3132702871784567\n",
      "4000 completed. Loss: 3.3161046417523177\n",
      "4200 completed. Loss: 2.8376270691317043\n",
      "4400 completed. Loss: 2.876528577208519\n",
      "4600 completed. Loss: 3.028920203000307\n",
      "4800 completed. Loss: 3.309912460818887\n",
      "5000 completed. Loss: 2.8965255695115775\n",
      "5200 completed. Loss: 2.68365045374725\n",
      "5400 completed. Loss: 2.9000325376400724\n",
      "5600 completed. Loss: 2.4472474473156036\n",
      "5800 completed. Loss: 3.023483421057463\n",
      "6000 completed. Loss: 2.3949431265331804\n",
      "6200 completed. Loss: 2.629445254551247\n",
      "6400 completed. Loss: 2.6464835894212593\n",
      "6600 completed. Loss: 2.6082080491259694\n",
      "6800 completed. Loss: 2.6850091064861044\n",
      "7000 completed. Loss: 2.7872310572396963\n",
      "7200 completed. Loss: 2.5722794231399893\n",
      "7400 completed. Loss: 2.979934654831886\n",
      "7600 completed. Loss: 2.7867777197994292\n",
      "7800 completed. Loss: 3.1908174016745763\n",
      "8000 completed. Loss: 3.7169231247808785\n",
      "8200 completed. Loss: 3.4455642072856425\n",
      "8400 completed. Loss: 2.875813997283112\n",
      "8600 completed. Loss: 2.9748802863247694\n",
      "8800 completed. Loss: 2.8805033869156613\n",
      "9000 completed. Loss: 3.166788487433223\n",
      "9200 completed. Loss: 3.043961779847741\n",
      "200 completed. Loss: 3.8323671460617335\n",
      "400 completed. Loss: 3.2098324919492005\n",
      "600 completed. Loss: 2.873581165317446\n",
      "800 completed. Loss: 3.192374289855361\n",
      "1000 completed. Loss: 2.8254657099372706\n",
      "1200 completed. Loss: 2.7303191180713475\n",
      "1400 completed. Loss: 3.001204636944458\n",
      "1600 completed. Loss: 2.5528878772817554\n",
      "1800 completed. Loss: 2.837724752749782\n",
      "2000 completed. Loss: 2.6943014710210265\n",
      "2200 completed. Loss: 2.8440999921387995\n",
      "2400 completed. Loss: 2.4487659869808702\n",
      "2600 completed. Loss: 2.8078407196700574\n",
      "2800 completed. Loss: 2.349461502307095\n",
      "3000 completed. Loss: 3.340684098638594\n",
      "3200 completed. Loss: 2.9160128333489412\n",
      "3400 completed. Loss: 2.790289320088923\n",
      "3600 completed. Loss: 3.0931129542039706\n",
      "3800 completed. Loss: 3.253192199449986\n",
      "4000 completed. Loss: 3.2964080274943264\n",
      "4200 completed. Loss: 2.748628505994566\n",
      "4400 completed. Loss: 2.856955882338807\n",
      "4600 completed. Loss: 3.037511089993641\n",
      "4800 completed. Loss: 3.307459637336433\n",
      "5000 completed. Loss: 2.894717104777228\n",
      "5200 completed. Loss: 2.672373761066701\n",
      "5400 completed. Loss: 2.9041973436577244\n",
      "5600 completed. Loss: 2.489552624048665\n",
      "5800 completed. Loss: 3.0568606725917196\n",
      "6000 completed. Loss: 2.405256894007325\n",
      "6200 completed. Loss: 2.649691076572053\n",
      "6400 completed. Loss: 2.674077147427015\n",
      "6600 completed. Loss: 2.6118615340138787\n",
      "6800 completed. Loss: 2.497802655790001\n",
      "7000 completed. Loss: 2.7673052630154418\n",
      "7200 completed. Loss: 2.547563388790004\n",
      "7400 completed. Loss: 2.9251590163167567\n",
      "7600 completed. Loss: 2.6811060862382874\n",
      "7800 completed. Loss: 3.1108121716044845\n",
      "8000 completed. Loss: 3.6475091288704427\n",
      "8200 completed. Loss: 3.3967270812857895\n",
      "8400 completed. Loss: 2.735766360386042\n",
      "8600 completed. Loss: 2.9130206542275845\n",
      "8800 completed. Loss: 2.837041177097708\n",
      "9000 completed. Loss: 3.0972430240060205\n",
      "9200 completed. Loss: 2.9969736041128634\n",
      "200 completed. Loss: 3.7410585272219032\n",
      "400 completed. Loss: 3.1479304467421025\n",
      "600 completed. Loss: 2.855698508191854\n",
      "800 completed. Loss: 3.215095457043499\n",
      "1000 completed. Loss: 2.811077163759619\n",
      "1200 completed. Loss: 2.7148705047182737\n",
      "1400 completed. Loss: 2.983228862257674\n",
      "1600 completed. Loss: 2.524259921172634\n",
      "1800 completed. Loss: 2.8172113850468303\n",
      "2000 completed. Loss: 2.6831011115014554\n",
      "2200 completed. Loss: 2.832637760329526\n",
      "2400 completed. Loss: 2.431234363983385\n",
      "2600 completed. Loss: 2.8051789879612623\n",
      "2800 completed. Loss: 2.336948082195595\n",
      "3000 completed. Loss: 3.3153575176000594\n",
      "3200 completed. Loss: 2.8964498305972666\n",
      "3400 completed. Loss: 2.7639646346494557\n",
      "3600 completed. Loss: 3.0862347169360147\n",
      "3800 completed. Loss: 3.1709020797442644\n",
      "4000 completed. Loss: 3.2926581943035127\n",
      "4200 completed. Loss: 2.600411744983867\n",
      "4400 completed. Loss: 2.8946412288118153\n",
      "4600 completed. Loss: 3.141209260020405\n",
      "4800 completed. Loss: 3.380674459915608\n",
      "5000 completed. Loss: 3.015915232617408\n",
      "5200 completed. Loss: 2.7812215230893345\n",
      "5400 completed. Loss: 3.0098766868328677\n",
      "5600 completed. Loss: 2.5508409702591597\n",
      "5800 completed. Loss: 3.0549425652809443\n",
      "6000 completed. Loss: 2.403968876749277\n",
      "6200 completed. Loss: 2.572638709284365\n",
      "6400 completed. Loss: 2.6959497154061682\n",
      "6600 completed. Loss: 2.5913101698085668\n",
      "6800 completed. Loss: 2.4502991260937415\n",
      "7000 completed. Loss: 2.8881357618514447\n",
      "7200 completed. Loss: 2.6383621856663377\n",
      "7400 completed. Loss: 3.02048534469679\n",
      "7600 completed. Loss: 2.6864331447705627\n",
      "7800 completed. Loss: 2.9214912286773322\n",
      "8000 completed. Loss: 3.396595072473865\n",
      "8200 completed. Loss: 3.2600502599682657\n",
      "8400 completed. Loss: 2.594986899890937\n",
      "8600 completed. Loss: 2.859155490947887\n",
      "8800 completed. Loss: 2.7707310767960736\n",
      "9000 completed. Loss: 2.9961873358581217\n",
      "9200 completed. Loss: 2.917521570324898\n",
      "200 completed. Loss: 3.6415279143769292\n",
      "400 completed. Loss: 3.1937649741955103\n",
      "600 completed. Loss: 2.8667588063981384\n",
      "800 completed. Loss: 3.2487130515277385\n",
      "1000 completed. Loss: 2.8028388431179336\n",
      "1200 completed. Loss: 2.698226342750713\n",
      "1400 completed. Loss: 2.9651921164803205\n",
      "1600 completed. Loss: 2.4970237454399467\n",
      "1800 completed. Loss: 2.7959820205077994\n",
      "2000 completed. Loss: 2.6752704598382113\n",
      "2200 completed. Loss: 2.8242371333786287\n",
      "2400 completed. Loss: 2.412779328697361\n",
      "2600 completed. Loss: 2.803144862316549\n",
      "2800 completed. Loss: 2.327399675445631\n",
      "3000 completed. Loss: 3.2968046747334303\n",
      "3200 completed. Loss: 2.8857896313327362\n",
      "3400 completed. Loss: 2.7484596362523734\n",
      "3600 completed. Loss: 3.078052974063903\n",
      "3800 completed. Loss: 3.1151701961923393\n",
      "4000 completed. Loss: 3.310173846501857\n",
      "4200 completed. Loss: 2.505944916224107\n",
      "4400 completed. Loss: 2.964618380283937\n",
      "4600 completed. Loss: 3.2389364968100565\n",
      "4800 completed. Loss: 3.5030128012783823\n",
      "5000 completed. Loss: 3.11789045589976\n",
      "5200 completed. Loss: 2.7626986573822796\n",
      "5400 completed. Loss: 2.7013008461426944\n",
      "5600 completed. Loss: 2.2593049396947027\n",
      "5800 completed. Loss: 2.9054589765681884\n",
      "6000 completed. Loss: 2.430817099986598\n",
      "6200 completed. Loss: 2.5702236181497575\n",
      "6400 completed. Loss: 2.6903129041567446\n",
      "6600 completed. Loss: 2.5934524328447877\n",
      "6800 completed. Loss: 2.459998439301271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 completed. Loss: 2.906378086162731\n",
      "7200 completed. Loss: 2.635761423818767\n",
      "7400 completed. Loss: 2.985149060431868\n",
      "7600 completed. Loss: 2.632029125019908\n",
      "7800 completed. Loss: 2.8373328613489868\n",
      "8000 completed. Loss: 3.382413159311982\n",
      "8200 completed. Loss: 3.2432836296688765\n",
      "8400 completed. Loss: 2.565210247086361\n",
      "8600 completed. Loss: 2.8382043437194078\n",
      "8800 completed. Loss: 2.742281955464277\n",
      "9000 completed. Loss: 2.960132850809023\n",
      "9200 completed. Loss: 2.8995315638184547\n",
      "200 completed. Loss: 3.6024652346875516\n",
      "400 completed. Loss: 3.104682398373261\n",
      "600 completed. Loss: 2.8395220782514663\n",
      "800 completed. Loss: 3.2715275794267655\n",
      "1000 completed. Loss: 2.800195242098998\n",
      "1200 completed. Loss: 2.6892444831179456\n",
      "1400 completed. Loss: 2.9545718101691456\n",
      "1600 completed. Loss: 2.477392451670021\n",
      "1800 completed. Loss: 2.7837229643960018\n",
      "2000 completed. Loss: 2.6671492590382697\n",
      "2200 completed. Loss: 2.8179521323833616\n",
      "2400 completed. Loss: 2.3993320389930157\n",
      "2600 completed. Loss: 2.799454787746072\n",
      "2800 completed. Loss: 2.319668185003102\n",
      "3000 completed. Loss: 3.2802172619476915\n",
      "3200 completed. Loss: 2.8795207520667465\n",
      "3400 completed. Loss: 2.7376128116436305\n",
      "3600 completed. Loss: 3.06313274839893\n",
      "3800 completed. Loss: 3.0956804462801664\n",
      "4000 completed. Loss: 3.311596474777907\n",
      "4200 completed. Loss: 2.4781506321707276\n",
      "4400 completed. Loss: 2.976248533912003\n",
      "4600 completed. Loss: 3.2342337761074305\n",
      "4800 completed. Loss: 3.5050722438003867\n",
      "5000 completed. Loss: 3.0462278148997575\n",
      "5200 completed. Loss: 2.6733645880781114\n",
      "5400 completed. Loss: 2.629691806184128\n",
      "5600 completed. Loss: 2.2099392673932017\n",
      "5800 completed. Loss: 2.8838938512466847\n",
      "6000 completed. Loss: 2.437438486488536\n",
      "6200 completed. Loss: 2.565504653621465\n",
      "6400 completed. Loss: 2.6789661073219033\n",
      "6600 completed. Loss: 2.586982220094651\n",
      "6800 completed. Loss: 2.457042509545572\n",
      "7000 completed. Loss: 2.880122287329286\n",
      "7200 completed. Loss: 2.598153848941438\n",
      "7400 completed. Loss: 2.9386489952169357\n",
      "7600 completed. Loss: 2.5910239229910075\n",
      "7800 completed. Loss: 2.7922061045933515\n",
      "8000 completed. Loss: 3.3828319621644916\n",
      "8200 completed. Loss: 3.231941958675161\n",
      "8400 completed. Loss: 2.544302038513124\n",
      "8600 completed. Loss: 2.8225274270400407\n",
      "8800 completed. Loss: 2.7233093018108048\n",
      "9000 completed. Loss: 2.9360657585132865\n",
      "9200 completed. Loss: 2.888693842124194\n",
      "200 completed. Loss: 3.580135122765787\n",
      "400 completed. Loss: 3.009039883930236\n",
      "600 completed. Loss: 2.8155195978470147\n",
      "800 completed. Loss: 3.2891943126916887\n",
      "1000 completed. Loss: 2.7980812078434973\n",
      "1200 completed. Loss: 2.682585467011668\n",
      "1400 completed. Loss: 2.945620144829154\n",
      "1600 completed. Loss: 2.4604358746763317\n",
      "1800 completed. Loss: 2.7740852096723394\n",
      "2000 completed. Loss: 2.659349821191281\n",
      "2200 completed. Loss: 2.8119130608439447\n",
      "2400 completed. Loss: 2.38788758803159\n",
      "2600 completed. Loss: 2.7948851665854453\n",
      "2800 completed. Loss: 2.3128238559607417\n",
      "3000 completed. Loss: 3.2649123884225264\n",
      "3200 completed. Loss: 2.8738532576547002\n",
      "3400 completed. Loss: 2.7280638107191773\n",
      "3600 completed. Loss: 3.048077239347622\n",
      "3800 completed. Loss: 3.082173297200352\n",
      "4000 completed. Loss: 3.3109386752778662\n",
      "4200 completed. Loss: 2.465541200671578\n",
      "4400 completed. Loss: 2.9701079467404634\n",
      "4600 completed. Loss: 3.197383431568742\n",
      "4800 completed. Loss: 3.4556626715045424\n",
      "5000 completed. Loss: 2.954189082579687\n",
      "5200 completed. Loss: 2.631340086348355\n",
      "5400 completed. Loss: 2.6101392474770546\n",
      "5600 completed. Loss: 2.188409830201417\n",
      "5800 completed. Loss: 2.870576134873554\n",
      "6000 completed. Loss: 2.438019568435848\n",
      "6200 completed. Loss: 2.5595391050726177\n",
      "6400 completed. Loss: 2.663742206664756\n",
      "6600 completed. Loss: 2.5760618506371973\n",
      "6800 completed. Loss: 2.449873108889442\n",
      "7000 completed. Loss: 2.84944448989816\n",
      "7200 completed. Loss: 2.5616173430718483\n",
      "7400 completed. Loss: 2.900340954363346\n",
      "7600 completed. Loss: 2.5611519181355833\n",
      "7800 completed. Loss: 2.7567099561123176\n",
      "8000 completed. Loss: 3.3838004163699225\n",
      "8200 completed. Loss: 3.22095723037608\n",
      "8400 completed. Loss: 2.525280793281272\n",
      "8600 completed. Loss: 2.8087029469851403\n",
      "8800 completed. Loss: 2.707301041984465\n",
      "9000 completed. Loss: 2.9156558656692506\n",
      "9200 completed. Loss: 2.878336385395378\n",
      "200 completed. Loss: 3.5608590790769084\n",
      "400 completed. Loss: 2.926915312111378\n",
      "600 completed. Loss: 2.799343023588881\n",
      "800 completed. Loss: 3.3066653447039425\n",
      "1000 completed. Loss: 2.797658396307379\n",
      "1200 completed. Loss: 2.6762160050775856\n",
      "1400 completed. Loss: 2.936887935004197\n",
      "1600 completed. Loss: 2.444649475105107\n",
      "1800 completed. Loss: 2.7654186679329724\n",
      "2000 completed. Loss: 2.6515138851385562\n",
      "2200 completed. Loss: 2.8060707348491998\n",
      "2400 completed. Loss: 2.3771456881798803\n",
      "2600 completed. Loss: 2.789778214963153\n",
      "2800 completed. Loss: 2.3062272774614394\n",
      "3000 completed. Loss: 3.250480717020109\n",
      "3200 completed. Loss: 2.868506268238416\n",
      "3400 completed. Loss: 2.7194268628116696\n",
      "3600 completed. Loss: 3.033214451735839\n",
      "3800 completed. Loss: 3.0718019757792354\n",
      "4000 completed. Loss: 3.309187739808112\n",
      "4200 completed. Loss: 2.4632344158622437\n",
      "4400 completed. Loss: 2.950827828776091\n",
      "4600 completed. Loss: 3.1439145912788806\n",
      "4800 completed. Loss: 3.384894111077301\n",
      "5000 completed. Loss: 2.8902189095970243\n",
      "5200 completed. Loss: 2.6091405029129238\n",
      "5400 completed. Loss: 2.602924176333472\n",
      "5600 completed. Loss: 2.1744745890051127\n",
      "5800 completed. Loss: 2.8596827169973404\n",
      "6000 completed. Loss: 2.4350792898982765\n",
      "6200 completed. Loss: 2.5526647541765124\n",
      "6400 completed. Loss: 2.648067144753877\n",
      "6600 completed. Loss: 2.564348720125854\n",
      "6800 completed. Loss: 2.4423150523239747\n",
      "7000 completed. Loss: 2.8222488201316445\n",
      "7200 completed. Loss: 2.530452413139865\n",
      "7400 completed. Loss: 2.868229610733688\n",
      "7600 completed. Loss: 2.537138085570186\n",
      "7800 completed. Loss: 2.724923055372201\n",
      "8000 completed. Loss: 3.385357360774651\n",
      "8200 completed. Loss: 3.209832614855841\n",
      "8400 completed. Loss: 2.5073154872749\n",
      "8600 completed. Loss: 2.79560644232668\n",
      "8800 completed. Loss: 2.6930071153608153\n",
      "9000 completed. Loss: 2.896638806802221\n",
      "9200 completed. Loss: 2.867707891250029\n",
      "200 completed. Loss: 3.542382918787189\n",
      "400 completed. Loss: 2.8612820530775935\n",
      "600 completed. Loss: 2.7886191766150294\n",
      "800 completed. Loss: 3.323402742408216\n",
      "1000 completed. Loss: 2.7984972525946796\n",
      "1200 completed. Loss: 2.6695895196683703\n",
      "1400 completed. Loss: 2.92798983364366\n",
      "1600 completed. Loss: 2.429807190913707\n",
      "1800 completed. Loss: 2.7575732055865227\n",
      "2000 completed. Loss: 2.6435584398172796\n",
      "2200 completed. Loss: 2.800196197829209\n",
      "2400 completed. Loss: 2.3668142702197654\n",
      "2600 completed. Loss: 2.7841188436653463\n",
      "2800 completed. Loss: 2.299682495770976\n",
      "3000 completed. Loss: 3.236672079442069\n",
      "3200 completed. Loss: 2.8634656228055246\n",
      "3400 completed. Loss: 2.7116714760567993\n",
      "3600 completed. Loss: 3.0187563364300876\n",
      "3800 completed. Loss: 3.0637216695118696\n",
      "4000 completed. Loss: 3.3062849877984264\n",
      "4200 completed. Loss: 2.4688698741456028\n",
      "4400 completed. Loss: 2.922989257872105\n",
      "4600 completed. Loss: 3.0823079320695252\n",
      "4800 completed. Loss: 3.321272856798023\n",
      "5000 completed. Loss: 2.8516922575281933\n",
      "5200 completed. Loss: 2.5932152593694626\n",
      "5400 completed. Loss: 2.598813041076064\n",
      "5600 completed. Loss: 2.1629301714804026\n",
      "5800 completed. Loss: 2.849790807720274\n",
      "6000 completed. Loss: 2.4305232826247813\n",
      "6200 completed. Loss: 2.545647599417716\n",
      "6400 completed. Loss: 2.632962337976787\n",
      "6600 completed. Loss: 2.5527552174031736\n",
      "6800 completed. Loss: 2.4353336786455473\n",
      "7000 completed. Loss: 2.798036004761234\n",
      "7200 completed. Loss: 2.502924252571538\n",
      "7400 completed. Loss: 2.840192349916324\n",
      "7600 completed. Loss: 2.51689640333876\n",
      "7800 completed. Loss: 2.696140561206266\n",
      "8000 completed. Loss: 3.387214866792783\n",
      "8200 completed. Loss: 3.198631436442956\n",
      "8400 completed. Loss: 2.4901517763454466\n",
      "8600 completed. Loss: 2.782562162708491\n",
      "8800 completed. Loss: 2.6797305818460884\n",
      "9000 completed. Loss: 2.878506712708622\n",
      "9200 completed. Loss: 2.8569878336694092\n",
      "200 completed. Loss: 3.523410377735272\n",
      "400 completed. Loss: 2.810252722823061\n",
      "600 completed. Loss: 2.781049212897196\n",
      "800 completed. Loss: 3.3386627422459423\n",
      "1000 completed. Loss: 2.8003260232508183\n",
      "1200 completed. Loss: 2.6624166169110683\n",
      "1400 completed. Loss: 2.9188808433152733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 completed. Loss: 2.415824693636969\n",
      "1800 completed. Loss: 2.7504391072224825\n",
      "2000 completed. Loss: 2.635458842013031\n",
      "2200 completed. Loss: 2.794037229944952\n",
      "2400 completed. Loss: 2.356672132811509\n",
      "2600 completed. Loss: 2.778055783379823\n",
      "2800 completed. Loss: 2.2930610172171146\n",
      "3000 completed. Loss: 3.223047074014321\n",
      "3200 completed. Loss: 2.8584624006040396\n",
      "3400 completed. Loss: 2.7046642475761473\n",
      "3600 completed. Loss: 3.004457666594535\n",
      "3800 completed. Loss: 3.0578306037094443\n",
      "4000 completed. Loss: 3.3006703472463412\n",
      "4200 completed. Loss: 2.4791645239817446\n",
      "4400 completed. Loss: 2.8889250380545857\n",
      "4600 completed. Loss: 3.0228398274537174\n",
      "4800 completed. Loss: 3.2758355271071196\n",
      "5000 completed. Loss: 2.82756771602435\n",
      "5200 completed. Loss: 2.579320977013558\n",
      "5400 completed. Loss: 2.5949241806007923\n",
      "5600 completed. Loss: 2.152180851446465\n",
      "5800 completed. Loss: 2.8403010950423777\n",
      "6000 completed. Loss: 2.4251308611221614\n",
      "6200 completed. Loss: 2.5385176309850066\n",
      "6400 completed. Loss: 2.617781606540084\n",
      "6600 completed. Loss: 2.5406530874408784\n",
      "6800 completed. Loss: 2.4285770081053486\n",
      "7000 completed. Loss: 2.774927634503692\n",
      "7200 completed. Loss: 2.4773582991678267\n",
      "7400 completed. Loss: 2.8149549002200365\n",
      "7600 completed. Loss: 2.4997539126034827\n",
      "7800 completed. Loss: 2.670544687737711\n",
      "8000 completed. Loss: 3.389168237783015\n",
      "8200 completed. Loss: 3.187493951944634\n",
      "8400 completed. Loss: 2.4733397777285426\n",
      "8600 completed. Loss: 2.769199407408014\n",
      "8800 completed. Loss: 2.6666617977665736\n",
      "9000 completed. Loss: 2.86074420043733\n",
      "9200 completed. Loss: 2.846334829274565\n",
      "200 completed. Loss: 3.503359575062059\n",
      "400 completed. Loss: 2.7718771405331792\n",
      "600 completed. Loss: 2.7753039417415857\n",
      "800 completed. Loss: 3.3523599274270235\n",
      "1000 completed. Loss: 2.8026756606902925\n",
      "1200 completed. Loss: 2.6545878762379287\n",
      "1400 completed. Loss: 2.9095107422024014\n",
      "1600 completed. Loss: 2.402333055799827\n",
      "1800 completed. Loss: 2.7440539798792454\n",
      "2000 completed. Loss: 2.627100623585284\n",
      "2200 completed. Loss: 2.787420821748674\n",
      "2400 completed. Loss: 2.3466128943813964\n",
      "2600 completed. Loss: 2.7717119800299406\n",
      "2800 completed. Loss: 2.2862503559328617\n",
      "3000 completed. Loss: 3.2092705289972945\n",
      "3200 completed. Loss: 2.8531602013052906\n",
      "3400 completed. Loss: 2.6980707745812835\n",
      "3600 completed. Loss: 2.9900017092190683\n",
      "3800 completed. Loss: 3.054007696537301\n",
      "4000 completed. Loss: 3.2911419214680793\n",
      "4200 completed. Loss: 2.490223201659974\n",
      "4400 completed. Loss: 2.851141999503598\n",
      "4600 completed. Loss: 2.973682009866461\n",
      "4800 completed. Loss: 3.2455810172110797\n",
      "5000 completed. Loss: 2.810213481443934\n",
      "5200 completed. Loss: 2.566012272918597\n",
      "5400 completed. Loss: 2.5905262672249227\n",
      "5600 completed. Loss: 2.141330353831872\n",
      "5800 completed. Loss: 2.830815607635304\n",
      "6000 completed. Loss: 2.41933922495693\n",
      "6200 completed. Loss: 2.53129447247833\n",
      "6400 completed. Loss: 2.602080118554877\n",
      "6600 completed. Loss: 2.52758681897074\n",
      "6800 completed. Loss: 2.4216554121486844\n",
      "7000 completed. Loss: 2.752103800103068\n",
      "7200 completed. Loss: 2.4531231078598648\n",
      "7400 completed. Loss: 2.7917247734731063\n",
      "7600 completed. Loss: 2.4850105494447052\n",
      "7800 completed. Loss: 2.647738664550707\n",
      "8000 completed. Loss: 3.391423582388088\n",
      "8200 completed. Loss: 3.176531921727583\n",
      "8400 completed. Loss: 2.456864809114486\n",
      "8600 completed. Loss: 2.7554245504457504\n",
      "8800 completed. Loss: 2.653590430500917\n",
      "9000 completed. Loss: 2.8428079363354484\n",
      "9200 completed. Loss: 2.8356182445958256\n",
      "200 completed. Loss: 3.4816295581962913\n",
      "400 completed. Loss: 2.7440677355835215\n",
      "600 completed. Loss: 2.7704490914940836\n",
      "800 completed. Loss: 3.3646561693772674\n",
      "1000 completed. Loss: 2.8054399957554415\n",
      "1200 completed. Loss: 2.6461195680312812\n",
      "1400 completed. Loss: 2.900111752054654\n",
      "1600 completed. Loss: 2.389171397741884\n",
      "1800 completed. Loss: 2.738162833014503\n",
      "2000 completed. Loss: 2.6184706680942327\n",
      "2200 completed. Loss: 2.7803003723081203\n",
      "2400 completed. Loss: 2.3366303869197145\n",
      "2600 completed. Loss: 2.7651390483696012\n",
      "2800 completed. Loss: 2.279206470446661\n",
      "3000 completed. Loss: 3.1950718705542385\n",
      "3200 completed. Loss: 2.8473192811687476\n",
      "3400 completed. Loss: 2.691626856811345\n",
      "3600 completed. Loss: 2.974957932122052\n",
      "3800 completed. Loss: 3.051760419886559\n",
      "4000 completed. Loss: 3.277588762594387\n",
      "4200 completed. Loss: 2.4986918523721395\n",
      "4400 completed. Loss: 2.813211004754994\n",
      "4600 completed. Loss: 2.9366605634242298\n",
      "4800 completed. Loss: 3.2246875574439766\n",
      "5000 completed. Loss: 2.7959429510566407\n",
      "5200 completed. Loss: 2.5528298271354286\n",
      "5400 completed. Loss: 2.5856122458400206\n",
      "5600 completed. Loss: 2.1300560980383305\n",
      "5800 completed. Loss: 2.821198768913746\n",
      "6000 completed. Loss: 2.4133017017319798\n",
      "6200 completed. Loss: 2.523981802677736\n",
      "6400 completed. Loss: 2.585665946640656\n",
      "6600 completed. Loss: 2.5133685350231825\n",
      "6800 completed. Loss: 2.4142081206524746\n",
      "7000 completed. Loss: 2.7295629249559714\n",
      "7200 completed. Loss: 2.4301363142766057\n",
      "7400 completed. Loss: 2.7700928064389156\n",
      "7600 completed. Loss: 2.472092876546085\n",
      "7800 completed. Loss: 2.6271411011740566\n",
      "8000 completed. Loss: 3.393649510536343\n",
      "8200 completed. Loss: 3.1656588560249657\n",
      "8400 completed. Loss: 2.4407495232950898\n",
      "8600 completed. Loss: 2.741231381222606\n",
      "8800 completed. Loss: 2.640430269697681\n",
      "9000 completed. Loss: 2.824523854253348\n",
      "9200 completed. Loss: 2.824673386774957\n",
      "200 completed. Loss: 3.458478797432035\n",
      "400 completed. Loss: 2.7246538150962443\n",
      "600 completed. Loss: 2.766223492696881\n",
      "800 completed. Loss: 3.375931476689875\n",
      "1000 completed. Loss: 2.808646874986589\n",
      "1200 completed. Loss: 2.637122364360839\n",
      "1400 completed. Loss: 2.890570456078276\n",
      "1600 completed. Loss: 2.3763686757721008\n",
      "1800 completed. Loss: 2.732702846080065\n",
      "2000 completed. Loss: 2.609579745512456\n",
      "2200 completed. Loss: 2.7727186744194476\n",
      "2400 completed. Loss: 2.326788284238428\n",
      "2600 completed. Loss: 2.7584066794719546\n",
      "2800 completed. Loss: 2.2719603150337933\n",
      "3000 completed. Loss: 3.1803735479153694\n",
      "3200 completed. Loss: 2.840867540682666\n",
      "3400 completed. Loss: 2.685156257916242\n",
      "3600 completed. Loss: 2.9591367419250307\n",
      "3800 completed. Loss: 3.050357075529173\n",
      "4000 completed. Loss: 3.2607812990434466\n",
      "4200 completed. Loss: 2.502852580489125\n",
      "4400 completed. Loss: 2.7780489146872425\n",
      "4600 completed. Loss: 2.909425290171057\n",
      "4800 completed. Loss: 3.2089730620756747\n",
      "5000 completed. Loss: 2.7830529368855057\n",
      "5200 completed. Loss: 2.539527907324955\n",
      "5400 completed. Loss: 2.580167843401432\n",
      "5600 completed. Loss: 2.1183302198536693\n",
      "5800 completed. Loss: 2.8114565443526955\n",
      "6000 completed. Loss: 2.4069734201952815\n",
      "6200 completed. Loss: 2.5164099564403295\n",
      "6400 completed. Loss: 2.568523382206913\n",
      "6600 completed. Loss: 2.498017295449972\n",
      "6800 completed. Loss: 2.406141676809639\n",
      "7000 completed. Loss: 2.7076348844310267\n",
      "7200 completed. Loss: 2.4083854877203703\n",
      "7400 completed. Loss: 2.749787889844738\n",
      "7600 completed. Loss: 2.4605584424268456\n",
      "7800 completed. Loss: 2.608179328478873\n",
      "8000 completed. Loss: 3.3950678107142447\n",
      "8200 completed. Loss: 3.154846056997776\n",
      "8400 completed. Loss: 2.4249674087017774\n",
      "8600 completed. Loss: 2.7266125229373572\n",
      "8800 completed. Loss: 2.6271134760556745\n",
      "9000 completed. Loss: 2.805974864917807\n",
      "9200 completed. Loss: 2.8134234192594887\n",
      "200 completed. Loss: 3.4345895807631313\n",
      "400 completed. Loss: 2.711527259349823\n",
      "600 completed. Loss: 2.762603081315756\n",
      "800 completed. Loss: 3.3863266399130225\n",
      "1000 completed. Loss: 2.812103017740883\n",
      "1200 completed. Loss: 2.6278069030493496\n",
      "1400 completed. Loss: 2.8807387065980583\n",
      "1600 completed. Loss: 2.3638801736664026\n",
      "1800 completed. Loss: 2.72765062129125\n",
      "2000 completed. Loss: 2.600408440232277\n",
      "2200 completed. Loss: 2.764667671550997\n",
      "2400 completed. Loss: 2.317089355266653\n",
      "2600 completed. Loss: 2.751519676791504\n",
      "2800 completed. Loss: 2.2645693029649556\n",
      "3000 completed. Loss: 3.165162605877267\n",
      "3200 completed. Loss: 2.8337486234516835\n",
      "3400 completed. Loss: 2.678500967146829\n",
      "3600 completed. Loss: 2.94252459526062\n",
      "3800 completed. Loss: 3.0492996571958066\n",
      "4000 completed. Loss: 3.2414596588350832\n",
      "4200 completed. Loss: 2.5025627550319767\n",
      "4400 completed. Loss: 2.746739581474103\n",
      "4600 completed. Loss: 2.8888546229247005\n",
      "4800 completed. Loss: 3.196049292944372\n",
      "5000 completed. Loss: 2.770720483926125\n",
      "5200 completed. Loss: 2.525890820212662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400 completed. Loss: 2.5743190459907055\n",
      "5600 completed. Loss: 2.1063863622024654\n",
      "5800 completed. Loss: 2.8016311717685314\n",
      "6000 completed. Loss: 2.4002073393389582\n",
      "6200 completed. Loss: 2.50840455615893\n",
      "6400 completed. Loss: 2.5508597760647533\n",
      "6600 completed. Loss: 2.481830311976373\n",
      "6800 completed. Loss: 2.3975738741224633\n",
      "7000 completed. Loss: 2.686636018478312\n",
      "7200 completed. Loss: 2.387833826793358\n",
      "7400 completed. Loss: 2.730514341169037\n",
      "7600 completed. Loss: 2.450047798450105\n",
      "7800 completed. Loss: 2.59044259024784\n",
      "8000 completed. Loss: 3.3956017111428083\n",
      "8200 completed. Loss: 3.144032588964328\n",
      "8400 completed. Loss: 2.40947138145566\n",
      "8600 completed. Loss: 2.7116514642257243\n",
      "8800 completed. Loss: 2.613650718173012\n",
      "9000 completed. Loss: 2.787256169230677\n",
      "9200 completed. Loss: 2.8017685550451277\n",
      "200 completed. Loss: 3.410888820718974\n",
      "400 completed. Loss: 2.7030593302473425\n",
      "600 completed. Loss: 2.759675440620631\n",
      "800 completed. Loss: 3.3958409141562877\n",
      "1000 completed. Loss: 2.8156380955083296\n",
      "1200 completed. Loss: 2.618315433282405\n",
      "1400 completed. Loss: 2.8706325857620687\n",
      "1600 completed. Loss: 2.3516841087117792\n",
      "1800 completed. Loss: 2.722942305672914\n",
      "2000 completed. Loss: 2.590993299894035\n",
      "2200 completed. Loss: 2.7561811128957197\n",
      "2400 completed. Loss: 2.3075838097045196\n",
      "2600 completed. Loss: 2.744475606745109\n",
      "2800 completed. Loss: 2.257083114013076\n",
      "3000 completed. Loss: 3.149514161966508\n",
      "3200 completed. Loss: 2.8259970381739548\n",
      "3400 completed. Loss: 2.671554580125958\n",
      "3600 completed. Loss: 2.925174415698275\n",
      "3800 completed. Loss: 3.0482825128920377\n",
      "4000 completed. Loss: 3.2204362924210725\n",
      "4200 completed. Loss: 2.498790452622343\n",
      "4400 completed. Loss: 2.7191240210738035\n",
      "4600 completed. Loss: 2.872469274085015\n",
      "4800 completed. Loss: 3.1845723543316127\n",
      "5000 completed. Loss: 2.7585420675715433\n",
      "5200 completed. Loss: 2.511796376849525\n",
      "5400 completed. Loss: 2.5682274531386793\n",
      "5600 completed. Loss: 2.0943934964574873\n",
      "5800 completed. Loss: 2.791750407824293\n",
      "6000 completed. Loss: 2.3929350785166026\n",
      "6200 completed. Loss: 2.499849814614281\n",
      "6400 completed. Loss: 2.533157032937743\n",
      "6600 completed. Loss: 2.4652471094951034\n",
      "6800 completed. Loss: 2.3887757325312124\n",
      "7000 completed. Loss: 2.6667895432328805\n",
      "7200 completed. Loss: 2.368363187452778\n",
      "7400 completed. Loss: 2.71203345249407\n",
      "7600 completed. Loss: 2.4402925119875\n",
      "7800 completed. Loss: 2.5736101742088793\n",
      "8000 completed. Loss: 3.3955004778131843\n",
      "8200 completed. Loss: 3.13318884995766\n",
      "8400 completed. Loss: 2.394228290505707\n",
      "8600 completed. Loss: 2.6964352359296755\n",
      "8800 completed. Loss: 2.600172024453059\n",
      "9000 completed. Loss: 2.768545801343862\n",
      "9200 completed. Loss: 2.789653797606006\n",
      "200 completed. Loss: 3.38823838760145\n",
      "400 completed. Loss: 2.6980455268733206\n",
      "600 completed. Loss: 2.757603111024946\n",
      "800 completed. Loss: 3.4043948573153466\n",
      "1000 completed. Loss: 2.8190482300240545\n",
      "1200 completed. Loss: 2.608875771071762\n",
      "1400 completed. Loss: 2.8602792362030596\n",
      "1600 completed. Loss: 2.3397881523612885\n",
      "1800 completed. Loss: 2.718549913633615\n",
      "2000 completed. Loss: 2.5813707572966815\n",
      "2200 completed. Loss: 2.747321513192728\n",
      "2400 completed. Loss: 2.2983301212033256\n",
      "2600 completed. Loss: 2.737320533785969\n",
      "2800 completed. Loss: 2.2495401517208666\n",
      "3000 completed. Loss: 3.133530670949258\n",
      "3200 completed. Loss: 2.8176586342882364\n",
      "3400 completed. Loss: 2.664247492607683\n",
      "3600 completed. Loss: 2.9072469218075274\n",
      "3800 completed. Loss: 3.047151341708377\n",
      "4000 completed. Loss: 3.1984347270056603\n",
      "4200 completed. Loss: 2.492702556629665\n",
      "4400 completed. Loss: 2.694606987067964\n",
      "4600 completed. Loss: 2.858708386532962\n",
      "4800 completed. Loss: 3.173849146515131\n",
      "5000 completed. Loss: 2.7463508414290847\n",
      "5200 completed. Loss: 2.497265528230928\n",
      "5400 completed. Loss: 2.562027124846354\n",
      "5600 completed. Loss: 2.0824333241395654\n",
      "5800 completed. Loss: 2.781899649957195\n",
      "6000 completed. Loss: 2.385202414132655\n",
      "6200 completed. Loss: 2.4907349714264275\n",
      "6400 completed. Loss: 2.515911854719743\n",
      "6600 completed. Loss: 2.44867849400267\n",
      "6800 completed. Loss: 2.3799788650078697\n",
      "7000 completed. Loss: 2.648154277233407\n",
      "7200 completed. Loss: 2.3498205630574374\n",
      "7400 completed. Loss: 2.6941614424763247\n",
      "7600 completed. Loss: 2.431086267139763\n",
      "7800 completed. Loss: 2.557470902558416\n",
      "8000 completed. Loss: 3.3949580000247805\n",
      "8200 completed. Loss: 3.1223063561785964\n",
      "8400 completed. Loss: 2.379218706106767\n",
      "8600 completed. Loss: 2.6810410244436933\n",
      "8800 completed. Loss: 2.586800298462622\n",
      "9000 completed. Loss: 2.7500441378715914\n",
      "9200 completed. Loss: 2.7770910003501923\n",
      "200 completed. Loss: 3.3675547663122414\n",
      "400 completed. Loss: 2.6957377023622393\n",
      "600 completed. Loss: 2.7565504050441088\n",
      "800 completed. Loss: 3.4120984771382066\n",
      "1000 completed. Loss: 2.8219565889472142\n",
      "1200 completed. Loss: 2.5997963393852115\n",
      "1400 completed. Loss: 2.8496442393492907\n",
      "1600 completed. Loss: 2.3282588755805045\n",
      "1800 completed. Loss: 2.7144473681971433\n",
      "2000 completed. Loss: 2.5715883270744233\n",
      "2200 completed. Loss: 2.7381833315454425\n",
      "2400 completed. Loss: 2.28939754802268\n",
      "2600 completed. Loss: 2.7301546954736113\n",
      "2800 completed. Loss: 2.242033031815663\n",
      "3000 completed. Loss: 3.1173418750514976\n",
      "3200 completed. Loss: 2.808848582985811\n",
      "3400 completed. Loss: 2.656597823398188\n",
      "3600 completed. Loss: 2.8889958432037384\n",
      "3800 completed. Loss: 3.045915671950206\n",
      "4000 completed. Loss: 3.176096642212942\n",
      "4200 completed. Loss: 2.4853436754015275\n",
      "4400 completed. Loss: 2.6725901165558024\n",
      "4600 completed. Loss: 2.8466721312049774\n",
      "4800 completed. Loss: 3.1635472654551267\n",
      "5000 completed. Loss: 2.7341527121397666\n",
      "5200 completed. Loss: 2.482446885998361\n",
      "5400 completed. Loss: 2.5558792765345424\n",
      "5600 completed. Loss: 2.070531435608864\n",
      "5800 completed. Loss: 2.7721661465521903\n",
      "6000 completed. Loss: 2.3771740280650557\n",
      "6200 completed. Loss: 2.4811476792395113\n",
      "6400 completed. Loss: 2.499596252203919\n",
      "6600 completed. Loss: 2.432494204249233\n",
      "6800 completed. Loss: 2.371298561054282\n",
      "7000 completed. Loss: 2.6307396990433336\n",
      "7200 completed. Loss: 2.332095802715048\n",
      "7400 completed. Loss: 2.676834777924232\n",
      "7600 completed. Loss: 2.422336771548726\n",
      "7800 completed. Loss: 2.5419092964939773\n",
      "8000 completed. Loss: 3.3941029667202383\n",
      "8200 completed. Loss: 3.1113877285132183\n",
      "8400 completed. Loss: 2.364461308848113\n",
      "8600 completed. Loss: 2.665561145301908\n",
      "8800 completed. Loss: 2.5736954987514764\n",
      "9000 completed. Loss: 2.731963998124702\n",
      "9200 completed. Loss: 2.7641883937176317\n"
     ]
    }
   ],
   "source": [
    "xs, ys = train(neural, optimizer, loss, n_scheduler, train_x, train_y, 30, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(xs)):\n",
    "    plt.plot(xs[i], ys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21.5459,  5.5099,  4.9950,  0.0237,  0.1341,  0.5924,  0.2855],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000e+01, 4.9000e+00, 5.9000e+00, 1.8000e-02, 1.3400e-01, 5.8600e-01,\n",
      "        2.7700e-01]) tensor(4.4198, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.5114, 1.4685, 0.4260, 0.0507, 0.1275, 0.4729, 0.0765],\n",
      "       grad_fn=<AddBackward0>) tensor([0.5000, 0.8000, 0.3000, 0.0000, 0.1580, 0.2500, 0.0710]) tensor(0.2199, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4651, 3.4207, 2.9553, 0.0281, 0.1052, 0.5373, 0.1654],\n",
      "       grad_fn=<AddBackward0>) tensor([7.5000, 3.9000, 1.5000, 0.0160, 0.1190, 0.5400, 0.0670]) tensor(0.8884, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0999,  3.4380,  2.4880,  0.0414,  0.1426,  0.5306,  0.1826],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 3.1000, 3.8000, 0.0390, 0.1050, 0.4160, 0.2940]) tensor(3.8373, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.8060,  3.7780,  3.9848,  0.0201,  0.1136,  0.5496,  0.2457],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8800e+01, 3.2000e+00, 6.0000e+00, 1.6000e-02, 9.5000e-02, 5.4200e-01,\n",
      "        3.2700e-01]) tensor(1.9095, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.0526, -0.2089,  0.8201,  0.0274,  0.0929,  0.4150,  0.1351],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 1.3000, 0.5000, 0.0290, 0.1080, 0.3570, 0.1050]) tensor(0.3493, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.4952, 2.2880, 0.6190, 0.0508, 0.1377, 0.4938, 0.0828],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 1.2000, 0.3000, 0.0990, 0.1100, 0.3870, 0.0650]) tensor(0.7545, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1502,  4.8779,  2.9534,  0.0432,  0.1448,  0.5597,  0.1729],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 1.7000, 3.5000, 0.0200, 0.0810, 0.5070, 0.2960]) tensor(9.2064, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.7154,  6.2218,  2.6479,  0.0602,  0.1796,  0.5936,  0.1469],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 4.1000, 1.0000, 0.0440, 0.1150, 0.5360, 0.0570]) tensor(5.3789, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8303,  4.6928,  1.7442,  0.0584,  0.1707,  0.5502,  0.1268],\n",
      "       grad_fn=<AddBackward0>) tensor([13.5000,  6.1000,  1.8000,  0.0510,  0.1550,  0.5350,  0.0880]) tensor(1.3018, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1152,  5.7840,  2.4570,  0.0580,  0.1780,  0.5783,  0.1513],\n",
      "       grad_fn=<AddBackward0>) tensor([16.2000,  6.4000,  1.2000,  0.0880,  0.1600,  0.5480,  0.0710]) tensor(0.9021, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2674,  3.5309,  3.6761,  0.0195,  0.1082,  0.5437,  0.2270],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 4.3000, 1.2000, 0.0290, 0.2240, 0.5500, 0.1130]) tensor(3.9443, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.3357,  1.6583,  5.1761, -0.0182,  0.0382,  0.4992,  0.3113],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 1.1000, 2.6000, 0.0160, 0.0620, 0.4980, 0.2690]) tensor(9.5418, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5866, 2.3678, 1.3007, 0.0439, 0.1349, 0.4954, 0.1291],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 1.0000, 0.5000, 0.0180, 0.1060, 0.5170, 0.0920]) tensor(1.6336, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8625, 2.5935, 2.1423, 0.0432, 0.1351, 0.4947, 0.1726],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 1.1000, 1.1000, 0.0100, 0.0880, 0.5260, 0.1370]) tensor(3.5802, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1249,  5.1811,  1.3371,  0.0682,  0.1865,  0.5566,  0.1011],\n",
      "       grad_fn=<AddBackward0>) tensor([11.3000,  7.8000,  0.5000,  0.1180,  0.2250,  0.5520,  0.0350]) tensor(1.2784, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.0092,  4.9667,  3.9102,  0.0381,  0.1507,  0.5714,  0.2406],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9400e+01, 3.7000e+00, 9.6000e+00, 1.6000e-02, 8.8000e-02, 5.1500e-01,\n",
      "        4.2500e-01]) tensor(5.1363, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4658, 3.1324, 1.8068, 0.0426, 0.1374, 0.5072, 0.1472],\n",
      "       grad_fn=<AddBackward0>) tensor([5.4000, 3.6000, 1.0000, 0.0820, 0.2050, 0.5090, 0.1100]) tensor(1.4680, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.8746, 3.1374, 0.9939, 0.0604, 0.1594, 0.4986, 0.1050],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 1.4000, 0.1000, 0.1260, 0.1940, 0.5030, 0.0290]) tensor(4.0823, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4308,  5.8056,  2.1797,  0.0639,  0.1867,  0.5690,  0.1419],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 4.9000, 1.1000, 0.0920, 0.1960, 0.5210, 0.1090]) tensor(4.4976, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.4513, 3.3522, 1.0087, 0.0547, 0.1540, 0.5113, 0.1011],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 1.6000, 1.9000, 0.0320, 0.0990, 0.3780, 0.2210]) tensor(2.0673, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2087,  5.3054,  1.6315,  0.0668,  0.1853,  0.5678,  0.1122],\n",
      "       grad_fn=<AddBackward0>) tensor([12.6000,  8.1000,  1.0000,  0.1270,  0.2500,  0.6380,  0.0660]) tensor(1.4513, grad_fn=<MseLossBackward0>)\n",
      "tensor([21.1638,  6.8469,  3.7426,  0.0594,  0.1991,  0.6168,  0.2240],\n",
      "       grad_fn=<AddBackward0>) tensor([29.5000, 13.6000,  5.6000,  0.0680,  0.3070,  0.6130,  0.3280]) tensor(16.9383, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.8866, 4.1297, 1.6737, 0.0574, 0.1662, 0.5365, 0.1308],\n",
      "       grad_fn=<AddBackward0>) tensor([12.5000,  6.6000,  0.7000,  0.1020,  0.1520,  0.4850,  0.0480]) tensor(1.9845, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9675, 1.6945, 1.8696, 0.0269, 0.1026, 0.4848, 0.1521],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 1.7000, 2.3000, 0.0130, 0.0770, 0.4450, 0.1690]) tensor(0.0536, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5086,  5.1561,  2.3731,  0.0603,  0.1760,  0.5543,  0.1560],\n",
      "       grad_fn=<AddBackward0>) tensor([13.6000,  7.9000,  4.4000,  0.0340,  0.2270,  0.5460,  0.2220]) tensor(1.8338, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7038,  5.3324,  2.2913,  0.0591,  0.1678,  0.5650,  0.1336],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 3.6000, 1.4000, 0.0700, 0.1450, 0.4630, 0.1110]) tensor(8.1647, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.3104e+01,  1.4159e+00,  4.5127e+00, -9.2103e-03,  5.2990e-02,\n",
      "         4.9032e-01,  2.9044e-01], grad_fn=<AddBackward0>) tensor([5.3000, 1.0000, 2.3000, 0.0180, 0.0680, 0.4930, 0.3070]) tensor(9.4255, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4384, 3.5908, 1.4694, 0.0516, 0.1515, 0.5271, 0.1202],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 2.7000, 0.5000, 0.0720, 0.1620, 0.5100, 0.0650]) tensor(2.2447, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6214,  3.5437,  3.1678,  0.0267,  0.1117,  0.5366,  0.1911],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 3.5000, 2.2000, 0.0240, 0.1170, 0.5470, 0.1410]) tensor(0.6084, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.1045,  4.7694,  3.2041,  0.0411,  0.1487,  0.5519,  0.2050],\n",
      "       grad_fn=<AddBackward0>) tensor([15.1000,  4.5000,  1.4000,  0.0500,  0.1400,  0.5900,  0.0890]) tensor(0.4775, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1503, 4.1708, 1.7792, 0.0497, 0.1488, 0.5428, 0.1200],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 2.8000, 0.9000, 0.0420, 0.1400, 0.5000, 0.0810]) tensor(3.4687, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.8481, 2.7674, 0.1524, 0.0596, 0.1522, 0.4998, 0.0544],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 0.9000, 0.3000, 0.0210, 0.1290, 0.3840, 0.0630]) tensor(0.9914, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2286,  2.7633,  3.1600,  0.0185,  0.0986,  0.5145,  0.2081],\n",
      "       grad_fn=<AddBackward0>) tensor([10.4000,  2.4000,  3.6000,  0.0170,  0.0780,  0.5770,  0.2210]) tensor(0.1452, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.7527,  3.7020,  4.1423,  0.0248,  0.1266,  0.5363,  0.2707],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2300e+01, 5.7000e+00, 5.6000e+00, 2.2000e-02, 1.6600e-01, 4.7700e-01,\n",
      "        3.0000e-01]) tensor(5.2708, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3510, 3.2927, 0.5787, 0.0664, 0.1712, 0.5068, 0.0841],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 3.6000, 0.5000, 0.1250, 0.1890, 0.3610, 0.0540]) tensor(0.3617, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3875,  3.4422,  2.2722,  0.0370,  0.1325,  0.5316,  0.1610],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 1.3000, 0.9000, 0.0280, 0.1280, 0.4500, 0.1790]) tensor(7.9007, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0138,  4.0901,  2.7512,  0.0402,  0.1380,  0.5383,  0.1781],\n",
      "       grad_fn=<AddBackward0>) tensor([10.5000,  3.1000,  1.8000,  0.0200,  0.1030,  0.6000,  0.0950]) tensor(0.5984, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2315,  4.1516,  2.5337,  0.0442,  0.1431,  0.5284,  0.1686],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 1.1000, 0.6000, 0.0250, 0.0820, 0.4650, 0.0810]) tensor(11.5465, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.8933,  5.6771,  3.7459,  0.0418,  0.1567,  0.5888,  0.2170],\n",
      "       grad_fn=<AddBackward0>) tensor([17.0000,  6.0000,  2.8000,  0.0600,  0.1630,  0.5380,  0.1520]) tensor(0.2577, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.8636, 0.2761, 1.0090, 0.0253, 0.0903, 0.4368, 0.1289],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 0.6000, 0.6000, 0.0210, 0.1110, 0.3750, 0.1900]) tensor(0.1727, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3992, 3.4955, 1.3501, 0.0537, 0.1572, 0.5248, 0.1202],\n",
      "       grad_fn=<AddBackward0>) tensor([11.2000,  4.0000,  1.3000,  0.0280,  0.1090,  0.4880,  0.0660]) tensor(1.1584, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4806,  5.1239,  2.5759,  0.0479,  0.1524,  0.5724,  0.1469],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 2.7000, 1.9000, 0.0310, 0.1290, 0.5240, 0.1630]) tensor(8.4774, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6346, 3.8482, 1.7321, 0.0526, 0.1573, 0.5232, 0.1389],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 2.3000, 0.7000, 0.0400, 0.1100, 0.5720, 0.0680]) tensor(1.4138, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7611, 1.0353, 2.3862, 0.0239, 0.1069, 0.4594, 0.2169],\n",
      "       grad_fn=<AddBackward0>) tensor([10.9000,  2.9000,  2.3000,  0.0230,  0.1440,  0.4830,  0.2090]) tensor(1.1516, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.9451, 2.1245, 0.6049, 0.0478, 0.1319, 0.4724, 0.0898],\n",
      "       grad_fn=<AddBackward0>) tensor([0.7000, 1.5000, 0.2000, 0.0780, 0.2000, 0.3450, 0.0590]) tensor(0.8025, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.8902, 0.6287, 1.8274, 0.0213, 0.0873, 0.4654, 0.1612],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 1.4000, 2.4000, 0.0280, 0.0740, 0.4660, 0.2560]) tensor(0.2012, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5743,  4.5878,  2.0359,  0.0511,  0.1538,  0.5538,  0.1297],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 1.2000, 1.0000, 0.0280, 0.0540, 0.4930, 0.1010]) tensor(6.0762, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.9534,  5.1194,  2.4953,  0.0562,  0.1747,  0.5684,  0.1652],\n",
      "       grad_fn=<AddBackward0>) tensor([22.9000,  7.3000,  3.5000,  0.0300,  0.1620,  0.5540,  0.1610]) tensor(12.2583, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.1621,  4.1848,  4.5989,  0.0206,  0.1175,  0.5637,  0.2726],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6400e+01, 5.1000e+00, 6.1000e+00, 2.4000e-02, 1.2400e-01, 6.7500e-01,\n",
      "        2.7200e-01]) tensor(10.1381, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6591,  3.2322,  3.9114,  0.0147,  0.0909,  0.5348,  0.2257],\n",
      "       grad_fn=<AddBackward0>) tensor([10.4000,  3.1000,  6.1000,  0.0260,  0.0910,  0.4950,  0.3470]) tensor(1.4182, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.4036, 1.9301, 1.5628, 0.0362, 0.1206, 0.4817, 0.1463],\n",
      "       grad_fn=<AddBackward0>) tensor([11.2000,  3.4000,  1.5000,  0.0170,  0.1120,  0.5650,  0.0900]) tensor(3.5972, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3993, 4.1219, 2.2898, 0.0440, 0.1357, 0.5483, 0.1341],\n",
      "       grad_fn=<AddBackward0>) tensor([7.4000, 4.4000, 1.5000, 0.0200, 0.1870, 0.5870, 0.1050]) tensor(0.6720, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9561, 4.5294, 0.9497, 0.0733, 0.1881, 0.5329, 0.0906],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 7.9000, 0.5000, 0.1200, 0.2930, 0.5700, 0.0390]) tensor(2.3185, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0481,  3.7470,  3.0794,  0.0270,  0.1145,  0.5442,  0.1861],\n",
      "       grad_fn=<AddBackward0>) tensor([10.9000,  3.2000,  2.4000,  0.0160,  0.1150,  0.5840,  0.1550]) tensor(0.2974, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4887, 3.7111, 0.5261, 0.0729, 0.1798, 0.5090, 0.0764],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 3.2000, 0.3000, 0.1220, 0.2140, 0.4800, 0.0430]) tensor(1.7841, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7282,  4.7611,  2.5198,  0.0469,  0.1485,  0.5614,  0.1488],\n",
      "       grad_fn=<AddBackward0>) tensor([13.4000,  6.2000,  2.5000,  0.0480,  0.1570,  0.5440,  0.1110]) tensor(0.6953, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.9685,  5.8941,  3.1826,  0.0514,  0.1727,  0.5900,  0.1904],\n",
      "       grad_fn=<AddBackward0>) tensor([12.3000,  2.1000,  1.7000,  0.0330,  0.0820,  0.6030,  0.1490]) tensor(5.4855, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3981,  2.4200,  3.6666,  0.0129,  0.0807,  0.5100,  0.2196],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.3000, 1.4000, 0.0360, 0.1190, 0.4440, 0.2160]) tensor(10.7510, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9637,  3.9330,  2.5241,  0.0428,  0.1453,  0.5379,  0.1746],\n",
      "       grad_fn=<AddBackward0>) tensor([20.2000,  5.3000,  4.4000,  0.0370,  0.1060,  0.5170,  0.1930]) tensor(10.4608, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3279,  3.3294,  2.2077,  0.0421,  0.1393,  0.5243,  0.1636],\n",
      "       grad_fn=<AddBackward0>) tensor([14.3000,  4.2000,  7.9000,  0.0270,  0.1150,  0.5050,  0.3580]) tensor(6.9967, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.3505,  5.1650,  3.8388,  0.0287,  0.1340,  0.5812,  0.2218],\n",
      "       grad_fn=<AddBackward0>) tensor([16.9000,  4.6000,  2.1000,  0.0500,  0.0960,  0.5170,  0.0970]) tensor(0.5096, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.8163,  6.4399,  2.6568,  0.0607,  0.1869,  0.6023,  0.1523],\n",
      "       grad_fn=<AddBackward0>) tensor([17.5000,  6.5000,  2.5000,  0.0380,  0.1560,  0.5640,  0.1070]) tensor(0.4097, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1326,  6.2323,  2.2009,  0.0741,  0.2041,  0.5769,  0.1420],\n",
      "       grad_fn=<AddBackward0>) tensor([14.5000,  7.7000,  2.0000,  0.0690,  0.2130,  0.5560,  0.0890]) tensor(0.3333, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5935,  6.3450,  2.6520,  0.0586,  0.1828,  0.5933,  0.1549],\n",
      "       grad_fn=<AddBackward0>) tensor([10.5000,  4.7000,  1.5000,  0.0500,  0.1360,  0.4910,  0.0820]) tensor(4.2850, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1131, 3.0075, 0.7950, 0.0560, 0.1563, 0.5089, 0.0978],\n",
      "       grad_fn=<AddBackward0>) tensor([8.8000, 4.9000, 0.8000, 0.0950, 0.1940, 0.5770, 0.0710]) tensor(1.5442, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3858, 4.1403, 1.8234, 0.0503, 0.1448, 0.5428, 0.1138],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 2.1000, 0.5000, 0.0130, 0.1320, 0.5630, 0.0490]) tensor(2.6826, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.5057, 1.0683, 0.5805, 0.0447, 0.1272, 0.4468, 0.1125],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 4.0000, 1.2000, 0.0690, 0.1890, 0.3280, 0.1300]) tensor(1.2854, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8393, 1.8071, 2.0584, 0.0272, 0.1037, 0.4857, 0.1624],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 0.9000, 1.0000, 0.0270, 0.0670, 0.4710, 0.1710]) tensor(2.1699, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8250, 3.1789, 1.1397, 0.0579, 0.1595, 0.4988, 0.1180],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 0.9000, 0.1000, 0.0220, 0.1350, 0.7370, 0.0250]) tensor(2.4854, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6711,  5.3865,  2.8765,  0.0499,  0.1644,  0.5727,  0.1749],\n",
      "       grad_fn=<AddBackward0>) tensor([14.2000, 10.0000,  2.6000,  0.0900,  0.2310,  0.4780,  0.1370]) tensor(3.0857, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3533, 2.1623, 2.0172, 0.0374, 0.1304, 0.4963, 0.1777],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 0.3000, 0.7000, 0.0000, 0.2000, 0.8330, 0.6670]) tensor(9.1622, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.4878,  5.7685,  3.8501,  0.0389,  0.1482,  0.5884,  0.2127],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5300e+01, 3.5000e+00, 4.4000e+00, 1.4000e-02, 9.5000e-02, 5.5600e-01,\n",
      "        1.9100e-01]) tensor(1.4628, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5098,  4.5446,  2.5456,  0.0498,  0.1514,  0.5455,  0.1590],\n",
      "       grad_fn=<AddBackward0>) tensor([8.8000, 5.5000, 1.7000, 0.0630, 0.1850, 0.5760, 0.0970]) tensor(1.2824, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.6140, 3.8283, 0.0698, 0.0797, 0.1866, 0.5135, 0.0432],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 1.3000, 0.1000, 0.1650, 0.1860, 0.4320, 0.0530]) tensor(1.8915, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2421,  3.0121,  3.0256,  0.0311,  0.1246,  0.5064,  0.2179],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 1.8000, 0.8000, 0.0340, 0.1000, 0.5140, 0.0870]) tensor(3.6134, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0945,  4.7394,  1.9584,  0.0564,  0.1671,  0.5541,  0.1333],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 3.3000, 1.2000, 0.0620, 0.1520, 0.5180, 0.1130]) tensor(3.2642, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.5313,  6.8872,  2.0405,  0.0741,  0.2072,  0.6014,  0.1188],\n",
      "       grad_fn=<AddBackward0>) tensor([9.0000, 7.6000, 1.2000, 0.1030, 0.2480, 0.4820, 0.0870]) tensor(4.5468, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4864, 4.5793, 0.0895, 0.0954, 0.2276, 0.5279, 0.0681],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 3.0000, 0.3000, 0.1250, 0.4120, 0.5240, 0.1000]) tensor(3.6405, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.6501e+01, 3.7789e+00, 4.5275e+00, 1.0237e-02, 9.5485e-02, 5.4604e-01,\n",
      "        2.6628e-01], grad_fn=<AddBackward0>) tensor([9.2000, 2.2000, 2.5000, 0.0130, 0.1080, 0.5040, 0.2180]) tensor(8.5595, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5325, 4.7573, 1.5717, 0.0610, 0.1700, 0.5534, 0.1070],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 5.8000, 1.3000, 0.0720, 0.2020, 0.5890, 0.0980]) tensor(0.9434, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2293,  2.1492,  3.3264,  0.0185,  0.0983,  0.4907,  0.2335],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4000, 1.2000, 1.0000, 0.0340, 0.1130, 0.3760, 0.2070]) tensor(12.0405, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5731,  5.7537,  2.4113,  0.0663,  0.1877,  0.5767,  0.1468],\n",
      "       grad_fn=<AddBackward0>) tensor([11.0000,  4.9000,  3.0000,  0.0290,  0.1590,  0.5460,  0.1710]) tensor(1.1000, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4694, 4.2981, 1.0351, 0.0707, 0.1861, 0.5336, 0.0999],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 3.2000, 0.8000, 0.0560, 0.1800, 0.5930, 0.1050]) tensor(1.4403, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3631, 4.1489, 1.9682, 0.0523, 0.1515, 0.5310, 0.1342],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 3.9000, 0.8000, 0.0510, 0.1270, 0.4730, 0.0450]) tensor(2.6814, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.0153,  3.4071, -0.5103,  0.0781,  0.1833,  0.5070,  0.0194],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 6.5000, 0.4000, 0.0960, 0.2600, 0.3850, 0.0320]) tensor(1.5998, grad_fn=<MseLossBackward0>)\n",
      "tensor([-1.2508,  1.0438, -0.7998,  0.0650,  0.1575,  0.4179,  0.0577],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1290, 0.0000]) tensor(0.9873, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.5582,  3.3222,  3.0954,  0.0394,  0.1473,  0.5130,  0.2370],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9400e+01, 3.9000e+00, 5.1000e+00, 1.5000e-02, 1.2000e-01, 5.2900e-01,\n",
      "        2.8200e-01]) tensor(3.9713, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2695,  4.6058,  2.5730,  0.0437,  0.1401,  0.5631,  0.1456],\n",
      "       grad_fn=<AddBackward0>) tensor([6.7000, 5.6000, 1.0000, 0.0880, 0.1760, 0.5280, 0.0700]) tensor(3.4791, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1210,  4.4305,  2.3117,  0.0474,  0.1436,  0.5527,  0.1359],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 2.7000, 0.9000, 0.0230, 0.1150, 0.5490, 0.0520]) tensor(1.3564, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.7936,  6.4689,  4.2499,  0.0467,  0.1730,  0.6035,  0.2431],\n",
      "       grad_fn=<AddBackward0>) tensor([20.7000,  6.2000,  2.5000,  0.0370,  0.1580,  0.5370,  0.1300]) tensor(0.4515, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.6089,  5.8398,  3.1148,  0.0530,  0.1745,  0.5901,  0.1860],\n",
      "       grad_fn=<AddBackward0>) tensor([10.1000,  4.6000,  0.7000,  0.0880,  0.1660,  0.5170,  0.0630]) tensor(7.1081, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9422,  4.8900,  2.6003,  0.0518,  0.1625,  0.5491,  0.1697],\n",
      "       grad_fn=<AddBackward0>) tensor([10.4000,  5.4000,  0.9000,  0.0780,  0.1670,  0.5260,  0.0550]) tensor(1.3755, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9354,  6.7563,  2.0999,  0.0731,  0.2078,  0.5929,  0.1307],\n",
      "       grad_fn=<AddBackward0>) tensor([11.9000,  3.9000,  3.0000,  0.0290,  0.1240,  0.4830,  0.1710]) tensor(2.6007, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.4936e+01, 2.8795e+00, 4.0196e+00, 1.2000e-02, 9.7246e-02, 5.2952e-01,\n",
      "        2.5806e-01], grad_fn=<AddBackward0>) tensor([15.4000,  2.5000,  6.6000,  0.0170,  0.0790,  0.5220,  0.3840]) tensor(1.0049, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3412, 1.3781, 2.1151, 0.0272, 0.1063, 0.4662, 0.1849],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 1.3000, 1.7000, 0.0000, 0.0920, 0.4240, 0.2560]) tensor(1.3479, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.7460,  1.6522, -0.0082,  0.0548,  0.1377,  0.4718,  0.0585],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.2000, 0.9000, 0.0430, 0.1070, 0.3720, 0.1880]) tensor(0.4129, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4689, 2.0645, 1.4959, 0.0457, 0.1336, 0.4726, 0.1425],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 2.7000, 0.2000, 0.0310, 0.2400, 0.3600, 0.0480]) tensor(1.5616, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6028,  2.7996,  3.4967,  0.0163,  0.0889,  0.5156,  0.2098],\n",
      "       grad_fn=<AddBackward0>) tensor([7.5000, 2.8000, 2.0000, 0.0330, 0.1260, 0.5020, 0.1600]) tensor(1.6960, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16.7026,  7.2457,  2.7070,  0.0698,  0.2032,  0.6123,  0.1475],\n",
      "       grad_fn=<AddBackward0>) tensor([13.8000, 10.7000,  0.9000,  0.1080,  0.2620,  0.5530,  0.0440]) tensor(3.3774, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.2497,  4.0787,  3.9729,  0.0192,  0.1089,  0.5598,  0.2295],\n",
      "       grad_fn=<AddBackward0>) tensor([15.3000,  5.5000,  2.2000,  0.0270,  0.1590,  0.5520,  0.1190]) tensor(0.7401, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5952,  4.1178,  2.8838,  0.0360,  0.1285,  0.5481,  0.1717],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5400e+01, 3.3000e+00, 2.8000e+00, 1.4000e-02, 9.3000e-02, 5.5300e-01,\n",
      "        1.3200e-01]) tensor(2.1651, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 5.4863,  4.7068, -0.2494,  0.0920,  0.2171,  0.5322,  0.0336],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 3.0000, 0.3000, 0.0840, 0.2110, 0.5970, 0.0380]) tensor(0.6285, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5453,  3.8209,  2.6566,  0.0330,  0.1220,  0.5372,  0.1637],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 1.8000, 3.9000, 0.0140, 0.1000, 0.5350, 0.3430]) tensor(3.6319, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7167,  3.6750,  2.8838,  0.0307,  0.1221,  0.5376,  0.1832],\n",
      "       grad_fn=<AddBackward0>) tensor([11.8000,  2.6000,  1.5000,  0.0120,  0.0900,  0.5250,  0.0880]) tensor(0.4411, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.0925, 1.8955, 0.8606, 0.0456, 0.1409, 0.4601, 0.1303],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 0.2000, 0.3000, 0.0400, 0.0360, 0.4700, 0.2000]) tensor(2.5125, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4482,  4.9853,  3.0559,  0.0423,  0.1497,  0.5650,  0.1856],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 2.0000, 0.4000, 0.0290, 0.1200, 0.5400, 0.0480]) tensor(8.9834, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.2625,  4.6135,  3.5664,  0.0324,  0.1337,  0.5614,  0.2133],\n",
      "       grad_fn=<AddBackward0>) tensor([13.9000,  2.3000,  2.1000,  0.0220,  0.0770,  0.5160,  0.1290]) tensor(1.3388, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.4053,  5.7746,  3.6354,  0.0452,  0.1616,  0.5815,  0.2128],\n",
      "       grad_fn=<AddBackward0>) tensor([13.2000,  4.2000,  1.2000,  0.0260,  0.1690,  0.5490,  0.0830]) tensor(3.7305, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9722, 4.4418, 0.1653, 0.0876, 0.2148, 0.5097, 0.0756],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 2.2000, 0.1000, 0.1280, 0.2080, 0.5270, 0.0230]) tensor(2.8612, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8114, 2.5725, 2.5199, 0.0310, 0.1123, 0.4977, 0.1786],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 1.0000, 2.3000, 0.0140, 0.0630, 0.4460, 0.2510]) tensor(6.0522, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.1846,  4.4664,  3.8056,  0.0256,  0.1191,  0.5643,  0.2167],\n",
      "       grad_fn=<AddBackward0>) tensor([11.1000,  4.1000,  9.1000,  0.0320,  0.1120,  0.5390,  0.3850]) tensor(6.4112, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.1877, 0.3697, 0.8999, 0.0318, 0.0970, 0.4428, 0.1161],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 0.3000, 0.3000, 0.0000, 0.0330, 0.7140, 0.0630]) tensor(0.7011, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3960, 3.9746, 1.6656, 0.0571, 0.1645, 0.5226, 0.1363],\n",
      "       grad_fn=<AddBackward0>) tensor([15.4000, 10.0000,  0.9000,  0.1010,  0.2470,  0.5780,  0.0450]) tensor(10.4229, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.4649,  5.0642,  4.5463,  0.0306,  0.1440,  0.5789,  0.2760],\n",
      "       grad_fn=<AddBackward0>) tensor([21.8000,  5.7000,  6.9000,  0.0460,  0.1530,  0.5450,  0.3800]) tensor(1.1056, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0714,  4.6390,  2.3349,  0.0552,  0.1725,  0.5474,  0.1692],\n",
      "       grad_fn=<AddBackward0>) tensor([14.8000,  4.4000,  1.6000,  0.0430,  0.1480,  0.5270,  0.1190]) tensor(0.5127, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.6963,  4.5627,  4.7578,  0.0178,  0.1106,  0.5723,  0.2637],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4200e+01, 1.8000e+00, 2.3000e+00, 1.0000e-02, 6.6000e-02, 5.2900e-01,\n",
      "        1.4500e-01]) tensor(3.7022, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.1920,  4.7785,  3.5029,  0.0385,  0.1423,  0.5586,  0.2105],\n",
      "       grad_fn=<AddBackward0>) tensor([17.8000,  4.7000,  6.8000,  0.0250,  0.1290,  0.5270,  0.3120]) tensor(2.5272, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.6342,  5.5008,  4.5950,  0.0286,  0.1373,  0.5894,  0.2585],\n",
      "       grad_fn=<AddBackward0>) tensor([25.1000,  4.3000,  3.7000,  0.0290,  0.1060,  0.5900,  0.1660]) tensor(4.5897, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.9628,  5.3293,  3.9185,  0.0339,  0.1384,  0.5840,  0.2178],\n",
      "       grad_fn=<AddBackward0>) tensor([14.2000,  2.8000,  1.9000,  0.0250,  0.0710,  0.5150,  0.0880]) tensor(2.5901, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2024,  6.7807,  1.5135,  0.0816,  0.2062,  0.5952,  0.0757],\n",
      "       grad_fn=<AddBackward0>) tensor([5.4000, 6.0000, 0.5000, 0.1090, 0.1820, 0.5630, 0.0320]) tensor(5.0441, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0213,  3.6486,  2.1039,  0.0405,  0.1367,  0.5271,  0.1512],\n",
      "       grad_fn=<AddBackward0>) tensor([8.8000, 3.0000, 1.3000, 0.0160, 0.1340, 0.4800, 0.0950]) tensor(0.3663, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2256, 5.0207, 1.4385, 0.0627, 0.1716, 0.5617, 0.0923],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 3.8000, 0.8000, 0.0440, 0.1440, 0.5800, 0.0540]) tensor(1.9480, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7990,  4.6733,  2.1141,  0.0522,  0.1637,  0.5531,  0.1462],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.8000, 0.3000, 0.0810, 0.1870, 0.4720, 0.0690]) tensor(11.9705, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.8430,  4.3551,  3.5156,  0.0329,  0.1373,  0.5546,  0.2241],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 0.4000, 0.4000, 0.0220, 0.0680, 0.4540, 0.1590]) tensor(30.9996, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3928, 2.2362, 1.1712, 0.0428, 0.1292, 0.5004, 0.1137],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 1.5000, 0.2000, 0.1440, 0.1820, 0.4190, 0.0710]) tensor(2.3801, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.8212, 3.1750, 1.9124, 0.0413, 0.1312, 0.5031, 0.1445],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 4.2000, 3.6000, 0.0110, 0.1590, 0.5980, 0.2000]) tensor(0.7925, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7522, 4.1205, 1.6645, 0.0627, 0.1736, 0.5305, 0.1330],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.9000, 0.4000, 0.0820, 0.1340, 0.3820, 0.0650]) tensor(6.8841, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2781,  4.2299,  2.1810,  0.0461,  0.1443,  0.5506,  0.1376],\n",
      "       grad_fn=<AddBackward0>) tensor([12.6000,  6.4000,  1.4000,  0.0790,  0.1390,  0.5350,  0.0690]) tensor(1.5309, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5182, 3.7864, 1.4816, 0.0541, 0.1517, 0.5166, 0.1153],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 2.3000, 0.6000, 0.0590, 0.1030, 0.5030, 0.0560]) tensor(2.7340, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.8719, 3.8738, 1.8681, 0.0527, 0.1561, 0.5369, 0.1381],\n",
      "       grad_fn=<AddBackward0>) tensor([13.0000,  5.1000,  1.3000,  0.0490,  0.1730,  0.5560,  0.0820]) tensor(1.6593, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4930, 4.3636, 1.6983, 0.0516, 0.1542, 0.5562, 0.1125],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 3.2000, 0.7000, 0.0390, 0.1490, 0.5300, 0.0660]) tensor(3.2201, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5824,  4.3183,  1.4728,  0.0686,  0.1957,  0.5254,  0.1494],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 0.6000, 0.1000, 0.0910, 0.1190, 0.3300, 0.0630]) tensor(17.6511, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2492, 4.8541, 1.0100, 0.0718, 0.1862, 0.5511, 0.0813],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 2.4000, 0.8000, 0.0880, 0.1670, 0.5080, 0.1250]) tensor(4.0895, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.6896, 1.3766, 0.8191, 0.0417, 0.1198, 0.4534, 0.1120],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 1.0000, 0.6000, 0.0520, 0.0830, 0.5270, 0.1120]) tensor(0.1412, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.8429e+01, 4.0923e+00, 4.7343e+00, 1.4934e-02, 1.1070e-01, 5.4804e-01,\n",
      "        2.8844e-01], grad_fn=<AddBackward0>) tensor([1.9000e+01, 3.6000e+00, 5.7000e+00, 1.5000e-02, 9.7000e-02, 5.3200e-01,\n",
      "        2.4500e-01]) tensor(0.2148, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9429,  3.1520,  3.5829,  0.0185,  0.0913,  0.5319,  0.2023],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 1.8000, 3.0000, 0.0380, 0.1010, 0.5950, 0.2770]) tensor(8.2248, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5250, 2.6232, 1.3328, 0.0473, 0.1343, 0.5009, 0.1137],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 4.8000, 1.3000, 0.0500, 0.2000, 0.4690, 0.1010]) tensor(0.8587, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8284, 3.6879, 2.0699, 0.0427, 0.1341, 0.5373, 0.1341],\n",
      "       grad_fn=<AddBackward0>) tensor([12.6000,  5.3000,  1.7000,  0.0540,  0.1400,  0.6030,  0.0830]) tensor(2.4240, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.7843, 4.5877, 0.5794, 0.0802, 0.1968, 0.5345, 0.0683],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 2.9000, 0.2000, 0.0900, 0.2690, 0.3770, 0.0430]) tensor(3.9810, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6702,  5.8138,  2.6739,  0.0589,  0.1780,  0.5710,  0.1648],\n",
      "       grad_fn=<AddBackward0>) tensor([7.7000, 3.6000, 1.1000, 0.0460, 0.1490, 0.5170, 0.0870]) tensor(7.9959, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9101, 4.1432, 2.3197, 0.0499, 0.1489, 0.5395, 0.1472],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 5.8000, 0.3000, 0.1290, 0.2640, 0.4890, 0.0310]) tensor(3.0538, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5394, 1.8142, 2.7732, 0.0192, 0.0973, 0.4863, 0.2076],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 0.8000, 0.7000, 0.0220, 0.0660, 0.4870, 0.1240]) tensor(4.9889, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.0058e+01, 1.8025e+00, 3.6232e+00, 4.6814e-03, 6.8298e-02, 4.9944e-01,\n",
      "        2.2704e-01], grad_fn=<AddBackward0>) tensor([4.7000, 2.0000, 2.4000, 0.0350, 0.1120, 0.4670, 0.2200]) tensor(4.3203, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.6934, 1.3476, 1.1484, 0.0417, 0.1198, 0.4717, 0.1221],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 1.0000, 0.0000, 0.0910, 0.1250, 0.0000, 0.0000]) tensor(2.1886, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5752, 4.6019, 1.7261, 0.0593, 0.1669, 0.5449, 0.1190],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 3.0000, 1.4000, 0.0340, 0.1320, 0.5560, 0.1120]) tensor(1.6466, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.9014,  4.6389,  3.4389,  0.0347,  0.1308,  0.5589,  0.1965],\n",
      "       grad_fn=<AddBackward0>) tensor([6.2000, 1.9000, 2.7000, 0.0340, 0.0980, 0.5200, 0.2840]) tensor(9.6242, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.9242, 1.2086, 1.1895, 0.0399, 0.1244, 0.4467, 0.1501],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 0.5000, 0.8000, 0.0450, 0.0240, 0.6560, 0.1850]) tensor(0.3154, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5111, 2.8812, 1.1451, 0.0476, 0.1372, 0.5041, 0.1038],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 3.5000, 1.9000, 0.0340, 0.1230, 0.5000, 0.1300]) tensor(0.1482, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.6141, 1.3086, 0.1679, 0.0486, 0.1331, 0.4471, 0.0883],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 3.5000, 1.5000, 0.0180, 0.1110, 0.4660, 0.0880]) tensor(3.6878, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1973,  5.7021,  1.7550,  0.0732,  0.1938,  0.5698,  0.1109],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 3.0000, 0.8000, 0.0650, 0.1460, 0.5080, 0.0720]) tensor(5.4916, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.5868, 5.6389, 1.0584, 0.0782, 0.1965, 0.5577, 0.0729],\n",
      "       grad_fn=<AddBackward0>) tensor([6.9000, 5.7000, 0.8000, 0.0860, 0.1770, 0.4780, 0.0490]) tensor(0.4176, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6098,  4.8271,  2.8308,  0.0471,  0.1491,  0.5592,  0.1661],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 3.1000, 0.9000, 0.0330, 0.1690, 0.5280, 0.0780]) tensor(7.5847, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.2398,  6.4305,  4.0498,  0.0471,  0.1680,  0.6057,  0.2218],\n",
      "       grad_fn=<AddBackward0>) tensor([21.4000, 10.2000,  1.5000,  0.0780,  0.2340,  0.5250,  0.0700]) tensor(3.6303, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2887,  4.6800,  2.7656,  0.0454,  0.1421,  0.5580,  0.1548],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 3.3000, 1.4000, 0.0140, 0.1230, 0.5470, 0.0740]) tensor(0.9002, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6077,  5.4791,  2.2697,  0.0573,  0.1648,  0.5808,  0.1222],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 5.5000, 1.1000, 0.1190, 0.1470, 0.6050, 0.0630]) tensor(2.2680, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9921,  5.1172,  2.8544,  0.0457,  0.1488,  0.5722,  0.1594],\n",
      "       grad_fn=<AddBackward0>) tensor([11.0000,  7.3000,  0.9000,  0.0890,  0.1880,  0.5640,  0.0470]) tensor(1.7956, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9048,  5.1627,  2.2679,  0.0571,  0.1625,  0.5607,  0.1306],\n",
      "       grad_fn=<AddBackward0>) tensor([8.9000, 3.1000, 1.1000, 0.0270, 0.1090, 0.6630, 0.0760]) tensor(1.3793, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.8987, 2.5984, 0.7648, 0.0570, 0.1533, 0.4919, 0.0999],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 1.3000, 0.1000, 0.0450, 0.1530, 0.3860, 0.0110]) tensor(1.3472, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2966,  5.4383,  1.4899,  0.0695,  0.1876,  0.5583,  0.1028],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 7.5000, 0.6000, 0.1440, 0.3020, 0.5190, 0.0420]) tensor(3.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4646,  5.0797,  3.5388,  0.0381,  0.1422,  0.5678,  0.2049],\n",
      "       grad_fn=<AddBackward0>) tensor([13.0000,  2.6000,  6.5000,  0.0210,  0.0690,  0.5320,  0.3030]) tensor(3.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6998,  4.9627,  2.5798,  0.0503,  0.1585,  0.5541,  0.1622],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 5.4000, 0.7000, 0.0950, 0.1730, 0.5300, 0.0500]) tensor(5.8497, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.0348,  4.7008,  3.7955,  0.0302,  0.1307,  0.5615,  0.2245],\n",
      "       grad_fn=<AddBackward0>) tensor([13.0000,  3.1000,  4.3000,  0.0270,  0.0950,  0.5140,  0.2420]) tensor(1.7187, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.5541,  6.5155,  2.5351,  0.0639,  0.1850,  0.6004,  0.1351],\n",
      "       grad_fn=<AddBackward0>) tensor([8.5000, 8.0000, 0.5000, 0.1440, 0.2010, 0.5800, 0.0280]) tensor(6.1452, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.1893, 3.6196, 1.6089, 0.0539, 0.1490, 0.5202, 0.1168],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 3.8000, 0.4000, 0.0960, 0.1820, 0.5510, 0.0360]) tensor(1.1727, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.9899, 1.4398, 0.2805, 0.0557, 0.1384, 0.4564, 0.0812],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 1.2000, 0.2000, 0.1050, 0.1900, 0.3490, 0.0490]) tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4163,  3.7439,  3.3506,  0.0250,  0.1209,  0.5529,  0.2129],\n",
      "       grad_fn=<AddBackward0>) tensor([18.3000,  5.1000,  3.0000,  0.0220,  0.1130,  0.5550,  0.1270]) tensor(2.4360, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3535, 3.2176, 2.3303, 0.0430, 0.1302, 0.5075, 0.1581],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 3.1000, 1.1000, 0.0420, 0.1400, 0.4150, 0.0920]) tensor(2.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8646,  4.3134,  3.0395,  0.0357,  0.1268,  0.5565,  0.1709],\n",
      "       grad_fn=<AddBackward0>) tensor([12.3000,  4.4000,  3.6000,  0.0350,  0.0940,  0.5380,  0.1420]) tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.0067e+00,  4.2104e+00, -5.6285e-01,  9.2955e-02,  2.0480e-01,\n",
      "         5.1989e-01,  3.4593e-03], grad_fn=<AddBackward0>) tensor([1.8000, 1.7000, 0.1000, 0.1080, 0.1500, 0.5970, 0.0270]) tensor(0.9706, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.5942,  6.2017,  2.4541,  0.0643,  0.1890,  0.5887,  0.1455],\n",
      "       grad_fn=<AddBackward0>) tensor([13.7000,  9.8000,  1.7000,  0.1380,  0.2450,  0.5940,  0.1200]) tensor(2.0465, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.9039, 2.5500, 1.0287, 0.0507, 0.1404, 0.4934, 0.1053],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 1.4000, 0.8000, 0.0180, 0.0830, 0.5360, 0.1050]) tensor(0.2032, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2008,  5.6669,  1.6827,  0.0742,  0.2084,  0.5556,  0.1386],\n",
      "       grad_fn=<AddBackward0>) tensor([16.8000,  8.2000,  2.3000,  0.0940,  0.1900,  0.4970,  0.1150]) tensor(2.8223, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.6472, 2.4044, 0.4839, 0.0552, 0.1480, 0.4807, 0.0871],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 0.6000, 0.2000, 0.0170, 0.1020, 0.4270, 0.0540]) tensor(1.1991, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.2163,  5.2231,  3.6130,  0.0360,  0.1415,  0.5777,  0.2068],\n",
      "       grad_fn=<AddBackward0>) tensor([8.7000, 2.2000, 3.3000, 0.0140, 0.1060, 0.5140, 0.2540]) tensor(9.3913, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.8161, 1.0374, 2.1343, 0.0264, 0.1064, 0.4615, 0.1974],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 1.1000, 1.5000, 0.0000, 0.0890, 0.3810, 0.1950]) tensor(2.5984, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.5429e+01, 3.3171e+00, 4.4736e+00, 5.7820e-03, 8.4618e-02, 5.4039e-01,\n",
      "        2.6269e-01], grad_fn=<AddBackward0>) tensor([12.1000,  2.0000,  3.3000,  0.0160,  0.0670,  0.5680,  0.2040]) tensor(2.0285, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5383, 2.7007, 1.4621, 0.0554, 0.1601, 0.4701, 0.1577],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 4.0000, 0.6000, 0.1460, 0.3420, 0.3260, 0.1500]) tensor(3.8401, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3230, 3.2938, 1.2974, 0.0520, 0.1502, 0.5152, 0.1153],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 0.7000, 0.5000, 0.0410, 0.1060, 0.5010, 0.1790]) tensor(5.2540, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8164, 3.4742, 1.1511, 0.0553, 0.1541, 0.5161, 0.1038],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 3.0000, 0.3000, 0.0940, 0.1460, 0.5410, 0.0330]) tensor(1.2698, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5387,  6.4660,  2.3445,  0.0679,  0.2001,  0.5935,  0.1465],\n",
      "       grad_fn=<AddBackward0>) tensor([15.4000,  9.7000,  2.3000,  0.1350,  0.2410,  0.5630,  0.1300]) tensor(1.4982, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3500,  3.8348,  3.2466,  0.0316,  0.1222,  0.5275,  0.2027],\n",
      "       grad_fn=<AddBackward0>) tensor([10.3000,  3.4000,  6.9000,  0.0250,  0.0870,  0.4940,  0.3230]) tensor(2.5365, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.3966, 3.1151, 0.9269, 0.0539, 0.1533, 0.5135, 0.1003],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 2.2000, 1.1000, 0.0390, 0.1170, 0.4840, 0.1310]) tensor(0.4885, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.1738, 2.2500, 0.7565, 0.0501, 0.1385, 0.4904, 0.0952],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 1.5000, 0.7000, 0.0320, 0.0870, 0.4900, 0.0680]) tensor(0.1284, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.7304,  6.0413,  2.9311,  0.0550,  0.1743,  0.5960,  0.1660],\n",
      "       grad_fn=<AddBackward0>) tensor([12.6000,  4.1000,  1.8000,  0.0440,  0.1280,  0.5490,  0.1080]) tensor(2.1222, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2312,  3.9838,  1.4637,  0.0674,  0.1831,  0.5368,  0.1310],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 1.2000, 0.3000, 0.3600, 0.0870, 0.2860, 0.1300]) tensor(13.7618, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2073,  5.2935,  1.2730,  0.0695,  0.1885,  0.5588,  0.0965],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 2.7000, 0.1000, 0.0970, 0.1770, 0.5480, 0.0230]) tensor(6.6627, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7649, 3.6780, 2.0577, 0.0442, 0.1422, 0.5266, 0.1490],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 1.9000, 2.0000, 0.0400, 0.0740, 0.5710, 0.1670]) tensor(0.6470, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.1773, 3.0512, 1.1290, 0.0526, 0.1447, 0.4982, 0.1029],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 1.4000, 0.2000, 0.0620, 0.1710, 0.5180, 0.0490]) tensor(2.2408, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3692, 2.5340, 1.3116, 0.0543, 0.1444, 0.4945, 0.1192],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 0.9000, 0.3000, 0.0190, 0.1330, 0.6470, 0.0610]) tensor(1.7077, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.0828, 0.9412, 1.7392, 0.0266, 0.1031, 0.4638, 0.1688],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 1.7000, 0.0000, 0.0630, 0.3640, 0.4700, 0.0000]) tensor(2.5726, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5737,  4.0763,  4.0803,  0.0221,  0.1138,  0.5525,  0.2402],\n",
      "       grad_fn=<AddBackward0>) tensor([11.8000,  2.8000,  3.6000,  0.0220,  0.1050,  0.5340,  0.2530]) tensor(2.3001, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.9131, 0.0991, 1.1486, 0.0449, 0.1298, 0.3816, 0.1905],\n",
      "       grad_fn=<AddBackward0>) tensor([0.4000, 0.8000, 0.2000, 0.0510, 0.0880, 0.3400, 0.0610]) tensor(1.9648, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5931, 2.5455, 1.3940, 0.0407, 0.1239, 0.5091, 0.1125],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 7.0000, 0.5000, 0.1670, 0.2100, 0.5580, 0.0360]) tensor(3.2777, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7993,  5.5411,  1.4655,  0.0696,  0.1899,  0.5636,  0.1025],\n",
      "       grad_fn=<AddBackward0>) tensor([3.9000, 2.0000, 0.7000, 0.0510, 0.1410, 0.5320, 0.1040]) tensor(8.6756, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1407,  4.9575,  3.2378,  0.0391,  0.1363,  0.5708,  0.1744],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 2.5000, 1.6000, 0.0200, 0.1260, 0.6190, 0.1160]) tensor(6.4596, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1813,  4.4833,  2.2286,  0.0437,  0.1450,  0.5510,  0.1430],\n",
      "       grad_fn=<AddBackward0>) tensor([8.2000, 4.4000, 0.7000, 0.0680, 0.1210, 0.5040, 0.0400]) tensor(1.6065, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.7376, 4.3697, 0.7594, 0.0756, 0.1897, 0.5226, 0.0833],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 3.2000, 0.4000, 0.0660, 0.1650, 0.6250, 0.0490]) tensor(0.3412, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0397,  6.2596,  2.7515,  0.0593,  0.1783,  0.5974,  0.1504],\n",
      "       grad_fn=<AddBackward0>) tensor([17.1000,  5.1000,  3.5000,  0.0420,  0.0940,  0.5560,  0.1300]) tensor(0.8799, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.7787,  6.1507,  2.6191,  0.0598,  0.1802,  0.5926,  0.1490],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 3.9000, 1.1000, 0.0540, 0.1500, 0.5520, 0.0770]) tensor(4.7392, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.2760e+01, 2.4075e+00, 4.1480e+00, 9.7865e-03, 8.3137e-02, 5.0514e-01,\n",
      "        2.5921e-01], grad_fn=<AddBackward0>) tensor([3.3000, 0.6000, 1.3000, 0.0250, 0.0520, 0.4670, 0.2640]) tensor(14.4106, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.9929e+01, 4.8711e+00, 4.7896e+00, 1.5613e-02, 1.1637e-01, 5.8128e-01,\n",
      "        2.7558e-01], grad_fn=<AddBackward0>) tensor([15.5000,  3.4000,  5.7000,  0.0190,  0.1050,  0.4750,  0.2980]) tensor(3.2319, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.9844,  5.1894,  3.6015,  0.0433,  0.1569,  0.5675,  0.2225],\n",
      "       grad_fn=<AddBackward0>) tensor([18.3000,  6.0000,  4.3000,  0.0210,  0.1490,  0.5580,  0.1940]) tensor(0.4110, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5761,  3.6427,  2.2157,  0.0505,  0.1596,  0.5135,  0.1811],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 1.8000, 0.7000, 0.0910, 0.1200, 0.4940, 0.1230]) tensor(6.0884, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6363, 0.3929, 0.6659, 0.0310, 0.0975, 0.4350, 0.1084],\n",
      "       grad_fn=<AddBackward0>) tensor([0.5000, 0.2000, 0.1000, 0.0000, 0.0950, 0.8680, 0.1250]) tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1429,  4.6410,  2.8022,  0.0399,  0.1428,  0.5604,  0.1719],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5400e+01, 2.8000e+00, 2.7000e+00, 1.4000e-02, 8.5000e-02, 5.8700e-01,\n",
      "        1.2800e-01]) tensor(1.2144, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2842, 4.1912, 2.3695, 0.0442, 0.1338, 0.5453, 0.1350],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 4.6000, 1.4000, 0.0860, 0.1450, 0.5740, 0.0940]) tensor(3.1609, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.1780, 4.3908, 1.0136, 0.0653, 0.1704, 0.5382, 0.0810],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 5.0000, 0.2000, 0.1530, 0.2350, 0.5680, 0.0160]) tensor(1.8781, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.8229,  4.3911,  3.9418,  0.0270,  0.1235,  0.5617,  0.2303],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([13.8000,  2.8000,  2.8000,  0.0160,  0.0990,  0.5510,  0.1590]) tensor(1.1333, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3208, 5.2526, 1.3328, 0.0688, 0.1776, 0.5587, 0.0801],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 3.2000, 0.6000, 0.0960, 0.1570, 0.4390, 0.0600]) tensor(5.3563, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4831,  5.7125,  2.4753,  0.0652,  0.1908,  0.5619,  0.1670],\n",
      "       grad_fn=<AddBackward0>) tensor([17.9000,  8.5000,  3.1000,  0.0720,  0.2150,  0.5450,  0.1600]) tensor(2.8338, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8631, 4.0558, 1.3688, 0.0599, 0.1674, 0.5288, 0.1167],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 4.3000, 0.5000, 0.1180, 0.1600, 0.6970, 0.0510]) tensor(0.6174, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5977,  5.4863,  2.3444,  0.0610,  0.1803,  0.5742,  0.1484],\n",
      "       grad_fn=<AddBackward0>) tensor([12.4000,  5.7000,  3.2000,  0.0550,  0.1500,  0.5470,  0.1650]) tensor(0.3163, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.5404, 4.4122, 0.7758, 0.0754, 0.1976, 0.5383, 0.0923],\n",
      "       grad_fn=<AddBackward0>) tensor([11.7000,  6.7000,  0.8000,  0.1290,  0.1710,  0.5240,  0.0570]) tensor(2.1746, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6124e+00, 1.3858e+00, 3.1217e+00, 7.8656e-03, 7.2851e-02, 4.7843e-01,\n",
      "        2.1636e-01], grad_fn=<AddBackward0>) tensor([3.8000, 0.9000, 1.0000, 0.0210, 0.0540, 0.4030, 0.1200]) tensor(3.9875, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4540, 2.1931, 1.9093, 0.0366, 0.1225, 0.4949, 0.1571],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 1.7000, 3.0000, 0.0430, 0.0640, 0.4620, 0.2540]) tensor(0.7521, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1779, 3.6160, 1.2955, 0.0578, 0.1638, 0.5177, 0.1202],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 1.7000, 0.1000, 0.1270, 0.2890, 0.3820, 0.0380]) tensor(7.4931, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6200,  4.3250,  2.6920,  0.0458,  0.1460,  0.5402,  0.1697],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 1.7000, 1.5000, 0.0320, 0.0800, 0.5330, 0.1340]) tensor(5.0808, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7219, 4.3873, 1.2713, 0.0670, 0.1740, 0.5367, 0.0954],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 3.4000, 0.5000, 0.0570, 0.1700, 0.4950, 0.0440]) tensor(1.8014, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2753,  3.6279,  2.3322,  0.0485,  0.1523,  0.5297,  0.1711],\n",
      "       grad_fn=<AddBackward0>) tensor([16.8000,  8.9000,  4.6000,  0.0570,  0.2100,  0.5100,  0.2060]) tensor(9.0664, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4548,  5.2678,  1.8276,  0.0633,  0.1743,  0.5638,  0.1117],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 5.2000, 1.2000, 0.0540, 0.1360, 0.6050, 0.0650]) tensor(0.3199, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4956,  4.6261,  3.2402,  0.0382,  0.1420,  0.5501,  0.2027],\n",
      "       grad_fn=<AddBackward0>) tensor([9.0000, 2.7000, 0.9000, 0.0310, 0.0970, 0.5620, 0.0570]) tensor(5.6301, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.8159,  3.4698,  3.9791,  0.0180,  0.1111,  0.5287,  0.2592],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 1.9000, 1.2000, 0.0110, 0.1020, 0.5450, 0.1070]) tensor(6.8022, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4077,  6.9127,  2.5735,  0.0701,  0.2056,  0.6013,  0.1528],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 3.7000, 0.4000, 0.1100, 0.2070, 0.5070, 0.0610]) tensor(16.4598, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.7311, 1.6714, 0.1246, 0.0523, 0.1383, 0.4647, 0.0746],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 0.3000, 0.3000, 0.0000, 0.0290, 0.5110, 0.0530]) tensor(0.4388, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8477,  4.5201,  2.0469,  0.0490,  0.1535,  0.5518,  0.1355],\n",
      "       grad_fn=<AddBackward0>) tensor([12.2000,  4.5000,  1.4000,  0.0220,  0.1450,  0.5670,  0.0770]) tensor(0.3217, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5785,  5.2503,  2.3164,  0.0602,  0.1758,  0.5680,  0.1459],\n",
      "       grad_fn=<AddBackward0>) tensor([18.2000,  9.3000,  2.1000,  0.0890,  0.2140,  0.5660,  0.1010]) tensor(6.8647, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.6166, 3.5519, 0.5117, 0.0690, 0.1712, 0.5209, 0.0647],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 3.3000, 0.8000, 0.0900, 0.1450, 0.5710, 0.0770]) tensor(0.1596, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8528, 2.6462, 1.3953, 0.0498, 0.1436, 0.5005, 0.1295],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 3.4000, 0.6000, 0.0370, 0.1350, 0.5550, 0.0410]) tensor(1.0286, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.8052, 3.8190, 0.5932, 0.0718, 0.1803, 0.5182, 0.0765],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 4.8000, 1.1000, 0.1130, 0.2220, 0.5050, 0.1010]) tensor(0.4984, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.1703,  5.0846,  3.2076,  0.0473,  0.1590,  0.5713,  0.1938],\n",
      "       grad_fn=<AddBackward0>) tensor([19.1000,  7.4000,  2.4000,  0.0320,  0.2110,  0.5710,  0.1290]) tensor(3.0661, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.5873, 2.2177, 0.4299, 0.0610, 0.1637, 0.4693, 0.1067],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 3.8000, 1.1000, 0.1100, 0.1350, 0.5010, 0.1110]) tensor(0.9451, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3474, 4.1574, 1.1906, 0.0628, 0.1725, 0.5269, 0.1080],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 2.0000, 0.6000, 0.0750, 0.1570, 0.5760, 0.0990]) tensor(4.3546, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6583,  5.6113,  2.5939,  0.0559,  0.1651,  0.5805,  0.1405],\n",
      "       grad_fn=<AddBackward0>) tensor([11.0000,  3.8000,  1.3000,  0.0260,  0.1170,  0.6120,  0.0640]) tensor(1.1021, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7621, 1.2119, 2.4709, 0.0225, 0.1094, 0.4607, 0.2254],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 0.2000, 0.6000, 0.0000, 0.0480, 0.5080, 0.2500]) tensor(10.1645, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3103,  5.1176,  2.4100,  0.0537,  0.1637,  0.5658,  0.1468],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 2.9000, 0.8000, 0.0510, 0.1630, 0.5130, 0.0870]) tensor(4.8045, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.0495, 0.9683, 0.7947, 0.0349, 0.1065, 0.4589, 0.1059],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 0.8000, 0.1000, 0.0220, 0.0850, 0.4860, 0.0250]) tensor(0.1344, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.8112, 3.8998, 2.0401, 0.0480, 0.1469, 0.5290, 0.1450],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 3.4000, 0.8000, 0.0280, 0.1310, 0.5510, 0.0440]) tensor(0.2943, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9757, 0.9360, 1.7964, 0.0330, 0.1131, 0.4514, 0.1791],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 2.0000, 2.0000, 0.0000, 0.1360, 0.4910, 0.1500]) tensor(2.1497, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1542,  5.8245,  1.9558,  0.0640,  0.1808,  0.5869,  0.1120],\n",
      "       grad_fn=<AddBackward0>) tensor([8.7000, 6.2000, 1.0000, 0.0860, 0.2010, 0.5920, 0.0610]) tensor(1.8556, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4283,  5.5768,  1.8037,  0.0774,  0.2118,  0.5513,  0.1469],\n",
      "       grad_fn=<AddBackward0>) tensor([16.2000, 10.4000,  2.1000,  0.1080,  0.2820,  0.5230,  0.1100]) tensor(4.4344, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3155, 2.7106, 2.8613, 0.0245, 0.1005, 0.5190, 0.1789],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 2.1000, 2.8000, 0.0110, 0.0860, 0.5110, 0.1620]) tensor(1.1073, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0692,  5.1031,  1.6317,  0.0645,  0.1837,  0.5459,  0.1256],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 2.4000, 0.3000, 0.0550, 0.1450, 0.5040, 0.0440]) tensor(7.6527, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.1026,  0.2694,  0.1427,  0.0334,  0.1054,  0.4221,  0.0960],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 0.1000, 0.1000, 0.0320, 0.0000, 0.5410, 0.1110]) tensor(0.2890, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.1163, 4.2178, 1.2142, 0.0684, 0.1733, 0.5336, 0.0919],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 4.6000, 0.3000, 0.1200, 0.1750, 0.5450, 0.0280]) tensor(1.1953, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5375,  3.9250,  2.2516,  0.0440,  0.1428,  0.5351,  0.1539],\n",
      "       grad_fn=<AddBackward0>) tensor([13.2000,  3.0000,  1.2000,  0.0230,  0.0880,  0.5730,  0.0630]) tensor(1.2948, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9242, 2.2220, 1.4109, 0.0453, 0.1369, 0.4862, 0.1421],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 0.6000, 0.4000, 0.0110, 0.0870, 0.4750, 0.0880]) tensor(3.7111, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5533,  5.0778,  2.5663,  0.0495,  0.1608,  0.5671,  0.1611],\n",
      "       grad_fn=<AddBackward0>) tensor([13.8000,  5.2000,  1.4000,  0.0700,  0.1210,  0.5230,  0.0750]) tensor(0.2068, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3500, 2.5619, 1.8665, 0.0389, 0.1246, 0.4969, 0.1485],\n",
      "       grad_fn=<AddBackward0>) tensor([6.9000, 2.4000, 1.0000, 0.0180, 0.1130, 0.5880, 0.0810]) tensor(0.1418, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3509,  4.8216,  2.0637,  0.0551,  0.1701,  0.5530,  0.1474],\n",
      "       grad_fn=<AddBackward0>) tensor([7.7000, 2.0000, 1.1000, 0.0230, 0.1130, 0.5150, 0.1020]) tensor(4.3613, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.0742, 2.9930, 0.4542, 0.0647, 0.1688, 0.4880, 0.0904],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 1.2000, 0.3000, 0.0920, 0.1850, 0.5300, 0.1090]) tensor(2.6073, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5354,  4.7846,  2.3135,  0.0508,  0.1571,  0.5557,  0.1454],\n",
      "       grad_fn=<AddBackward0>) tensor([8.7000, 3.3000, 1.4000, 0.0710, 0.1150, 0.5330, 0.1110]) tensor(1.5831, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1278, 3.2177, 2.3810, 0.0384, 0.1268, 0.5221, 0.1588],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 2.4000, 2.0000, 0.0250, 0.0960, 0.4800, 0.1410]) tensor(0.5940, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.5634, 2.5461, 0.3131, 0.0611, 0.1649, 0.4807, 0.0920],\n",
      "       grad_fn=<AddBackward0>) tensor([0.7000, 0.2000, 0.0000, 0.0670, 0.0000, 0.3330, 0.0000]) tensor(2.9409, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6389,  3.9018,  1.8122,  0.0526,  0.1616,  0.5329,  0.1466],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 2.4000, 0.6000, 0.0820, 0.1750, 0.4900, 0.0890]) tensor(4.7589, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.1931,  4.8163,  4.4016,  0.0200,  0.1185,  0.5735,  0.2522],\n",
      "       grad_fn=<AddBackward0>) tensor([16.1000,  5.1000,  7.8000,  0.0250,  0.1470,  0.5460,  0.3780]) tensor(2.2897, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5506, 3.8281, 1.7437, 0.0471, 0.1486, 0.5342, 0.1319],\n",
      "       grad_fn=<AddBackward0>) tensor([6.2000, 5.0000, 0.8000, 0.0740, 0.2130, 0.5390, 0.0860]) tensor(1.9282, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9912,  3.8117,  3.4497,  0.0272,  0.1107,  0.5404,  0.1972],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 2.6000, 4.7000, 0.0270, 0.1020, 0.4900, 0.3030]) tensor(6.8311, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.6428, -0.2491,  0.4457,  0.0319,  0.0935,  0.4243,  0.1039],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 1.2000, 1.7000, 0.0200, 0.0930, 0.4900, 0.2380]) tensor(1.6824, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5008, 3.3783, 1.7511, 0.0475, 0.1390, 0.5172, 0.1267],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 3.9000, 1.0000, 0.0700, 0.1850, 0.5790, 0.0840]) tensor(0.6926, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.3501, 2.0846, 0.3274, 0.0580, 0.1445, 0.4728, 0.0706],\n",
      "       grad_fn=<AddBackward0>) tensor([0.8000, 1.3000, 0.1000, 0.0820, 0.2440, 0.4190, 0.0450]) tensor(0.1406, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7818,  4.4033,  2.5880,  0.0432,  0.1431,  0.5442,  0.1636],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 4.7000, 1.5000, 0.0500, 0.1420, 0.5420, 0.0940]) tensor(2.3349, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6549,  5.3142,  3.3332,  0.0437,  0.1502,  0.5754,  0.1866],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 2.9000, 3.7000, 0.0310, 0.0960, 0.4960, 0.2230]) tensor(4.5037, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4324,  3.7517,  3.8109,  0.0200,  0.1047,  0.5410,  0.2216],\n",
      "       grad_fn=<AddBackward0>) tensor([11.3000,  3.7000,  6.7000,  0.0220,  0.0970,  0.4680,  0.3170]) tensor(1.8445, grad_fn=<MseLossBackward0>)\n",
      "tensor([-5.9729,  2.4371, -2.5321,  0.0972,  0.1963,  0.4578, -0.0753],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 3.0000, 0.0000, 0.1000, 0.5000, 0.0000, 0.0000]) tensor(6.1016, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5219, 1.4900, 1.3379, 0.0339, 0.1265, 0.4356, 0.1731],\n",
      "       grad_fn=<AddBackward0>) tensor([0.7000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000]) tensor(5.4224, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.2920, 1.7898, 0.4594, 0.0554, 0.1528, 0.4572, 0.1138],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 1.5000, 1.0000, 0.2500, 0.2860, 0.3330, 0.3330]) tensor(1.6190, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2466, 5.1197, 0.8516, 0.0761, 0.1939, 0.5536, 0.0722],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 2.1000, 0.2000, 0.0760, 0.1430, 0.4160, 0.0300]) tensor(5.1507, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7912,  5.5069,  1.7307,  0.0663,  0.1816,  0.5712,  0.1059],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 4.1000, 0.8000, 0.0770, 0.1740, 0.6430, 0.0690]) tensor(2.9169, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7427,  4.8181,  1.7168,  0.0719,  0.2071,  0.5482,  0.1594],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 1.3000, 0.3000, 0.0260, 0.1600, 0.5280, 0.0870]) tensor(13.9978, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4812,  5.2731,  1.4962,  0.0666,  0.1833,  0.5691,  0.1008],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 5.1000, 1.5000, 0.0420, 0.2130, 0.5610, 0.1030]) tensor(0.6233, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0940,  3.3566,  1.9858,  0.0449,  0.1458,  0.5214,  0.1572],\n",
      "       grad_fn=<AddBackward0>) tensor([13.9000,  4.2000,  3.0000,  0.0250,  0.1270,  0.5240,  0.1450]) tensor(2.3180, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4720,  5.4417,  3.1162,  0.0444,  0.1551,  0.5812,  0.1815],\n",
      "       grad_fn=<AddBackward0>) tensor([6.8000, 4.6000, 1.2000, 0.0860, 0.2120, 0.4860, 0.1100]) tensor(11.3718, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.8812,  5.7854,  2.8008,  0.0520,  0.1674,  0.5934,  0.1584],\n",
      "       grad_fn=<AddBackward0>) tensor([14.6000,  6.2000,  2.5000,  0.0570,  0.1340,  0.5640,  0.1140]) tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8874,  5.7840,  2.0865,  0.0711,  0.1955,  0.5656,  0.1378],\n",
      "       grad_fn=<AddBackward0>) tensor([15.5000, 13.5000,  1.2000,  0.1230,  0.2940,  0.6440,  0.0580]) tensor(9.5962, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6430, 2.2751, 1.2303, 0.0495, 0.1358, 0.5017, 0.1145],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 0.6000, 0.8000, 0.0220, 0.0980, 0.5420, 0.1940]) tensor(3.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3028, 1.3133, 1.3817, 0.0353, 0.1178, 0.4603, 0.1508],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 1.5000, 0.0000, 0.0770, 0.1820, 0.2500, 0.0000]) tensor(2.3540, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6050,  4.2942,  3.9392,  0.0241,  0.1142,  0.5627,  0.2197],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8900e+01, 3.5000e+00, 3.2000e+00, 1.4000e-02, 8.6000e-02, 5.7400e-01,\n",
      "        1.4500e-01]) tensor(2.8044, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5300,  5.5497,  2.6156,  0.0553,  0.1689,  0.5782,  0.1523],\n",
      "       grad_fn=<AddBackward0>) tensor([13.9000,  6.1000,  1.5000,  0.0350,  0.1760,  0.5680,  0.0730]) tensor(0.2416, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3791, 3.1976, 1.6304, 0.0499, 0.1499, 0.5015, 0.1447],\n",
      "       grad_fn=<AddBackward0>) tensor([9.5000, 4.5000, 1.2000, 0.0810, 0.1680, 0.5340, 0.1020]) tensor(0.4489, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 4.8351,  5.5540, -1.4099,  0.1193,  0.2684,  0.5312, -0.0099],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 0.9000, 0.1000, 0.0690, 0.1210, 0.4670, 0.0300]) tensor(5.1099, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3087, 1.4343, 1.6101, 0.0393, 0.1196, 0.4646, 0.1536],\n",
      "       grad_fn=<AddBackward0>) tensor([0.6000, 0.4000, 0.0000, 0.0000, 0.1430, 0.3570, 0.0000]) tensor(3.6960, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9893, 6.6054, 0.6210, 0.0950, 0.2329, 0.5771, 0.0543],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 2.8000, 0.3000, 0.0780, 0.2150, 0.3850, 0.0380]) tensor(10.5354, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8480,  4.3720,  2.5170,  0.0420,  0.1377,  0.5517,  0.1494],\n",
      "       grad_fn=<AddBackward0>) tensor([5.4000, 2.1000, 1.0000, 0.0480, 0.0890, 0.5330, 0.0830]) tensor(5.3073, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.7346e+01, 4.1371e+00, 4.8550e+00, 7.7192e-03, 9.1593e-02, 5.6729e-01,\n",
      "        2.6747e-01], grad_fn=<AddBackward0>) tensor([14.0000,  2.7000,  5.1000,  0.0230,  0.0790,  0.5240,  0.2530]) tensor(1.9036, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7137,  3.6468,  3.0718,  0.0311,  0.1194,  0.5371,  0.1884],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 2.9000, 6.1000, 0.0130, 0.0940, 0.5260, 0.3090]) tensor(2.2242, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4140,  5.1174,  3.5958,  0.0331,  0.1387,  0.5783,  0.2089],\n",
      "       grad_fn=<AddBackward0>) tensor([11.8000,  2.9000,  2.1000,  0.0260,  0.0920,  0.4500,  0.1210]) tensor(4.0671, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.2371, 4.6399, 0.6014, 0.0855, 0.2049, 0.5391, 0.0702],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 2.3000, 0.4000, 0.1060, 0.1940, 0.3560, 0.0780]) tensor(4.7110, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.8225,  4.1783,  4.0472,  0.0240,  0.1239,  0.5492,  0.2532],\n",
      "       grad_fn=<AddBackward0>) tensor([19.5000,  5.0000,  2.5000,  0.0330,  0.1180,  0.5410,  0.1140]) tensor(1.4653, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1842, 0.5832, 2.8392, 0.0097, 0.0774, 0.4513, 0.2300],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 1.0000, 2.3000, 0.0210, 0.0820, 0.4900, 0.3780]) tensor(2.1155, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9331, 1.4730, 2.6947, 0.0189, 0.0903, 0.4731, 0.2015],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 1.6000, 3.3000, 0.0240, 0.0870, 0.4230, 0.2760]) tensor(1.9414, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0860, 4.8053, 1.0996, 0.0774, 0.1976, 0.5452, 0.0971],\n",
      "       grad_fn=<AddBackward0>) tensor([11.6000,  8.0000,  1.7000,  0.1210,  0.2480,  0.5000,  0.1080]) tensor(2.4134, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5295,  6.7486,  2.1865,  0.0769,  0.2147,  0.5850,  0.1438],\n",
      "       grad_fn=<AddBackward0>) tensor([15.5000, 11.4000,  2.0000,  0.0990,  0.2680,  0.5670,  0.0980]) tensor(3.0967, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 4.8219,  4.0576, -0.1069,  0.0844,  0.2016,  0.5148,  0.0469],\n",
      "       grad_fn=<AddBackward0>) tensor([16.7000, 16.3000,  0.8000,  0.1520,  0.3710,  0.6780,  0.0420]) tensor(41.6926, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6957, 4.6991, 2.2182, 0.0542, 0.1516, 0.5585, 0.1228],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 5.7000, 0.4000, 0.0880, 0.2760, 0.5310, 0.0260]) tensor(4.3286, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3720, 3.8337, 1.4091, 0.0628, 0.1652, 0.5141, 0.1148],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 5.6000, 1.0000, 0.1110, 0.1970, 0.5120, 0.0760]) tensor(1.4153, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.1218, -0.4979,  1.1819,  0.0142,  0.0683,  0.4257,  0.1402],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 3.6000, 3.0000, 0.0400, 0.1270, 0.3800, 0.1750]) tensor(4.4074, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7273, 3.5078, 2.2221, 0.0445, 0.1397, 0.5153, 0.1605],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 1.6000, 1.0000, 0.0210, 0.0950, 0.5460, 0.0940]) tensor(3.7933, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.1718, 1.5161, 1.3395, 0.0365, 0.1133, 0.4754, 0.1278],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 0.7000, 0.4000, 0.0160, 0.0760, 0.5300, 0.0740]) tensor(0.2690, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9078, 2.5853, 2.2849, 0.0287, 0.1083, 0.5108, 0.1572],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 2.3000, 1.2000, 0.0190, 0.0930, 0.5510, 0.0790]) tensor(0.6398, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.0029, 2.0248, 0.2775, 0.0574, 0.1498, 0.4822, 0.0794],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 2.0000, 0.0000, 0.0650, 0.1290, 0.4170, 0.0000]) tensor(0.0253, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.8321, 2.5748, 1.7355, 0.0380, 0.1189, 0.4953, 0.1311],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 0.9000, 0.5000, 0.0560, 0.0550, 0.3890, 0.0930]) tensor(2.8300, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1968,  3.5609,  2.7820,  0.0312,  0.1206,  0.5338,  0.1779],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 1.3000, 1.9000, 0.0120, 0.0600, 0.5030, 0.1390]) tensor(3.1244, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.7133, 3.2690, 1.2670, 0.0491, 0.1425, 0.5192, 0.1054],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 2.4000, 0.6000, 0.0610, 0.1170, 0.4490, 0.0560]) tensor(1.6476, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3222,  5.2233,  1.3832,  0.0738,  0.1997,  0.5582,  0.1133],\n",
      "       grad_fn=<AddBackward0>) tensor([13.2000, 10.0000,  1.2000,  0.1180,  0.2280,  0.4640,  0.0600]) tensor(3.7701, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3754, 4.1294, 1.6819, 0.0555, 0.1617, 0.5277, 0.1302],\n",
      "       grad_fn=<AddBackward0>) tensor([6.2000, 3.5000, 0.8000, 0.0880, 0.1420, 0.5280, 0.0870]) tensor(1.6086, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6310,  6.9193,  1.8976,  0.0799,  0.2131,  0.5994,  0.1081],\n",
      "       grad_fn=<AddBackward0>) tensor([12.3000, 12.0000,  0.8000,  0.1590,  0.2470,  0.5730,  0.0440]) tensor(4.1146, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8032, 1.1909, 0.3291, 0.0504, 0.1300, 0.4512, 0.0870],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 1.9000, 0.3000, 0.1160, 0.1420, 0.5790, 0.0510]) tensor(0.4864, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.5694,  5.0983,  2.9166,  0.0449,  0.1566,  0.5670,  0.1814],\n",
      "       grad_fn=<AddBackward0>) tensor([16.8000,  5.2000,  1.5000,  0.0520,  0.1140,  0.5020,  0.0680]) tensor(1.0017, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.0398,  6.0504,  3.5789,  0.0474,  0.1676,  0.5964,  0.2061],\n",
      "       grad_fn=<AddBackward0>) tensor([19.3000,  4.9000,  4.1000,  0.0240,  0.1410,  0.5670,  0.2110]) tensor(0.4550, grad_fn=<MseLossBackward0>)\n",
      "tensor([-6.0312,  0.3780, -1.4221,  0.0577,  0.1339,  0.3926,  0.0142],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([0., 0., 0., 0., 0., 0., 0.]) tensor(5.5309, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6006,  3.1833,  3.3809,  0.0264,  0.1140,  0.5299,  0.2152],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2100e+01, 2.7000e+00, 2.9000e+00, 1.1000e-02, 9.1000e-02, 4.6000e-01,\n",
      "        1.4900e-01]) tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.7876, 1.0699, 1.4235, 0.0320, 0.1080, 0.4653, 0.1456],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 2.0000, 1.5000, 0.0140, 0.1470, 0.5590, 0.2310]) tensor(0.1270, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5010,  4.7227,  2.3476,  0.0483,  0.1578,  0.5596,  0.1545],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 2.1000, 0.4000, 0.0690, 0.1640, 0.4990, 0.0800]) tensor(11.6085, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.7183, 2.4744, 0.8885, 0.0534, 0.1500, 0.4932, 0.1112],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 1.1000, 0.2000, 0.0430, 0.1520, 0.3160, 0.0430]) tensor(2.8848, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0325,  6.1710,  1.4725,  0.0793,  0.2088,  0.5687,  0.1056],\n",
      "       grad_fn=<AddBackward0>) tensor([6.8000, 5.2000, 0.8000, 0.1520, 0.2080, 0.5710, 0.0680]) tensor(4.1115, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.8610,  4.0567,  3.8865,  0.0226,  0.1130,  0.5465,  0.2304],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 1.9000, 3.2000, 0.0170, 0.0860, 0.4690, 0.2660]) tensor(7.8555, grad_fn=<MseLossBackward0>)\n",
      "tensor([22.7781,  7.0869,  4.4913,  0.0480,  0.1817,  0.6245,  0.2514],\n",
      "       grad_fn=<AddBackward0>) tensor([25.6000,  7.3000,  2.9000,  0.0500,  0.1840,  0.5570,  0.1490]) tensor(1.5080, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8605,  6.3142,  2.0152,  0.0725,  0.1927,  0.5900,  0.1056],\n",
      "       grad_fn=<AddBackward0>) tensor([10.0000, 13.5000,  1.0000,  0.1350,  0.2940,  0.5470,  0.0480]) tensor(8.0209, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.2861, 0.8278, 2.3312, 0.0174, 0.0787, 0.4619, 0.1791],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 1.0000, 1.1000, 0.0100, 0.0550, 0.5070, 0.1110]) tensor(0.3340, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1625, 0.5895, 2.3744, 0.0171, 0.0818, 0.4648, 0.1900],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 1.6000, 3.2000, 0.0000, 0.1070, 0.1770, 0.3330]) tensor(3.2320, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4960, 3.1688, 2.6028, 0.0325, 0.1170, 0.5264, 0.1664],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 2.6000, 4.5000, 0.0260, 0.1010, 0.5360, 0.2970]) tensor(1.3830, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9875, 3.3065, 1.1824, 0.0514, 0.1425, 0.5287, 0.0911],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 1.8000, 0.4000, 0.0440, 0.1600, 0.6610, 0.0740]) tensor(2.1518, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.7839,  6.8461,  2.5821,  0.0730,  0.2011,  0.5982,  0.1390],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 8.4000, 0.9000, 0.1480, 0.3090, 0.6100, 0.0760]) tensor(9.4077, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0897,  3.8610,  3.1751,  0.0292,  0.1170,  0.5399,  0.1897],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 2.3000, 3.4000, 0.0180, 0.0940, 0.5550, 0.2490]) tensor(4.9806, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0866, 4.4423, 1.4847, 0.0612, 0.1708, 0.5341, 0.1165],\n",
      "       grad_fn=<AddBackward0>) tensor([10.2000,  7.9000,  0.9000,  0.0850,  0.2700,  0.5940,  0.0570]) tensor(1.9364, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9186, 3.2494, 2.3161, 0.0373, 0.1245, 0.5199, 0.1552],\n",
      "       grad_fn=<AddBackward0>) tensor([6.7000, 2.1000, 2.8000, 0.0110, 0.0810, 0.5080, 0.1870]) tensor(0.9259, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.5627, 2.3419, 0.4901, 0.0584, 0.1532, 0.4799, 0.0893],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 3.5000, 0.3000, 0.1960, 0.1750, 0.5080, 0.0560]) tensor(0.2271, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6507,  5.6056,  1.4306,  0.0767,  0.2001,  0.5656,  0.1003],\n",
      "       grad_fn=<AddBackward0>) tensor([15.6000,  9.3000,  3.9000,  0.0770,  0.1980,  0.5960,  0.1600]) tensor(6.3210, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4111, 4.4933, 1.5899, 0.0618, 0.1669, 0.5397, 0.1083],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 4.1000, 1.4000, 0.0680, 0.1690, 0.4600, 0.1150]) tensor(1.5943, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7930, 3.4961, 1.9211, 0.0395, 0.1311, 0.5268, 0.1358],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 2.6000, 2.4000, 0.0570, 0.1120, 0.4870, 0.2020]) tensor(2.2036, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1018,  5.6084,  2.3633,  0.0597,  0.1709,  0.5744,  0.1324],\n",
      "       grad_fn=<AddBackward0>) tensor([12.1000,  8.1000,  2.3000,  0.0930,  0.1660,  0.4880,  0.1100]) tensor(0.8887, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3533, 3.6729, 1.4793, 0.0550, 0.1572, 0.5138, 0.1272],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 1.3000, 0.2000, 0.0890, 0.0950, 0.5770, 0.0430]) tensor(5.2888, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6111,  4.6101,  2.3566,  0.0463,  0.1486,  0.5547,  0.1476],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 2.5000, 0.9000, 0.0120, 0.1110, 0.5120, 0.0620]) tensor(1.8417, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.3618,  6.2858,  2.9408,  0.0553,  0.1777,  0.5898,  0.1717],\n",
      "       grad_fn=<AddBackward0>) tensor([16.1000,  2.6000,  2.8000,  0.0190,  0.0700,  0.5930,  0.1370]) tensor(1.9554, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1804,  3.7029,  2.5775,  0.0363,  0.1306,  0.5360,  0.1703],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 2.6000, 1.5000, 0.0370, 0.0940, 0.5210, 0.1140]) tensor(0.6125, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0888, 4.9050, 1.7471, 0.0612, 0.1642, 0.5554, 0.1034],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 6.8000, 2.5000, 0.0390, 0.1620, 0.5990, 0.1030]) tensor(1.2176, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0008,  4.9773,  2.6931,  0.0512,  0.1596,  0.5529,  0.1687],\n",
      "       grad_fn=<AddBackward0>) tensor([7.7000, 6.4000, 0.8000, 0.1040, 0.2070, 0.5630, 0.0530]) tensor(4.8178, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1618, 2.2742, 2.7921, 0.0229, 0.1000, 0.5017, 0.1913],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 1.8000, 2.1000, 0.0350, 0.0950, 0.5520, 0.2320]) tensor(2.5756, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5885, 6.2592, 1.0048, 0.0817, 0.2049, 0.5862, 0.0579],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 4.5000, 1.1000, 0.0820, 0.1450, 0.5210, 0.0700]) tensor(2.6046, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6247, 2.1950, 1.8479, 0.0374, 0.1148, 0.4911, 0.1388],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 1.4000, 1.1000, 0.0090, 0.0660, 0.5860, 0.0690]) tensor(0.2585, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.5055,  7.7516,  2.9589,  0.0751,  0.2167,  0.6244,  0.1609],\n",
      "       grad_fn=<AddBackward0>) tensor([11.7000,  6.6000,  2.3000,  0.1120,  0.2370,  0.5720,  0.1650]) tensor(6.8685, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9359,  6.6850,  1.6843,  0.0751,  0.2047,  0.5984,  0.0972],\n",
      "       grad_fn=<AddBackward0>) tensor([9.4000, 7.5000, 1.3000, 0.1310, 0.2040, 0.5750, 0.0750]) tensor(1.9026, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6606, 4.6353, 0.8970, 0.0710, 0.1898, 0.5412, 0.0905],\n",
      "       grad_fn=<AddBackward0>) tensor([5.9000, 3.8000, 0.9000, 0.0820, 0.1740, 0.4550, 0.0940]) tensor(1.1895, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6761, 4.4828, 0.4008, 0.0784, 0.1915, 0.5190, 0.0618],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 3.6000, 0.9000, 0.1330, 0.1540, 0.4850, 0.0930]) tensor(1.2488, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7636, 3.7497, 2.0107, 0.0396, 0.1301, 0.5316, 0.1321],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 3.2000, 0.9000, 0.0440, 0.1160, 0.5700, 0.0610]) tensor(1.3114, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.3017, 2.2534, 0.7855, 0.0530, 0.1367, 0.4972, 0.0833],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 2.2000, 1.0000, 0.0340, 0.1090, 0.3400, 0.0760]) tensor(0.1540, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9306, 1.4218, 2.3381, 0.0224, 0.0989, 0.4738, 0.1912],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 1.1000, 1.5000, 0.0880, 0.0880, 0.3380, 0.4180]) tensor(6.0332, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.6706, 0.4157, 1.2353, 0.0305, 0.1115, 0.4300, 0.1686],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 1.8000, 0.3000, 0.0330, 0.1940, 0.4000, 0.0530]) tensor(0.9016, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.6978, 2.1289, 0.6870, 0.0529, 0.1325, 0.4779, 0.0781],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 0.9000, 0.8000, 0.0110, 0.0990, 0.4990, 0.1350]) tensor(0.2241, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.6200,  6.2635,  2.7767,  0.0570,  0.1784,  0.6006,  0.1556],\n",
      "       grad_fn=<AddBackward0>) tensor([11.3000,  8.8000,  0.7000,  0.1190,  0.2480,  0.5790,  0.0410]) tensor(4.2045, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.1598, 2.8822, 1.0480, 0.0544, 0.1415, 0.5014, 0.0906],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 2.2000, 0.2000, 0.0900, 0.1870, 0.6060, 0.0300]) tensor(0.8381, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7831,  4.8168,  2.6358,  0.0519,  0.1597,  0.5502,  0.1682],\n",
      "       grad_fn=<AddBackward0>) tensor([11.0000,  7.6000,  7.3000,  0.0360,  0.1830,  0.5560,  0.2860]) tensor(4.6707, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2125,  4.2752,  1.4349,  0.0681,  0.1918,  0.5357,  0.1381],\n",
      "       grad_fn=<AddBackward0>) tensor([8.7000, 5.3000, 0.7000, 0.1130, 0.2080, 0.4200, 0.0630]) tensor(1.1320, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.6019,  2.5419, -0.4793,  0.0739,  0.1760,  0.4737,  0.0454],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 2.7000, 0.6000, 0.0930, 0.0850, 0.5300, 0.0600]) tensor(2.3424, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.8774,  6.2346,  2.6141,  0.0670,  0.2028,  0.5884,  0.1733],\n",
      "       grad_fn=<AddBackward0>) tensor([21.8000,  8.5000,  1.5000,  0.0760,  0.1800,  0.5190,  0.0820]) tensor(4.3741, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.0596, 1.2799, 0.4743, 0.0441, 0.1177, 0.4684, 0.0795],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 2.3000, 0.8000, 0.0270, 0.1280, 0.4020, 0.0830]) tensor(0.3503, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.9436, 0.9532, 1.6910, 0.0265, 0.0982, 0.4680, 0.1543],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 1.0000, 1.3000, 0.0190, 0.0630, 0.5530, 0.1310]) tensor(0.0264, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.3070, 3.2655, 1.4316, 0.0520, 0.1434, 0.5194, 0.1078],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 4.9000, 0.4000, 0.1200, 0.2230, 0.6360, 0.0430]) tensor(0.6092, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.0403,  5.0095,  3.6242,  0.0369,  0.1484,  0.5671,  0.2256],\n",
      "       grad_fn=<AddBackward0>) tensor([17.8000,  6.5000,  2.2000,  0.0530,  0.1510,  0.5270,  0.1050]) tensor(0.6919, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1590,  4.4911,  3.3214,  0.0358,  0.1350,  0.5497,  0.2026],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 1.5000, 0.8000, 0.0230, 0.0690, 0.5610, 0.0680]) tensor(9.1077, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.9269, 1.9122, 0.1867, 0.0565, 0.1432, 0.4771, 0.0690],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 2.0000, 0.6000, 0.0650, 0.1450, 0.5140, 0.0880]) tensor(0.0364, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1962,  4.7148,  2.3823,  0.0510,  0.1636,  0.5576,  0.1627],\n",
      "       grad_fn=<AddBackward0>) tensor([16.5000,  6.4000,  2.4000,  0.0790,  0.1340,  0.5400,  0.1210]) tensor(1.9656, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.2074,  5.2656,  3.4411,  0.0437,  0.1560,  0.5750,  0.2055],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 2.3000, 0.3000, 0.0460, 0.1850, 0.5340, 0.0470]) tensor(16.6923, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.1986,  6.7490,  2.8271,  0.0658,  0.1996,  0.6006,  0.1685],\n",
      "       grad_fn=<AddBackward0>) tensor([14.5000,  6.6000,  2.4000,  0.0450,  0.1870,  0.5600,  0.1400]) tensor(1.0700, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5328, 3.8412, 1.6712, 0.0518, 0.1574, 0.5311, 0.1335],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 3.1000, 1.2000, 0.0520, 0.1200, 0.4380, 0.1020]) tensor(0.8892, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.6781, 2.4186, 0.6461, 0.0521, 0.1411, 0.4844, 0.0895],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 2.2000, 0.4000, 0.1260, 0.0760, 0.4670, 0.0520]) tensor(0.2154, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.7638, 1.1559, 1.4224, 0.0330, 0.1106, 0.4644, 0.1458],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 1.8000, 0.5000, 0.0910, 0.1870, 0.5280, 0.1110]) tensor(0.8517, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4009,  4.8140,  3.0014,  0.0389,  0.1446,  0.5652,  0.1840],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 5.4000, 2.6000, 0.0620, 0.1380, 0.4350, 0.1480]) tensor(3.9389, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4681,  2.9663,  2.8624,  0.0312,  0.1272,  0.5097,  0.2139],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 0.9000, 1.2000, 0.0420, 0.1090, 0.4970, 0.3000]) tensor(14.0843, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.7909, 4.2183, 0.6502, 0.0697, 0.1813, 0.5321, 0.0753],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 1.9000, 0.5000, 0.0680, 0.1690, 0.3950, 0.0920]) tensor(3.2828, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.6037, 2.1562, 1.3279, 0.0439, 0.1356, 0.4821, 0.1413],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 0.9000, 0.4000, 0.0330, 0.1010, 0.4280, 0.0860]) tensor(2.6395, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.9297,  5.2228,  4.5233,  0.0275,  0.1273,  0.5790,  0.2474],\n",
      "       grad_fn=<AddBackward0>) tensor([16.3000,  4.5000,  6.5000,  0.0500,  0.0980,  0.5480,  0.2980]) tensor(1.0129, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5567,  4.5577,  2.3797,  0.0500,  0.1585,  0.5575,  0.1576],\n",
      "       grad_fn=<AddBackward0>) tensor([13.7000,  8.2000,  1.6000,  0.1090,  0.1670,  0.4860,  0.0790]) tensor(2.1709, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5232,  4.3314,  3.1451,  0.0354,  0.1246,  0.5523,  0.1732],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 3.7000, 8.0000, 0.0280, 0.1070, 0.5050, 0.3960]) tensor(5.6304, grad_fn=<MseLossBackward0>)\n",
      "tensor([-3.0545,  0.3813, -0.3626,  0.0447,  0.1134,  0.4244,  0.0528],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 0.3000, 0.0000, 0.0000, 0.1110, 0.0000, 0.0000]) tensor(1.3790, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7265, 3.7058, 2.0189, 0.0493, 0.1506, 0.5193, 0.1520],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 1.7000, 0.9000, 0.0340, 0.1040, 0.4670, 0.1030]) tensor(3.5538, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4244, 4.1993, 1.2618, 0.0615, 0.1762, 0.5299, 0.1187],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 1.6000, 0.5000, 0.1090, 0.1180, 0.4820, 0.1140]) tensor(5.7306, grad_fn=<MseLossBackward0>)\n",
      "tensor([-3.8667,  0.4441, -1.2069,  0.0602,  0.1401,  0.4144,  0.0272],\n",
      "       grad_fn=<AddBackward0>) tensor([0.8000, 0.7000, 0.0000, 0.1110, 0.0670, 0.4600, 0.0000]) tensor(3.3302, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.7140, 2.9118, 1.2579, 0.0528, 0.1484, 0.5056, 0.1180],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 3.3000, 1.1000, 0.0320, 0.1400, 0.5330, 0.0920]) tensor(0.0272, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.6173, 2.1338, 0.8778, 0.0545, 0.1412, 0.4756, 0.1034],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000, 0.4000, 0.3000, 0.0120, 0.0830, 0.5260, 0.0980]) tensor(1.3835, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7611,  5.8728,  1.8697,  0.0661,  0.1896,  0.5860,  0.1179],\n",
      "       grad_fn=<AddBackward0>) tensor([13.5000,  6.4000,  1.1000,  0.0860,  0.1600,  0.5410,  0.0630]) tensor(0.2032, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0169, 4.4764, 1.1892, 0.0644, 0.1720, 0.5405, 0.0924],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 3.9000, 0.2000, 0.1080, 0.2500, 0.6060, 0.0290]) tensor(2.0584, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0102,  4.4612,  1.3039,  0.0671,  0.1856,  0.5421,  0.1165],\n",
      "       grad_fn=<AddBackward0>) tensor([10.0000,  6.2000,  0.7000,  0.0890,  0.2330,  0.5010,  0.0600]) tensor(0.4851, grad_fn=<MseLossBackward0>)\n",
      "tensor([23.8890,  8.4649,  3.8621,  0.0732,  0.2314,  0.6421,  0.2223],\n",
      "       grad_fn=<AddBackward0>) tensor([27.0000, 11.0000,  4.6000,  0.0720,  0.3050,  0.5620,  0.2620]) tensor(2.3804, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.1588, 1.2216, 1.0354, 0.0390, 0.1214, 0.4430, 0.1396],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000, 0.8000, 0.6000, 0.0600, 0.0860, 0.4240, 0.1880]) tensor(1.3897, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1668, 5.7174, 0.7093, 0.0841, 0.2077, 0.5609, 0.0575],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 3.2000, 0.4000, 0.0850, 0.2120, 0.3960, 0.0450]) tensor(6.0089, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1689, 4.5233, 0.4408, 0.0789, 0.1942, 0.5310, 0.0608],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 2.7000, 0.3000, 0.0760, 0.1960, 0.4840, 0.0390]) tensor(1.7373, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1852,  4.3628,  2.6899,  0.0418,  0.1462,  0.5518,  0.1765],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 1.1000, 1.5000, 0.0150, 0.0920, 0.4870, 0.2020]) tensor(13.0023, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5963, 2.9173, 1.4425, 0.0525, 0.1508, 0.5119, 0.1298],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 2.4000, 0.6000, 0.0890, 0.1880, 0.4570, 0.0790]) tensor(2.4223, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.6916, 1.9274, 0.8461, 0.0426, 0.1245, 0.4863, 0.0983],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.7000, 0.3000, 0.0800, 0.1480, 0.5460, 0.0460]) tensor(0.4130, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7048,  6.0198,  2.4523,  0.0630,  0.1809,  0.5897,  0.1368],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 4.8000, 0.8000, 0.0850, 0.1420, 0.5780, 0.0500]) tensor(3.3757, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7618, 4.6085, 1.8790, 0.0589, 0.1653, 0.5448, 0.1248],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 3.2000, 2.5000, 0.0180, 0.1230, 0.5220, 0.1670]) tensor(0.8892, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.7785,  7.5137,  1.8157,  0.0946,  0.2468,  0.5927,  0.1258],\n",
      "       grad_fn=<AddBackward0>) tensor([16.3000,  8.8000,  0.9000,  0.1310,  0.1880,  0.5720,  0.0480]) tensor(0.3966, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.0374, 2.7696, 0.7900, 0.0570, 0.1521, 0.5036, 0.0926],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 1.1000, 0.4000, 0.0470, 0.1240, 0.4520, 0.0890]) tensor(2.1084, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.8379,  2.5709, -0.5308,  0.0770,  0.1833,  0.4616,  0.0526],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 2.2000, 1.2000, 0.0190, 0.1890, 0.4160, 0.1930]) tensor(0.5574, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.4433, 4.3105, 1.5026, 0.0617, 0.1719, 0.5262, 0.1247],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 4.7000, 1.7000, 0.0960, 0.1250, 0.4880, 0.1170]) tensor(0.4137, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5894,  5.1405,  2.3377,  0.0547,  0.1669,  0.5607,  0.1490],\n",
      "       grad_fn=<AddBackward0>) tensor([9.0000, 3.9000, 1.9000, 0.0400, 0.1400, 0.4710, 0.1380]) tensor(2.0890, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2564, 0.9337, 2.4349, 0.0244, 0.1049, 0.4479, 0.2203],\n",
      "       grad_fn=<AddBackward0>) tensor([8.1000, 1.1000, 1.9000, 0.0230, 0.0400, 0.4370, 0.1980]) tensor(0.0490, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1955,  3.4237,  2.1098,  0.0389,  0.1366,  0.5231,  0.1594],\n",
      "       grad_fn=<AddBackward0>) tensor([8.9000, 2.5000, 2.1000, 0.0260, 0.1140, 0.4980, 0.1740]) tensor(0.3619, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.9136,  5.2067,  3.7575,  0.0364,  0.1391,  0.5802,  0.2071],\n",
      "       grad_fn=<AddBackward0>) tensor([11.2000,  2.3000,  4.1000,  0.0120,  0.0890,  0.5570,  0.2610]) tensor(4.3987, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4084,  5.5520,  3.2721,  0.0498,  0.1673,  0.5795,  0.1970],\n",
      "       grad_fn=<AddBackward0>) tensor([9.4000, 3.4000, 0.9000, 0.0510, 0.1860, 0.5840, 0.0960]) tensor(8.4837, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.2094, 3.6707, 0.7573, 0.0740, 0.1912, 0.4996, 0.1090],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 2.3000, 1.5000, 0.0680, 0.2080, 0.4520, 0.1790]) tensor(1.1102, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7195, 2.7328, 2.0936, 0.0467, 0.1463, 0.4947, 0.1796],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 0.8000, 0.2000, 0.0160, 0.1430, 0.4300, 0.0570]) tensor(9.7838, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7223, 2.4052, 2.6539, 0.0259, 0.0991, 0.5009, 0.1724],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 2.1000, 3.6000, 0.0130, 0.0880, 0.4890, 0.2550]) tensor(1.2801, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6284, 3.0074, 1.8651, 0.0426, 0.1367, 0.5183, 0.1458],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 2.2000, 0.5000, 0.0310, 0.2120, 0.4490, 0.0830]) tensor(3.6919, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2289,  5.0663,  2.6939,  0.0473,  0.1549,  0.5708,  0.1602],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 3.3000, 0.8000, 0.0210, 0.1710, 0.6140, 0.0720]) tensor(3.2784, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.9457, 2.8165, 0.6334, 0.0588, 0.1560, 0.5015, 0.0877],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 2.1000, 0.4000, 0.0830, 0.1770, 0.4730, 0.0650]) tensor(1.1584, grad_fn=<MseLossBackward0>)\n",
      "tensor([22.5269,  7.3104,  4.1683,  0.0529,  0.1905,  0.6296,  0.2342],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7700e+01, 6.8000e+00, 2.7000e+00, 2.3000e-02, 1.7400e-01, 5.8900e-01,\n",
      "        1.2700e-01]) tensor(4.1703, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.5574,  3.9370,  3.9238,  0.0242,  0.1241,  0.5483,  0.2499],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000e+01, 2.6000e+00, 5.8000e+00, 1.2000e-02, 7.7000e-02, 5.5500e-01,\n",
      "        3.2300e-01]) tensor(1.6117, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3219,  1.5349,  2.7653,  0.0214,  0.1067,  0.4731,  0.2280],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 0.5000, 0.3000, 0.0000, 0.0830, 0.3570, 0.0630]) tensor(9.7676, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4710, 2.8467, 0.7501, 0.0597, 0.1607, 0.4876, 0.1042],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 1.8000, 0.2000, 0.0900, 0.1630, 0.4160, 0.0630]) tensor(1.7294, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5685,  4.8583,  2.1473,  0.0586,  0.1704,  0.5475,  0.1456],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 3.6000, 1.0000, 0.0920, 0.1400, 0.4890, 0.0840]) tensor(6.2096, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.3653,  5.6106,  3.4520,  0.0443,  0.1575,  0.5848,  0.1983],\n",
      "       grad_fn=<AddBackward0>) tensor([12.4000,  6.4000,  3.3000,  0.0690,  0.1750,  0.4670,  0.2070]) tensor(2.3406, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5378, 2.1678, 1.0322, 0.0487, 0.1453, 0.4981, 0.1246],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 0.6000, 0.2000, 0.0190, 0.1050, 0.4150, 0.0830]) tensor(3.2651, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8945, 3.3338, 1.3526, 0.0535, 0.1493, 0.5068, 0.1166],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 2.2000, 0.5000, 0.0890, 0.1340, 0.5450, 0.0750]) tensor(2.3451, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6136, 2.0345, 1.2715, 0.0422, 0.1278, 0.4836, 0.1291],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 3.1000, 1.3000, 0.0630, 0.1500, 0.5680, 0.1250]) tensor(0.1653, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3697,  5.2432,  1.6005,  0.0671,  0.1878,  0.5640,  0.1155],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 4.9000, 0.9000, 0.0890, 0.1750, 0.4970, 0.0740]) tensor(2.8156, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.1707,  5.5076,  3.1647,  0.0496,  0.1724,  0.5752,  0.2044],\n",
      "       grad_fn=<AddBackward0>) tensor([23.0000,  7.5000,  3.4000,  0.0660,  0.1540,  0.5300,  0.1740]) tensor(5.4299, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.2615, 1.0817, 0.4627, 0.0454, 0.1229, 0.4573, 0.0915],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 1.8000, 1.0000, 0.1090, 0.0870, 0.3490, 0.1430]) tensor(0.1592, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.5776,  5.1394,  4.7548,  0.0222,  0.1265,  0.5745,  0.2736],\n",
      "       grad_fn=<AddBackward0>) tensor([11.2000,  1.7000,  1.9000,  0.0130,  0.0870,  0.5150,  0.1800]) tensor(12.8824, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5890, 5.4152, 1.5245, 0.0681, 0.1821, 0.5632, 0.0953],\n",
      "       grad_fn=<AddBackward0>) tensor([8.1000, 8.2000, 0.8000, 0.1240, 0.2640, 0.5090, 0.0540]) tensor(1.5016, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7334,  5.5061,  1.3773,  0.0683,  0.1889,  0.5737,  0.0955],\n",
      "       grad_fn=<AddBackward0>) tensor([10.0000,  5.3000,  1.0000,  0.0980,  0.1500,  0.5470,  0.0670]) tensor(0.1038, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.2147, 1.4738, 0.3891, 0.0497, 0.1309, 0.4541, 0.0859],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 2.6000, 0.4000, 0.0580, 0.2050, 0.4720, 0.0510]) tensor(0.4564, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3760, 1.5035, 2.4755, 0.0242, 0.1008, 0.4926, 0.1891],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 1.2000, 1.4000, 0.0620, 0.0420, 0.4290, 0.1320]) tensor(0.9866, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5989,  5.8392,  3.3927,  0.0502,  0.1623,  0.5940,  0.1820],\n",
      "       grad_fn=<AddBackward0>) tensor([15.7000,  7.1000,  4.4000,  0.0600,  0.1780,  0.5840,  0.1980]) tensor(0.3736, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.3419,  5.4867,  3.0803,  0.0516,  0.1670,  0.5682,  0.1884],\n",
      "       grad_fn=<AddBackward0>) tensor([7.7000, 4.5000, 0.5000, 0.1010, 0.1640, 0.5670, 0.0420]) tensor(9.4363, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2153,  4.0997,  2.6144,  0.0439,  0.1468,  0.5410,  0.1759],\n",
      "       grad_fn=<AddBackward0>) tensor([5.4000, 1.8000, 0.6000, 0.0330, 0.1200, 0.4980, 0.0740]) tensor(7.9726, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2433,  2.8063,  2.6580,  0.0374,  0.1330,  0.5013,  0.2044],\n",
      "       grad_fn=<AddBackward0>) tensor([21.2000,  3.2000,  2.7000,  0.0240,  0.0870,  0.5660,  0.1210]) tensor(14.1865, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0026,  4.3206,  2.1212,  0.0529,  0.1606,  0.5484,  0.1463],\n",
      "       grad_fn=<AddBackward0>) tensor([15.4000,  8.2000,  1.4000,  0.0750,  0.2050,  0.5830,  0.0860]) tensor(4.9878, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6038,  4.6607,  3.7390,  0.0334,  0.1257,  0.5601,  0.2023],\n",
      "       grad_fn=<AddBackward0>) tensor([7.4000, 2.6000, 1.1000, 0.0210, 0.1140, 0.5650, 0.0970]) tensor(7.1014, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6944, 2.5373, 2.3595, 0.0316, 0.1206, 0.5119, 0.1780],\n",
      "       grad_fn=<AddBackward0>) tensor([12.9000,  3.5000,  1.1000,  0.0220,  0.1360,  0.5740,  0.0770]) tensor(1.8290, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8626, 4.3929, 1.7727, 0.0571, 0.1592, 0.5356, 0.1193],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 2.4000, 3.6000, 0.0170, 0.0820, 0.5360, 0.1890]) tensor(1.3950, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1613, 5.5128, 0.8859, 0.0781, 0.1960, 0.5581, 0.0651],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 5.7000, 0.7000, 0.0950, 0.2230, 0.5600, 0.0530]) tensor(0.3583, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9534, 1.9773, 2.8416, 0.0248, 0.1059, 0.4874, 0.2117],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 1.2000, 2.6000, 0.0320, 0.0520, 0.5510, 0.2490]) tensor(3.6011, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.8589, 1.2587, 1.1441, 0.0382, 0.1104, 0.4641, 0.1171],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 1.8000, 4.5000, 0.0290, 0.0850, 0.3120, 0.3370]) tensor(1.6615, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1819,  4.6993,  1.9897,  0.0546,  0.1580,  0.5427,  0.1294],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 9.5000, 2.4000, 0.1180, 0.2390, 0.5450, 0.1120]) tensor(5.2546, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0386,  2.7794,  3.6470,  0.0174,  0.0956,  0.5179,  0.2276],\n",
      "       grad_fn=<AddBackward0>) tensor([10.6000,  3.2000,  2.5000,  0.0360,  0.1160,  0.5000,  0.1790]) tensor(0.5094, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4873,  6.2668,  1.5531,  0.0789,  0.1994,  0.5836,  0.0830],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 7.5000, 0.9000, 0.1410, 0.2100, 0.6450, 0.0520]) tensor(3.0293, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.8842, 2.6396, 1.7494, 0.0424, 0.1342, 0.4968, 0.1510],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 1.7000, 0.9000, 0.0280, 0.1460, 0.5900, 0.1220]) tensor(1.5895, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6751,  4.0822,  2.4045,  0.0416,  0.1385,  0.5367,  0.1563],\n",
      "       grad_fn=<AddBackward0>) tensor([11.0000,  3.0000,  4.7000,  0.0240,  0.0890,  0.5250,  0.2370]) tensor(0.9365, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1419,  5.1382,  1.1580,  0.0772,  0.2078,  0.5375,  0.1187],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 3.3000, 0.4000, 0.1010, 0.1800, 0.4420, 0.0440]) tensor(6.1330, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4610,  5.9075,  2.3846,  0.0661,  0.1919,  0.5700,  0.1560],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 5.0000, 0.6000, 0.1030, 0.1770, 0.6340, 0.0480]) tensor(4.5291, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.7764,  6.6989,  3.1860,  0.0642,  0.1909,  0.6101,  0.1709],\n",
      "       grad_fn=<AddBackward0>) tensor([14.9000,  8.9000,  2.4000,  0.0490,  0.2770,  0.5750,  0.1270]) tensor(1.2849, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.5341, 1.6567, 1.2889, 0.0338, 0.1085, 0.4869, 0.1189],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 2.5000, 0.4000, 0.0760, 0.0780, 0.6040, 0.0300]) tensor(0.3017, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3213, 4.5879, 0.8029, 0.0763, 0.1969, 0.5271, 0.0934],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 2.9000, 0.2000, 0.1570, 0.2430, 0.5050, 0.0350]) tensor(4.9748, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.7225,  5.0586,  3.1350,  0.0427,  0.1546,  0.5707,  0.1953],\n",
      "       grad_fn=<AddBackward0>) tensor([13.9000,  3.8000,  2.4000,  0.0270,  0.1070,  0.5150,  0.1360]) tensor(0.7793, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3328, 4.0689, 0.5691, 0.0688, 0.1732, 0.5259, 0.0622],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 2.3000, 0.4000, 0.1240, 0.2090, 0.4170, 0.0850]) tensor(2.5520, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.8319, 0.0267, 1.7777, 0.0141, 0.0747, 0.4450, 0.1678],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 0.2000, 1.0000, 0.0000, 0.0240, 0.6210, 0.2000]) tensor(0.2787, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0835,  4.7160,  1.6024,  0.0583,  0.1688,  0.5492,  0.1161],\n",
      "       grad_fn=<AddBackward0>) tensor([12.8000,  6.0000,  1.8000,  0.0450,  0.1630,  0.5570,  0.0870]) tensor(1.2954, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6820,  5.4636,  2.4572,  0.0636,  0.1780,  0.5781,  0.1419],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 3.4000, 0.9000, 0.0740, 0.1560, 0.5940, 0.0930]) tensor(6.4149, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1285,  5.9737,  1.8287,  0.0671,  0.1875,  0.5820,  0.1099],\n",
      "       grad_fn=<AddBackward0>) tensor([11.7000, 10.2000,  1.3000,  0.0970,  0.2400,  0.5260,  0.0660]) tensor(2.6190, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.0140e+01, 4.8907e+00, 5.0646e+00, 1.9500e-02, 1.1992e-01, 5.8365e-01,\n",
      "        2.8548e-01], grad_fn=<AddBackward0>) tensor([2.7300e+01, 5.3000e+00, 5.2000e+00, 2.0000e-02, 1.2200e-01, 6.4100e-01,\n",
      "        2.2500e-01]) tensor(7.3519, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.4817, 1.7338, 0.0171, 0.0557, 0.1400, 0.4585, 0.0647],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 2.6000, 0.3000, 0.1080, 0.1430, 0.6360, 0.0340]) tensor(1.3403, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7573, 2.1077, 2.4829, 0.0299, 0.1159, 0.4941, 0.1949],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 2.0000, 2.6000, 0.0550, 0.1360, 0.4990, 0.3550]) tensor(4.2621, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2300,  3.7711,  3.6946,  0.0227,  0.1123,  0.5409,  0.2254],\n",
      "       grad_fn=<AddBackward0>) tensor([12.8000,  2.6000,  2.7000,  0.0140,  0.0950,  0.6010,  0.1590]) tensor(0.6306, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3873,  4.6385,  2.8790,  0.0437,  0.1376,  0.5573,  0.1584],\n",
      "       grad_fn=<AddBackward0>) tensor([9.9000, 2.3000, 3.2000, 0.0120, 0.0740, 0.5460, 0.1460]) tensor(1.1127, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9474, 3.1717, 0.9489, 0.0641, 0.1644, 0.5146, 0.0964],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 5.4000, 0.8000, 0.1370, 0.2820, 0.7350, 0.0830]) tensor(0.9120, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2078,  4.3713,  2.6293,  0.0462,  0.1442,  0.5539,  0.1564],\n",
      "       grad_fn=<AddBackward0>) tensor([12.1000,  3.6000,  4.7000,  0.0140,  0.1080,  0.6720,  0.2380]) tensor(0.8145, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5012,  3.3151,  3.1405,  0.0217,  0.1038,  0.5349,  0.1920],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 1.6000, 2.1000, 0.0150, 0.0830, 0.4830, 0.1650]) tensor(5.5504, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.8720, 1.9246, 2.3130, 0.0298, 0.1211, 0.4798, 0.2035],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 0.7000, 0.6000, 0.0510, 0.0590, 0.3780, 0.1460]) tensor(8.3998, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4495,  5.7289,  2.6057,  0.0549,  0.1724,  0.5829,  0.1552],\n",
      "       grad_fn=<AddBackward0>) tensor([12.0000,  7.4000,  1.9000,  0.0660,  0.1910,  0.4720,  0.1030]) tensor(1.3295, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 4.8749,  4.1269, -0.1456,  0.0882,  0.2087,  0.5147,  0.0474],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 3.5000, 0.3000, 0.2020, 0.1220, 0.3410, 0.0530]) tensor(0.7068, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3236,  3.8860,  2.5272,  0.0427,  0.1403,  0.5285,  0.1721],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 2.4000, 0.8000, 0.0110, 0.0950, 0.5510, 0.0490]) tensor(1.9654, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.2110,  5.4101,  4.1515,  0.0333,  0.1427,  0.5808,  0.2388],\n",
      "       grad_fn=<AddBackward0>) tensor([22.1000,  4.3000,  4.8000,  0.0230,  0.0920,  0.5080,  0.2090]) tensor(2.3980, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5235,  3.2460,  3.2378,  0.0219,  0.1098,  0.5252,  0.2117],\n",
      "       grad_fn=<AddBackward0>) tensor([8.8000, 2.4000, 3.9000, 0.0200, 0.0850, 0.4680, 0.2410]) tensor(2.1462, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3457, 1.7428, 1.2679, 0.0420, 0.1270, 0.4758, 0.1344],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 1.5000, 0.4000, 0.0280, 0.1460, 0.3710, 0.0580]) tensor(1.9145, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2534, 4.2911, 2.0271, 0.0505, 0.1476, 0.5465, 0.1251],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 6.2000, 0.4000, 0.1340, 0.2070, 0.6080, 0.0330]) tensor(2.0652, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.9051,  3.7715,  3.6614,  0.0193,  0.1059,  0.5516,  0.2155],\n",
      "       grad_fn=<AddBackward0>) tensor([13.7000,  4.4000,  9.8000,  0.0430,  0.1030,  0.5400,  0.4250]) tensor(5.4520, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.8925,  5.4954,  3.6360,  0.0392,  0.1432,  0.5873,  0.1955],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 2.1000, 2.8000, 0.0130, 0.0780, 0.5340, 0.1760]) tensor(8.1464, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7048,  3.5498,  3.5377,  0.0252,  0.1113,  0.5338,  0.2140],\n",
      "       grad_fn=<AddBackward0>) tensor([8.3000, 2.3000, 4.0000, 0.0250, 0.0910, 0.5030, 0.2780]) tensor(3.0262, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5321,  2.9903,  3.1299,  0.0165,  0.0967,  0.5345,  0.1962],\n",
      "       grad_fn=<AddBackward0>) tensor([8.1000, 5.7000, 2.9000, 0.0610, 0.1960, 0.4850, 0.2020]) tensor(2.7413, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4158,  4.4646,  2.1612,  0.0550,  0.1578,  0.5415,  0.1408],\n",
      "       grad_fn=<AddBackward0>) tensor([7.4000, 7.3000, 6.9000, 0.0310, 0.1800, 0.5260, 0.2610]) tensor(5.6581, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7181,  4.2498,  2.1098,  0.0458,  0.1466,  0.5522,  0.1381],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 3.7000, 1.1000, 0.0330, 0.1620, 0.5420, 0.0820]) tensor(1.4058, grad_fn=<MseLossBackward0>)\n",
      "tensor([23.8631,  6.1710,  5.3448,  0.0246,  0.1413,  0.6140,  0.2973],\n",
      "       grad_fn=<AddBackward0>) tensor([25.0000,  4.1000,  7.7000,  0.0320,  0.0930,  0.5500,  0.3720]) tensor(1.5914, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2256, 3.3737, 1.7752, 0.0419, 0.1347, 0.5210, 0.1323],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 1.7000, 1.2000, 0.0240, 0.0790, 0.4880, 0.0980]) tensor(0.8258, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 5.5332,  5.0248, -0.0586,  0.0878,  0.2075,  0.5475,  0.0258],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 5.4000, 0.3000, 0.1460, 0.2330, 0.6080, 0.0280]) tensor(0.0397, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.7368,  2.5295, -0.4574,  0.0688,  0.1638,  0.4728,  0.0364],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 4.3000, 0.7000, 0.1430, 0.2100, 0.5170, 0.0790]) tensor(0.9899, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9216, 3.5926, 0.9344, 0.0626, 0.1630, 0.5073, 0.0933],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 7.2000, 1.2000, 0.1190, 0.2080, 0.6110, 0.0740]) tensor(1.8919, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8412, 3.6940, 1.0976, 0.0604, 0.1614, 0.5142, 0.1008],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 4.0000, 0.2000, 0.0990, 0.1980, 0.4980, 0.0180]) tensor(1.8216, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.2523, 0.7391, 0.2733, 0.0407, 0.1145, 0.4338, 0.0915],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 0.8000, 0.7000, 0.0000, 0.0880, 0.3310, 0.0930]) tensor(0.3706, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.8535,  4.4620,  3.6558,  0.0347,  0.1346,  0.5510,  0.2206],\n",
      "       grad_fn=<AddBackward0>) tensor([14.6000,  4.8000,  2.5000,  0.0320,  0.1270,  0.5030,  0.1250]) tensor(0.2180, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0212,  4.2818,  2.1806,  0.0538,  0.1607,  0.5321,  0.1560],\n",
      "       grad_fn=<AddBackward0>) tensor([10.4000,  3.5000,  3.0000,  0.0390,  0.1000,  0.5230,  0.1550]) tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.3490,  5.4555,  4.6907,  0.0293,  0.1354,  0.5889,  0.2590],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9500e+01, 3.1000e+00, 5.6000e+00, 1.2000e-02, 9.3000e-02, 6.0100e-01,\n",
      "        2.7000e-01]) tensor(0.9143, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.7927, 1.7442, 1.3186, 0.0430, 0.1287, 0.4741, 0.1404],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 1.1000, 0.8000, 0.0430, 0.0990, 0.4930, 0.1140]) tensor(0.8489, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4171, 4.6104, 1.6889, 0.0547, 0.1597, 0.5525, 0.1121],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 3.9000, 0.3000, 0.1000, 0.1750, 0.4970, 0.0380]) tensor(3.1365, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.6523,  1.2794, -0.2910,  0.0543,  0.1335,  0.4665,  0.0445],\n",
      "       grad_fn=<AddBackward0>) tensor([0.8000, 0.7000, 0.3000, 0.0430, 0.1460, 0.3880, 0.1030]) tensor(0.4006, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0966,  6.3458,  1.0666,  0.0828,  0.2097,  0.5852,  0.0654],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 5.9000, 0.6000, 0.1170, 0.1630, 0.5420, 0.0380]) tensor(1.1777, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.3657, 3.3891, 0.6385, 0.0623, 0.1590, 0.5106, 0.0714],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 2.5000, 0.7000, 0.0490, 0.1480, 0.4620, 0.0720]) tensor(0.1976, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5221,  5.0902,  2.5050,  0.0549,  0.1711,  0.5567,  0.1677],\n",
      "       grad_fn=<AddBackward0>) tensor([11.3000,  6.7000,  1.0000,  0.1280,  0.2140,  0.5440,  0.0760]) tensor(1.4014, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7990, 2.9981, 2.3680, 0.0316, 0.1155, 0.5235, 0.1571],\n",
      "       grad_fn=<AddBackward0>) tensor([11.6000,  4.8000,  5.5000,  0.0280,  0.1240,  0.5230,  0.2550]) tensor(2.9874, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9575,  4.6804,  2.6003,  0.0480,  0.1510,  0.5425,  0.1641],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 4.4000, 1.4000, 0.0770, 0.1310, 0.5250, 0.0850]) tensor(4.6304, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7237,  5.4235,  2.8924,  0.0510,  0.1600,  0.5774,  0.1627],\n",
      "       grad_fn=<AddBackward0>) tensor([8.3000, 3.4000, 1.3000, 0.0230, 0.1510, 0.5270, 0.1020]) tensor(5.1506, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.8584, 4.5166, 1.9583, 0.0576, 0.1644, 0.5285, 0.1386],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 4.4000, 0.4000, 0.0920, 0.2110, 0.5030, 0.0360]) tensor(3.0646, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2381,  4.8823,  3.3694,  0.0377,  0.1374,  0.5696,  0.1893],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 2.0000, 4.2000, 0.0190, 0.0900, 0.5960, 0.3240]) tensor(6.8476, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.2818,  5.5916,  3.4751,  0.0503,  0.1704,  0.5698,  0.2167],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3500e+01, 3.1000e+00, 1.1000e+00, 1.1000e-02, 1.1400e-01, 5.2500e-01,\n",
      "        6.9000e-02]) tensor(3.7399, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1873,  3.5290,  3.2409,  0.0251,  0.1106,  0.5357,  0.1985],\n",
      "       grad_fn=<AddBackward0>) tensor([11.7000,  3.4000,  2.3000,  0.0160,  0.1050,  0.5490,  0.1230]) tensor(0.1636, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.7565,  5.4721,  3.5431,  0.0457,  0.1596,  0.5733,  0.2118],\n",
      "       grad_fn=<AddBackward0>) tensor([18.2000,  5.4000,  2.4000,  0.0380,  0.1220,  0.5910,  0.1040]) tensor(0.4870, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7273, 4.8724, 1.6382, 0.0658, 0.1723, 0.5468, 0.1035],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000, 1.6000, 0.3000, 0.0720, 0.1950, 0.4060, 0.0740]) tensor(10.0994, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.6228, 3.7594, 1.6231, 0.0527, 0.1496, 0.5225, 0.1183],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 1.7000, 0.8000, 0.0430, 0.1220, 0.5570, 0.0990]) tensor(3.1313, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0284,  3.8778,  2.2750,  0.0449,  0.1458,  0.5261,  0.1644],\n",
      "       grad_fn=<AddBackward0>) tensor([3.9000, 1.5000, 2.4000, 0.0420, 0.0710, 0.4870, 0.2480]) tensor(8.0712, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9699,  5.8041,  2.3162,  0.0639,  0.1941,  0.5662,  0.1645],\n",
      "       grad_fn=<AddBackward0>) tensor([14.1000,  6.3000,  2.1000,  0.0800,  0.1890,  0.5190,  0.1380]) tensor(0.1504, grad_fn=<MseLossBackward0>)\n",
      "tensor([23.0069,  7.8944,  3.9742,  0.0633,  0.2104,  0.6370,  0.2242],\n",
      "       grad_fn=<AddBackward0>) tensor([28.0000, 11.8000,  2.1000,  0.0670,  0.2690,  0.5800,  0.1100]) tensor(6.2454, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.4464, 2.7317, 0.3001, 0.0615, 0.1570, 0.4974, 0.0677],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.5000, 0.0000, 0.0860, 0.1230, 0.5300, 0.0000]) tensor(0.4896, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5979, 3.0934, 0.9907, 0.0562, 0.1566, 0.5051, 0.1083],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 4.4000, 0.1000, 0.1710, 0.2290, 0.5660, 0.0100]) tensor(0.7265, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.4760, 1.0430, 2.0059, 0.0230, 0.0872, 0.4714, 0.1554],\n",
      "       grad_fn=<AddBackward0>) tensor([8.7000, 2.0000, 2.4000, 0.0230, 0.0630, 0.5960, 0.1560]) tensor(2.7042, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.7863, 3.1007, 0.4130, 0.0681, 0.1635, 0.4977, 0.0616],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 1.5000, 0.2000, 0.0560, 0.1410, 0.4080, 0.0450]) tensor(0.4860, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.0045,  6.6945,  3.6760,  0.0533,  0.1800,  0.6092,  0.2046],\n",
      "       grad_fn=<AddBackward0>) tensor([21.6000,  6.6000,  4.2000,  0.0320,  0.1760,  0.5830,  0.1970]) tensor(1.0030, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0964, 4.2163, 2.0808, 0.0460, 0.1395, 0.5475, 0.1239],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 2.1000, 1.2000, 0.0190, 0.1090, 0.4730, 0.0930]) tensor(4.4623, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9078,  6.5022,  1.6949,  0.0836,  0.2171,  0.5825,  0.1093],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([8.1000, 5.0000, 0.7000, 0.0940, 0.1910, 0.6070, 0.0650]) tensor(3.7664, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1626, 1.7539, 3.0059, 0.0141, 0.0855, 0.4811, 0.2136],\n",
      "       grad_fn=<AddBackward0>) tensor([9.5000e+00, 1.8000e+00, 4.2000e+00, 9.0000e-03, 7.5000e-02, 5.3700e-01,\n",
      "        2.7300e-01]) tensor(0.2212, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6922,  4.6567,  2.5602,  0.0497,  0.1628,  0.5602,  0.1725],\n",
      "       grad_fn=<AddBackward0>) tensor([10.2000,  5.5000,  0.7000,  0.0970,  0.2150,  0.5360,  0.0640]) tensor(2.3406, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.8072, 3.9419, 1.6455, 0.0549, 0.1535, 0.5239, 0.1194],\n",
      "       grad_fn=<AddBackward0>) tensor([10.8000, 10.4000,  1.5000,  0.1040,  0.2690,  0.5930,  0.0650]) tensor(7.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9268,  4.3177,  3.0849,  0.0376,  0.1337,  0.5559,  0.1827],\n",
      "       grad_fn=<AddBackward0>) tensor([12.4000,  6.1000,  5.5000,  0.0280,  0.1620,  0.5370,  0.2410]) tensor(1.3274, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1859,  2.1909,  2.9511,  0.0260,  0.1089,  0.4833,  0.2179],\n",
      "       grad_fn=<AddBackward0>) tensor([8.2000, 1.8000, 2.4000, 0.0090, 0.0990, 0.5270, 0.2310]) tensor(0.6290, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1257,  4.7919,  2.1136,  0.0500,  0.1549,  0.5563,  0.1335],\n",
      "       grad_fn=<AddBackward0>) tensor([12.3000,  2.9000,  1.9000,  0.0240,  0.0770,  0.5660,  0.0930]) tensor(0.7161, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2561e+00, 1.7193e+00, 3.5653e+00, 7.1975e-03, 6.8440e-02, 5.0002e-01,\n",
      "        2.1955e-01], grad_fn=<AddBackward0>) tensor([12.7000,  2.8000,  6.7000,  0.0180,  0.0770,  0.6050,  0.3100]) tensor(3.2677, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7353, 4.8439, 0.3206, 0.0866, 0.2135, 0.5286, 0.0707],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 1.4000, 0.2000, 0.1160, 0.1460, 0.4520, 0.0800]) tensor(7.0755, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2212,  2.7865,  2.0649,  0.0430,  0.1496,  0.4947,  0.1930],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 0.5000, 0.3000, 0.0620, 0.0450, 0.2410, 0.1140]) tensor(15.2650, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.0011, 1.3371, 1.8078, 0.0252, 0.0992, 0.4813, 0.1547],\n",
      "       grad_fn=<AddBackward0>) tensor([5.9000, 2.3000, 1.7000, 0.0150, 0.1300, 0.5510, 0.1350]) tensor(0.1365, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4101,  5.2190,  3.7575,  0.0360,  0.1429,  0.5799,  0.2143],\n",
      "       grad_fn=<AddBackward0>) tensor([11.9000,  5.9000,  0.8000,  0.0910,  0.2090,  0.5250,  0.0680]) tensor(4.2263, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1104,  3.4406,  3.6468,  0.0161,  0.0978,  0.5391,  0.2174],\n",
      "       grad_fn=<AddBackward0>) tensor([11.1000,  2.5000,  2.8000,  0.0220,  0.0750,  0.5260,  0.1410]) tensor(0.8072, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.8259, 3.4233, 2.4032, 0.0409, 0.1342, 0.5247, 0.1639],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 1.7000, 0.4000, 0.0590, 0.1720, 0.5420, 0.0870]) tensor(6.5360, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1861, 3.5111, 0.5375, 0.0684, 0.1779, 0.5089, 0.0865],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 2.3000, 0.9000, 0.0410, 0.1180, 0.5010, 0.0980]) tensor(0.6847, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.0418,  4.4179,  2.5777,  0.0517,  0.1689,  0.5472,  0.1877],\n",
      "       grad_fn=<AddBackward0>) tensor([12.8000,  4.9000,  0.8000,  0.0840,  0.1980,  0.5630,  0.0860]) tensor(0.7067, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7231,  4.9343,  2.7982,  0.0411,  0.1460,  0.5727,  0.1662],\n",
      "       grad_fn=<AddBackward0>) tensor([11.5000,  7.4000,  1.0000,  0.0870,  0.1710,  0.5130,  0.0560]) tensor(2.0391, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2353, 3.1296, 1.6819, 0.0427, 0.1369, 0.5225, 0.1323],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 2.4000, 0.3000, 0.0900, 0.1800, 0.5400, 0.0530]) tensor(3.1607, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0889, 4.5215, 1.0885, 0.0658, 0.1767, 0.5455, 0.0891],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 3.4000, 0.4000, 0.1160, 0.2020, 0.5500, 0.0600]) tensor(1.9869, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.4657, 1.5216, 2.0454, 0.0251, 0.0984, 0.4901, 0.1602],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 2.1000, 2.4000, 0.0160, 0.0950, 0.5490, 0.1630]) tensor(0.7170, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.0585, 4.8348, 0.2846, 0.0857, 0.2042, 0.5360, 0.0490],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 1.9000, 0.2000, 0.1070, 0.1760, 0.4630, 0.0430]) tensor(4.0721, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0366,  4.8789,  2.8832,  0.0500,  0.1668,  0.5533,  0.1968],\n",
      "       grad_fn=<AddBackward0>) tensor([17.2000,  2.9000,  2.3000,  0.0260,  0.0690,  0.5150,  0.1120]) tensor(1.2794, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8394,  6.3624,  2.2257,  0.0711,  0.1876,  0.5969,  0.1069],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 5.3000, 0.1000, 0.1470, 0.2510, 0.5750, 0.0140]) tensor(9.5890, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8242,  3.6809,  2.2138,  0.0455,  0.1471,  0.5213,  0.1668],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 3.7000, 1.3000, 0.0330, 0.1600, 0.5560, 0.0930]) tensor(0.3344, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.9642, 0.8131, 0.5330, 0.0436, 0.1192, 0.4363, 0.1048],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 1.4000, 0.0000, 0.0940, 0.1130, 0.5920, 0.0000]) tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.6728,  4.3346, -1.2337,  0.1000,  0.2222,  0.5118, -0.0148],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 2.8000, 0.7000, 0.0700, 0.2090, 0.5400, 0.0970]) tensor(1.1242, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.2282,  3.1533, -0.4495,  0.0806,  0.1881,  0.4828,  0.0398],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 3.3000, 0.4000, 0.0910, 0.2010, 0.5470, 0.0510]) tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 3.6346,  5.8475, -1.4382,  0.1216,  0.2646,  0.5508, -0.0370],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 2.7000, 0.2000, 0.1230, 0.2370, 0.6600, 0.0280]) tensor(1.9850, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6576, 3.1084, 1.6105, 0.0540, 0.1619, 0.5018, 0.1574],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 1.0000, 0.3000, 0.0610, 0.0250, 0.4400, 0.0380]) tensor(3.5983, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.8887, 6.0152, 0.2239, 0.0996, 0.2347, 0.5603, 0.0406],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 2.4000, 0.3000, 0.1070, 0.1830, 0.5810, 0.0450]) tensor(4.6200, grad_fn=<MseLossBackward0>)\n",
      "tensor([22.0175,  7.0115,  4.2278,  0.0504,  0.1839,  0.6204,  0.2391],\n",
      "       grad_fn=<AddBackward0>) tensor([27.6000,  5.5000,  3.9000,  0.0650,  0.0930,  0.5510,  0.1870]) tensor(4.7961, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.9692,  6.0632,  3.0302,  0.0511,  0.1684,  0.5969,  0.1691],\n",
      "       grad_fn=<AddBackward0>) tensor([11.9000,  4.0000,  1.7000,  0.0300,  0.1220,  0.5410,  0.0950]) tensor(3.2280, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.4714e+01, 3.6841e+00, 4.4757e+00, 1.0287e-02, 8.7830e-02, 5.5479e-01,\n",
      "        2.4464e-01], grad_fn=<AddBackward0>) tensor([11.3000,  3.1000,  6.1000,  0.0220,  0.0850,  0.5370,  0.2790]) tensor(2.0907, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.4454, 1.0406, 1.4040, 0.0336, 0.1061, 0.4774, 0.1344],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 1.3000, 1.0000, 0.0000, 0.1470, 0.4600, 0.1740]) tensor(0.5742, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.6497, 2.4448, 0.7910, 0.0575, 0.1465, 0.4784, 0.0962],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 2.0000, 0.4000, 0.0470, 0.1210, 0.4300, 0.0390]) tensor(0.1313, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0171,  6.0854,  2.4389,  0.0658,  0.1940,  0.5682,  0.1618],\n",
      "       grad_fn=<AddBackward0>) tensor([9.4000, 4.6000, 0.8000, 0.0950, 0.1800, 0.5900, 0.0660]) tensor(5.2078, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.0995,  5.2163,  4.0917,  0.0326,  0.1356,  0.5748,  0.2299],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6400e+01, 2.9000e+00, 5.4000e+00, 1.6000e-02, 7.6000e-02, 5.3900e-01,\n",
      "        2.4900e-01]) tensor(1.0817, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9175,  5.7727,  1.6552,  0.0735,  0.1935,  0.5735,  0.1015],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 8.1000, 1.2000, 0.1240, 0.2120, 0.5400, 0.0730]) tensor(2.3763, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.0360,  8.3819,  1.8429,  0.0982,  0.2513,  0.6249,  0.1003],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 3.7000, 0.6000, 0.1240, 0.1990, 0.5270, 0.0820]) tensor(20.4391, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1714,  6.4243,  2.4007,  0.0709,  0.1974,  0.5815,  0.1422],\n",
      "       grad_fn=<AddBackward0>) tensor([5.4000, 3.7000, 0.9000, 0.0710, 0.1460, 0.4870, 0.0810]) tensor(12.3752, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4609,  4.5812,  2.9001,  0.0423,  0.1413,  0.5586,  0.1698],\n",
      "       grad_fn=<AddBackward0>) tensor([10.0000,  4.0000,  1.7000,  0.0380,  0.1280,  0.4930,  0.1050]) tensor(1.1204, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7106, 5.7824, 1.0328, 0.0829, 0.2093, 0.5623, 0.0800],\n",
      "       grad_fn=<AddBackward0>) tensor([10.5000,  7.3000,  0.9000,  0.0850,  0.2080,  0.5130,  0.0520]) tensor(0.4210, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.8023, 2.5100, 2.4650, 0.0285, 0.1153, 0.5023, 0.1857],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.0000, 1.4000, 0.0220, 0.0710, 0.3540, 0.1930]) tensor(6.5312, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8924,  4.7362,  2.6548,  0.0453,  0.1452,  0.5625,  0.1534],\n",
      "       grad_fn=<AddBackward0>) tensor([11.0000,  6.1000,  1.6000,  0.0670,  0.1520,  0.5620,  0.0820]) tensor(0.5392, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7619, 4.1942, 1.3335, 0.0577, 0.1601, 0.5414, 0.0970],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 1.4000, 0.7000, 0.0570, 0.1410, 0.4740, 0.1360]) tensor(5.5928, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.7291,  4.1082,  3.3662,  0.0374,  0.1403,  0.5543,  0.2135],\n",
      "       grad_fn=<AddBackward0>) tensor([18.7000,  4.2000,  3.3000,  0.0470,  0.0860,  0.5260,  0.1780]) tensor(2.2552, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1500,  3.7148,  2.2173,  0.0531,  0.1653,  0.5287,  0.1778],\n",
      "       grad_fn=<AddBackward0>) tensor([16.2000,  4.7000,  2.3000,  0.0290,  0.1590,  0.5720,  0.1910]) tensor(2.4832, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.2021,  6.8022,  3.8383,  0.0574,  0.1889,  0.6164,  0.2161],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5100e+01, 8.3000e+00, 4.8000e+00, 2.3000e-02, 2.3200e-01, 6.5100e-01,\n",
      "        2.1800e-01]) tensor(3.8802, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4680, 4.8680, 0.6444, 0.0858, 0.2171, 0.5329, 0.0945],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 1.8000, 0.2000, 0.1330, 0.1540, 0.5780, 0.0530]) tensor(7.3507, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4949, 4.4257, 1.1387, 0.0629, 0.1733, 0.5351, 0.0994],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 6.4000, 0.6000, 0.1220, 0.1660, 0.6920, 0.0420]) tensor(0.8068, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6130,  4.6608,  2.8488,  0.0390,  0.1371,  0.5640,  0.1645],\n",
      "       grad_fn=<AddBackward0>) tensor([11.8000,  4.9000,  1.5000,  0.0900,  0.1020,  0.6470,  0.0880]) tensor(0.3649, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0760, 4.2509, 1.6073, 0.0534, 0.1573, 0.5378, 0.1177],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 3.5000, 1.0000, 0.0510, 0.1460, 0.4990, 0.0840]) tensor(0.6914, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6432,  4.1935,  2.1673,  0.0439,  0.1429,  0.5483,  0.1410],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 2.8000, 0.9000, 0.0210, 0.1380, 0.5240, 0.0710]) tensor(4.2866, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4458, 3.5381, 1.1564, 0.0564, 0.1600, 0.5197, 0.1090],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 3.2000, 0.7000, 0.0420, 0.1990, 0.5110, 0.0880]) tensor(0.7669, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4215,  4.5358,  2.6850,  0.0445,  0.1361,  0.5602,  0.1429],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 1.9000, 0.9000, 0.0300, 0.0870, 0.5230, 0.0570]) tensor(7.1581, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.8963, 2.2541, 0.6876, 0.0495, 0.1377, 0.4841, 0.0939],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 1.4000, 1.1000, 0.0190, 0.0770, 0.4360, 0.1200]) tensor(0.1658, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5421, 2.3095, 1.7117, 0.0423, 0.1287, 0.4846, 0.1471],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 2.2000, 0.1000, 0.0850, 0.1470, 0.4020, 0.0250]) tensor(2.9470, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4717, 1.9035, 2.1309, 0.0284, 0.1071, 0.4906, 0.1672],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 2.3000, 3.1000, 0.0280, 0.1240, 0.4830, 0.2460]) tensor(0.8948, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9114, 4.2247, 1.7687, 0.0563, 0.1580, 0.5437, 0.1183],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 2.9000, 1.1000, 0.0520, 0.1660, 0.5240, 0.1150]) tensor(3.0947, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.8091, 3.0517, 0.8063, 0.0587, 0.1521, 0.5043, 0.0854],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4000, 1.0000, 1.6000, 0.0140, 0.0640, 0.5040, 0.1610]) tensor(1.5227, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.6570,  5.8339,  3.9969,  0.0403,  0.1600,  0.6023,  0.2323],\n",
      "       grad_fn=<AddBackward0>) tensor([23.4000,  7.0000,  3.0000,  0.0270,  0.1650,  0.5670,  0.1430]) tensor(2.3390, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6940, 2.9434, 2.2893, 0.0473, 0.1445, 0.4863, 0.1867],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 5.4000, 0.4000, 0.1530, 0.2210, 0.4280, 0.0390]) tensor(7.5896, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4165,  4.4449,  3.0580,  0.0398,  0.1361,  0.5563,  0.1766],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 5.1000, 1.0000, 0.1130, 0.1820, 0.4390, 0.0850]) tensor(7.3084, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8717, 4.2834, 0.4026, 0.0807, 0.1994, 0.5305, 0.0695],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 1.3000, 0.6000, 0.0280, 0.1820, 0.5080, 0.1480]) tensor(4.2642, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7046, 4.6444, 1.6874, 0.0560, 0.1569, 0.5531, 0.1031],\n",
      "       grad_fn=<AddBackward0>) tensor([5.9000, 5.7000, 1.1000, 0.1190, 0.2020, 0.5330, 0.0830]) tensor(1.3331, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2524,  5.7881,  2.7090,  0.0534,  0.1682,  0.5825,  0.1556],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 2.6000, 1.0000, 0.0290, 0.1240, 0.5020, 0.0820]) tensor(8.9765, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.4373, 2.0114, 1.1158, 0.0445, 0.1281, 0.4900, 0.1116],\n",
      "       grad_fn=<AddBackward0>) tensor([11.0000,  4.7000,  1.0000,  0.0430,  0.1340,  0.4900,  0.0560]) tensor(7.1876, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2055,  3.7539,  3.7664,  0.0247,  0.1137,  0.5364,  0.2298],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 2.9000, 4.9000, 0.0250, 0.0960, 0.5670, 0.2320]) tensor(3.0605, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3285,  5.1815,  2.0368,  0.0615,  0.1797,  0.5492,  0.1452],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 1.7000, 0.6000, 0.0640, 0.0840, 0.4670, 0.0720]) tensor(10.7847, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7971,  4.8140,  2.7178,  0.0485,  0.1484,  0.5608,  0.1541],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 3.1000, 0.9000, 0.0490, 0.1440, 0.5000, 0.0770]) tensor(6.5580, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.3695,  6.2010,  2.0481,  0.0771,  0.2067,  0.5708,  0.1342],\n",
      "       grad_fn=<AddBackward0>) tensor([8.3000, 6.8000, 2.4000, 0.1330, 0.2060, 0.5060, 0.1550]) tensor(3.7414, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.3164,  6.8602,  2.1341,  0.0719,  0.2015,  0.5997,  0.1197],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 5.4000, 0.5000, 0.1020, 0.2410, 0.5180, 0.0510]) tensor(11.7922, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.4546,  5.5951,  1.8944,  0.0633,  0.1780,  0.5730,  0.1143],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 3.2000, 1.1000, 0.0570, 0.1650, 0.5150, 0.1080]) tensor(6.4988, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9954,  7.2501,  1.3434,  0.0952,  0.2401,  0.6034,  0.0848],\n",
      "       grad_fn=<AddBackward0>) tensor([13.4000,  7.9000,  2.1000,  0.0820,  0.2820,  0.5920,  0.1360]) tensor(0.1662, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7400,  5.9857,  1.5015,  0.0733,  0.1939,  0.5760,  0.0912],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 9.3000, 0.7000, 0.1640, 0.3180, 0.5120, 0.0460]) tensor(6.5375, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.2109,  5.4362,  3.5612,  0.0417,  0.1547,  0.5851,  0.2092],\n",
      "       grad_fn=<AddBackward0>) tensor([15.3000,  3.8000,  3.1000,  0.0320,  0.0980,  0.5210,  0.1440]) tensor(0.9362, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3034, 4.2153, 1.7556, 0.0519, 0.1535, 0.5453, 0.1200],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 5.5000, 1.1000, 0.0690, 0.1930, 0.5100, 0.0730]) tensor(0.8713, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5818,  4.0059,  2.6054,  0.0434,  0.1430,  0.5340,  0.1741],\n",
      "       grad_fn=<AddBackward0>) tensor([6.2000, 3.8000, 1.2000, 0.0530, 0.1840, 0.5260, 0.1020]) tensor(4.4269, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1616,  5.3393,  2.5545,  0.0547,  0.1631,  0.5640,  0.1472],\n",
      "       grad_fn=<AddBackward0>) tensor([10.2000,  7.3000,  3.0000,  0.0790,  0.1990,  0.5150,  0.1680]) tensor(1.1279, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9709, 0.4618, 2.2887, 0.0102, 0.0721, 0.4585, 0.1890],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 1.3000, 1.1000, 0.0430, 0.1260, 0.4620, 0.1680]) tensor(1.4803, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.9165,  5.3183,  3.7027,  0.0364,  0.1502,  0.5815,  0.2236],\n",
      "       grad_fn=<AddBackward0>) tensor([12.8000,  3.4000,  5.5000,  0.0170,  0.1070,  0.4370,  0.2810]) tensor(4.7307, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6639,  5.0399,  2.2412,  0.0546,  0.1640,  0.5610,  0.1391],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 5.9000, 1.0000, 0.1020, 0.2390, 0.5520, 0.0870]) tensor(1.8492, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3853, 4.5015, 1.4191, 0.0570, 0.1607, 0.5457, 0.0987],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 8.3000, 2.0000, 0.1150, 0.2110, 0.5730, 0.1050]) tensor(2.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.5871, 0.2451, 1.4607, 0.0234, 0.0784, 0.4628, 0.1251],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 1.6000, 0.4000, 0.0610, 0.2000, 0.4700, 0.0630]) tensor(0.4751, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7561, 2.1464, 2.8326, 0.0246, 0.1057, 0.4995, 0.2022],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 2.2000, 1.4000, 0.0390, 0.1340, 0.5260, 0.1510]) tensor(0.9583, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5390, 2.8157, 2.3966, 0.0312, 0.1179, 0.5081, 0.1738],\n",
      "       grad_fn=<AddBackward0>) tensor([7.4000, 2.7000, 2.9000, 0.0220, 0.0930, 0.4940, 0.2020]) tensor(0.6919, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7159,  2.0558,  3.0726,  0.0216,  0.1137,  0.4955,  0.2412],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 0.3000, 0.5000, 0.0530, 0.0480, 0.4290, 0.4440]) tensor(19.3641, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.9745,  5.7139,  3.0751,  0.0534,  0.1623,  0.5805,  0.1653],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 3.1000, 1.8000, 0.0220, 0.1290, 0.5420, 0.1250]) tensor(4.4656, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.0517,  4.2520,  3.2451,  0.0298,  0.1281,  0.5583,  0.1988],\n",
      "       grad_fn=<AddBackward0>) tensor([14.4000,  4.5000,  3.3000,  0.0360,  0.1270,  0.5330,  0.1880]) tensor(0.0267, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7084,  4.5603,  2.3830,  0.0478,  0.1467,  0.5475,  0.1446],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 3.7000, 1.5000, 0.0540, 0.1110, 0.5690, 0.0850]) tensor(0.9791, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6310,  3.5455,  3.3958,  0.0231,  0.1047,  0.5376,  0.1973],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 2.3000, 5.1000, 0.0140, 0.0970, 0.4600, 0.3320]) tensor(6.0099, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.6209, 1.3015, 1.7522, 0.0319, 0.1129, 0.4626, 0.1710],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 1.4000, 1.2000, 0.0080, 0.1080, 0.5150, 0.1670]) tensor(0.3339, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6490, 3.9917, 1.8171, 0.0576, 0.1601, 0.5150, 0.1363],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 3.2000, 0.5000, 0.0800, 0.1890, 0.5090, 0.0660]) tensor(3.0403, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.1458,  7.0218,  3.0278,  0.0650,  0.1946,  0.6185,  0.1605],\n",
      "       grad_fn=<AddBackward0>) tensor([18.9000,  9.6000,  3.5000,  0.1010,  0.1940,  0.6170,  0.1460]) tensor(1.4213, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.3863,  4.0142,  3.4883,  0.0278,  0.1240,  0.5439,  0.2183],\n",
      "       grad_fn=<AddBackward0>) tensor([18.0000,  4.4000,  4.1000,  0.0220,  0.1070,  0.5530,  0.2090]) tensor(1.9403, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4473,  8.1182,  1.9581,  0.0899,  0.2402,  0.6220,  0.1115],\n",
      "       grad_fn=<AddBackward0>) tensor([13.6000, 13.8000,  1.1000,  0.1460,  0.3730,  0.5180,  0.0600]) tensor(5.8802, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2034,  4.2886,  3.5828,  0.0280,  0.1206,  0.5608,  0.2062],\n",
      "       grad_fn=<AddBackward0>) tensor([13.9000,  3.5000,  4.3000,  0.0250,  0.0990,  0.5560,  0.2040]) tensor(0.1756, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.1998,  4.9631,  3.9757,  0.0320,  0.1269,  0.5762,  0.2110],\n",
      "       grad_fn=<AddBackward0>) tensor([6.7000, 1.5000, 1.3000, 0.0170, 0.0750, 0.5900, 0.1100]) tensor(13.0588, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6659, 1.3357, 3.0347, 0.0160, 0.0900, 0.4827, 0.2243],\n",
      "       grad_fn=<AddBackward0>) tensor([7.7000, 2.5000, 4.2000, 0.0130, 0.1120, 0.4810, 0.3030]) tensor(0.9407, grad_fn=<MseLossBackward0>)\n",
      "tensor([21.4288,  5.7700,  4.8954,  0.0286,  0.1422,  0.6012,  0.2756],\n",
      "       grad_fn=<AddBackward0>) tensor([28.3000,  6.3000,  5.4000,  0.0340,  0.1410,  0.5760,  0.2360]) tensor(6.8215, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1160,  3.6948,  3.3349,  0.0238,  0.1073,  0.5403,  0.1955],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 2.1000, 5.8000, 0.0090, 0.0870, 0.5160, 0.3630]) tensor(4.9747, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3053,  4.7322,  1.4990,  0.0593,  0.1721,  0.5586,  0.1106],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 3.4000, 0.3000, 0.1170, 0.1650, 0.4980, 0.0370]) tensor(5.2753, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.6874,  5.0595,  3.4764,  0.0352,  0.1389,  0.5771,  0.1999],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6600e+01, 3.7000e+00, 2.2000e+00, 1.2000e-02, 9.9000e-02, 5.3300e-01,\n",
      "        9.9000e-02]) tensor(0.6178, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.3578,  3.7314,  3.4513,  0.0263,  0.1153,  0.5326,  0.2146],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 2.2000, 4.0000, 0.0140, 0.0880, 0.5290, 0.2520]) tensor(5.7953, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9447, 3.9595, 1.1766, 0.0595, 0.1645, 0.5320, 0.1016],\n",
      "       grad_fn=<AddBackward0>) tensor([10.2000,  7.7000,  0.8000,  0.1580,  0.2310,  0.6180,  0.0550]) tensor(2.7490, grad_fn=<MseLossBackward0>)\n",
      "tensor([21.2457,  7.7801,  3.5340,  0.0649,  0.2084,  0.6311,  0.1980],\n",
      "       grad_fn=<AddBackward0>) tensor([13.9000,  5.4000,  1.0000,  0.0650,  0.1950,  0.5280,  0.0780]) tensor(9.4386, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6212,  5.6126,  1.9001,  0.0751,  0.1980,  0.5616,  0.1249],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 5.6000, 1.4000, 0.0660, 0.2160, 0.4710, 0.0950]) tensor(1.9105, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1821,  5.3240,  1.4265,  0.0751,  0.1961,  0.5584,  0.1032],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 4.2000, 1.2000, 0.0700, 0.1890, 0.5450, 0.1000]) tensor(1.6344, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.8718, 4.6711, 0.2294, 0.0792, 0.1899, 0.5331, 0.0384],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 4.1000, 0.7000, 0.0820, 0.1830, 0.5840, 0.0550]) tensor(0.6341, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.1202, 3.8843, 0.8676, 0.0656, 0.1631, 0.5235, 0.0714],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 1.9000, 0.1000, 0.0850, 0.1570, 0.6070, 0.0190]) tensor(2.2229, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5817,  4.3175,  3.7647,  0.0250,  0.1219,  0.5612,  0.2242],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7700e+01, 2.9000e+00, 4.3000e+00, 1.4000e-02, 7.5000e-02, 5.2100e-01,\n",
      "        1.9000e-01]) tensor(0.9698, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4450, 3.2698, 1.6254, 0.0538, 0.1498, 0.5142, 0.1271],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 1.2000, 0.6000, 0.0190, 0.1360, 0.3330, 0.1250]) tensor(5.3194, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.2426, 2.6684, 1.9058, 0.0443, 0.1320, 0.4945, 0.1496],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 1.9000, 1.1000, 0.0380, 0.1090, 0.4670, 0.1230]) tensor(1.6795, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.8221,  1.8455, -0.3671,  0.0615,  0.1543,  0.4607,  0.0544],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.3000, 0.5000, 0.0000]) tensor(0.1301, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.3362,  7.8947,  2.6362,  0.0818,  0.2350,  0.6210,  0.1613],\n",
      "       grad_fn=<AddBackward0>) tensor([18.1000, 11.0000,  1.6000,  0.1410,  0.2610,  0.4990,  0.0920]) tensor(1.7526, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2105, 4.4477, 1.6745, 0.0610, 0.1624, 0.5422, 0.1069],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 7.5000, 3.4000, 0.0680, 0.2040, 0.4790, 0.1710]) tensor(1.9670, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.8841, 2.5029, 1.1088, 0.0567, 0.1492, 0.4825, 0.1157],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 1.0000, 0.1000, 0.0670, 0.1030, 0.4860, 0.0360]) tensor(2.1054, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4206,  6.1212,  2.7724,  0.0550,  0.1740,  0.5930,  0.1580],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 2.8000, 0.8000, 0.0380, 0.1460, 0.5390, 0.0770]) tensor(13.2477, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3592,  4.6574,  1.8273,  0.0610,  0.1756,  0.5463,  0.1369],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 1.3000, 1.0000, 0.0300, 0.0730, 0.4540, 0.1110]) tensor(7.4881, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.0309, 2.9672, 0.3753, 0.0687, 0.1708, 0.4842, 0.0816],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 4.0000, 1.8000, 0.0990, 0.2650, 0.5440, 0.2250]) tensor(0.4875, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.8897,  7.4419,  3.4910,  0.0646,  0.2029,  0.6245,  0.1926],\n",
      "       grad_fn=<AddBackward0>) tensor([23.1000,  8.9000,  1.0000,  0.0980,  0.1850,  0.6150,  0.0470]) tensor(2.6657, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6622, 0.0363, 0.4976, 0.0291, 0.0982, 0.4248, 0.1148],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 1.0000, 0.0000, 0.1050, 0.0670, 0.3400, 0.0000]) tensor(0.2300, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.9491,  3.2793, -0.2075,  0.0774,  0.1845,  0.4908,  0.0487],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 2.7000, 0.3000, 0.0680, 0.1730, 0.5670, 0.0340]) tensor(0.1891, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6273,  5.5387,  2.8050,  0.0533,  0.1640,  0.5788,  0.1573],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 1.7000, 0.9000, 0.0290, 0.1110, 0.4790, 0.1020]) tensor(15.3222, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.9787,  6.0488,  3.1259,  0.0619,  0.1996,  0.5891,  0.2098],\n",
      "       grad_fn=<AddBackward0>) tensor([20.2000,  7.8000,  2.1000,  0.0920,  0.2470,  0.5840,  0.1740]) tensor(0.8022, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7128, 3.1387, 1.6179, 0.0448, 0.1371, 0.5183, 0.1263],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 0.9000, 0.6000, 0.0190, 0.0770, 0.5120, 0.0950]) tensor(4.4544, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 3.9839,  4.2385, -0.4286,  0.0953,  0.2198,  0.4978,  0.0406],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 3.4000, 0.6000, 0.0380, 0.2240, 0.4050, 0.0720]) tensor(0.2745, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5165,  7.4030,  1.7615,  0.0949,  0.2461,  0.5862,  0.1259],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 5.4000, 0.8000, 0.0860, 0.1890, 0.4660, 0.0620]) tensor(6.2290, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.8196, 2.2053, 1.6016, 0.0403, 0.1240, 0.4895, 0.1361],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 1.7000, 0.4000, 0.1150, 0.1370, 0.4010, 0.1070]) tensor(2.5530, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0046,  5.5551,  2.7496,  0.0587,  0.1701,  0.5754,  0.1530],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([7.1000, 6.0000, 1.1000, 0.0790, 0.2210, 0.4780, 0.0760]) tensor(5.4003, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.7814,  2.0894, -0.1844,  0.0638,  0.1582,  0.4695,  0.0605],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 4.0000, 0.6000, 0.0630, 0.1930, 0.6820, 0.0630]) tensor(1.3192, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6007,  5.6023,  1.0998,  0.0863,  0.2234,  0.5603,  0.1054],\n",
      "       grad_fn=<AddBackward0>) tensor([20.7000,  6.0000,  0.9000,  0.0880,  0.1260,  0.6290,  0.0590]) tensor(11.8588, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.3427,  6.4409,  2.2787,  0.0665,  0.1859,  0.5994,  0.1179],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 9.0000, 0.8000, 0.1330, 0.2330, 0.6110, 0.0400]) tensor(5.6385, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.6348, 1.4121, 1.0417, 0.0405, 0.1268, 0.4508, 0.1395],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 0.6000, 0.4000, 0.0240, 0.0700, 0.5470, 0.1070]) tensor(0.9338, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.8279,  5.8703,  2.9551,  0.0587,  0.1758,  0.5794,  0.1698],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 3.3000, 0.6000, 0.0570, 0.1370, 0.5590, 0.0540]) tensor(6.4253, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4889,  5.9680,  2.1483,  0.0664,  0.1963,  0.5778,  0.1457],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 4.1000, 0.6000, 0.0980, 0.2080, 0.5240, 0.0620]) tensor(5.7967, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5833,  5.3185,  2.1067,  0.0576,  0.1729,  0.5719,  0.1342],\n",
      "       grad_fn=<AddBackward0>) tensor([8.9000, 6.0000, 0.7000, 0.0990, 0.2010, 0.6190, 0.0510]) tensor(2.2888, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3592, 2.9434, 2.1991, 0.0410, 0.1338, 0.5103, 0.1659],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 1.0000, 1.2000, 0.0160, 0.0940, 0.6350, 0.2150]) tensor(3.3996, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.7857, 4.4417, 0.0914, 0.0911, 0.2117, 0.5200, 0.0542],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 2.9000, 0.1000, 0.1700, 0.2900, 0.4550, 0.0250]) tensor(2.9660, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6470,  5.9477,  2.1380,  0.0694,  0.1910,  0.5750,  0.1293],\n",
      "       grad_fn=<AddBackward0>) tensor([13.4000,  9.5000,  3.7000,  0.0810,  0.2340,  0.5580,  0.1610]) tensor(2.2327, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7730,  5.5392,  2.1036,  0.0624,  0.1811,  0.5802,  0.1305],\n",
      "       grad_fn=<AddBackward0>) tensor([11.8000,  4.6000,  1.4000,  0.0360,  0.1550,  0.5340,  0.0760]) tensor(0.3329, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6184, 3.5236, 1.1264, 0.0514, 0.1410, 0.5274, 0.0831],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 3.8000, 1.0000, 0.0520, 0.1590, 0.5130, 0.0750]) tensor(0.7163, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5937, 2.5265, 1.6277, 0.0412, 0.1223, 0.4892, 0.1293],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 2.2000, 1.1000, 0.0160, 0.1760, 0.4860, 0.1310]) tensor(1.6053, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.9560,  3.8045,  2.3750,  0.0578,  0.1795,  0.5246,  0.2029],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.4000, 0.4000, 0.3000, 0.1540, 0.4330, 0.2500]) tensor(21.4738, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7766, 3.8009, 1.9480, 0.0501, 0.1414, 0.5330, 0.1217],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 3.1000, 1.7000, 0.0180, 0.1350, 0.4800, 0.1080]) tensor(1.1809, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.3202, 1.1379, 2.1624, 0.0275, 0.1005, 0.4748, 0.1757],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 0.5000, 0.0000, 0.0590, 0.0530, 0.0000, 0.0000]) tensor(6.4697, grad_fn=<MseLossBackward0>)\n",
      "tensor([-1.3599,  2.0843, -1.0397,  0.0720,  0.1656,  0.4678,  0.0085],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 2.9000, 0.2000, 0.0600, 0.1460, 0.4480, 0.0150]) tensor(3.0303, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8614, 2.4549, 2.6430, 0.0250, 0.1043, 0.5085, 0.1800],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 2.0000, 1.1000, 0.0140, 0.1020, 0.5200, 0.0890]) tensor(0.3984, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2269, 5.9441, 1.1648, 0.0780, 0.1981, 0.5709, 0.0711],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4000, 3.3000, 0.3000, 0.1200, 0.1870, 0.4950, 0.0330]) tensor(7.7650, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.8580, 2.6733, 0.4489, 0.0659, 0.1635, 0.4845, 0.0842],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 2.9000, 0.8000, 0.1450, 0.1400, 0.4620, 0.1050]) tensor(0.1312, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9865, 4.2223, 0.8688, 0.0721, 0.1871, 0.5343, 0.0913],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 5.4000, 0.9000, 0.1180, 0.2150, 0.4590, 0.0930]) tensor(1.2306, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.5791, 0.3497, 1.0306, 0.0262, 0.0943, 0.4452, 0.1317],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 2.7000, 1.2000, 0.0180, 0.1250, 0.4380, 0.1300]) tensor(1.4360, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.3352,  5.5293,  3.6885,  0.0455,  0.1605,  0.5768,  0.2192],\n",
      "       grad_fn=<AddBackward0>) tensor([17.7000,  5.7000,  2.6000,  0.0380,  0.1390,  0.5800,  0.1210]) tensor(0.1939, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3563,  5.1925,  2.2004,  0.0579,  0.1666,  0.5614,  0.1323],\n",
      "       grad_fn=<AddBackward0>) tensor([10.6000,  8.2000,  1.1000,  0.0770,  0.2150,  0.5530,  0.0510]) tensor(1.5482, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5773, 2.2087, 0.8375, 0.0501, 0.1460, 0.4814, 0.1184],\n",
      "       grad_fn=<AddBackward0>) tensor([3.9000, 2.2000, 0.4000, 0.0880, 0.1210, 0.4790, 0.0570]) tensor(0.4301, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.1809, 2.0496, 0.7528, 0.0502, 0.1357, 0.4735, 0.0991],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 1.6000, 0.3000, 0.0660, 0.1710, 0.7140, 0.0670]) tensor(0.0781, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.2248, 2.7985, 1.1059, 0.0484, 0.1412, 0.5097, 0.1072],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 1.4000, 0.1000, 0.0440, 0.1740, 0.4990, 0.0370]) tensor(2.9747, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.3312, 4.3514, 1.1068, 0.0641, 0.1642, 0.5415, 0.0747],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 3.2000, 0.9000, 0.0510, 0.1740, 0.5370, 0.0810]) tensor(1.5961, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2462, 2.7534, 2.1536, 0.0307, 0.1130, 0.5234, 0.1478],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 2.2000, 0.8000, 0.0270, 0.1800, 0.5540, 0.0930]) tensor(2.3115, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0402,  3.4902,  3.2057,  0.0278,  0.1099,  0.5307,  0.1902],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 2.9000, 5.5000, 0.0150, 0.0930, 0.5310, 0.2660]) tensor(2.3024, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1368, 3.4717, 2.2193, 0.0420, 0.1327, 0.5179, 0.1530],\n",
      "       grad_fn=<AddBackward0>) tensor([5.4000, 4.2000, 1.6000, 0.0440, 0.1510, 0.5650, 0.1150]) tensor(2.1259, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.9886,  7.4115,  2.6718,  0.0738,  0.2114,  0.6121,  0.1486],\n",
      "       grad_fn=<AddBackward0>) tensor([13.2000,  5.1000,  2.7000,  0.0630,  0.1320,  0.5300,  0.1520]) tensor(2.8158, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7610,  3.8150,  3.1452,  0.0345,  0.1192,  0.5433,  0.1772],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 2.3000, 2.8000, 0.0110, 0.1250, 0.5580, 0.2500]) tensor(4.2998, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2462,  4.7918,  1.9387,  0.0566,  0.1625,  0.5538,  0.1230],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 5.6000, 0.5000, 0.1250, 0.2350, 0.5150, 0.0430]) tensor(3.2157, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1642, 4.7552, 1.9482, 0.0559, 0.1562, 0.5554, 0.1128],\n",
      "       grad_fn=<AddBackward0>) tensor([12.2000,  7.1000,  1.2000,  0.0720,  0.1460,  0.6100,  0.0510]) tensor(2.1831, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3387, 4.7493, 0.6746, 0.0777, 0.1929, 0.5426, 0.0680],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 4.5000, 0.4000, 0.1400, 0.2530, 0.5890, 0.0440]) tensor(1.9126, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3653,  5.3650,  1.3969,  0.0695,  0.1890,  0.5598,  0.1015],\n",
      "       grad_fn=<AddBackward0>) tensor([10.1000,  5.1000,  1.5000,  0.0530,  0.1800,  0.5300,  0.1120]) tensor(0.0218, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.7204,  6.9360,  3.1198,  0.0639,  0.2017,  0.6090,  0.1857],\n",
      "       grad_fn=<AddBackward0>) tensor([21.4000,  5.2000,  1.8000,  0.0370,  0.1410,  0.5330,  0.0860]) tensor(1.7080, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0393,  7.0302,  1.1562,  0.0967,  0.2442,  0.5946,  0.0868],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 3.6000, 0.4000, 0.1660, 0.2750, 0.6620, 0.0720]) tensor(9.8842, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.1654,  5.9772,  3.9663,  0.0410,  0.1590,  0.5980,  0.2268],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2700e+01, 4.3000e+00, 4.0000e+00, 2.0000e-02, 1.1300e-01, 5.3200e-01,\n",
      "        1.8900e-01]) tensor(2.1879, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6452, 2.2748, 2.9628, 0.0200, 0.0964, 0.4955, 0.2037],\n",
      "       grad_fn=<AddBackward0>) tensor([8.7000, 2.4000, 2.3000, 0.0190, 0.0870, 0.5750, 0.1410]) tensor(0.1941, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.5099,  1.8711, -0.7049,  0.0690,  0.1703,  0.4362,  0.0569],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 7.0000, 0.0000, 0.1110, 0.7140, 0.0000, 0.0000]) tensor(3.9362, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4332,  5.6451,  2.5302,  0.0578,  0.1725,  0.5740,  0.1495],\n",
      "       grad_fn=<AddBackward0>) tensor([10.7000,  8.3000,  1.2000,  0.0920,  0.2120,  0.5230,  0.0640]) tensor(2.3287, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.7330,  4.8403,  4.0465,  0.0320,  0.1394,  0.5718,  0.2426],\n",
      "       grad_fn=<AddBackward0>) tensor([23.0000,  5.1000,  4.8000,  0.0380,  0.1180,  0.5660,  0.2230]) tensor(4.0539, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.1729,  5.6102,  3.8412,  0.0415,  0.1524,  0.5789,  0.2188],\n",
      "       grad_fn=<AddBackward0>) tensor([17.2000,  2.7000,  4.4000,  0.0230,  0.0590,  0.5090,  0.2140]) tensor(1.2566, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6076,  4.2322,  2.6176,  0.0411,  0.1441,  0.5457,  0.1741],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 0.5000, 1.4000, 0.0130, 0.0570, 0.4820, 0.2860]) tensor(15.9466, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.3549,  5.6619,  2.7294,  0.0533,  0.1671,  0.5865,  0.1562],\n",
      "       grad_fn=<AddBackward0>) tensor([10.1000,  4.5000,  2.0000,  0.0350,  0.1630,  0.5140,  0.1260]) tensor(2.8561, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1877,  5.1361,  3.0208,  0.0469,  0.1564,  0.5624,  0.1826],\n",
      "       grad_fn=<AddBackward0>) tensor([8.5000, 3.0000, 1.0000, 0.0240, 0.1490, 0.5040, 0.0770]) tensor(5.8588, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.7416,  2.2558, -1.0570,  0.0812,  0.1823,  0.4613,  0.0174],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 9.3000, 0.8000, 0.0910, 0.3190, 0.3890, 0.0530]) tensor(10.4032, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8034,  4.6993,  2.4832,  0.0498,  0.1541,  0.5565,  0.1522],\n",
      "       grad_fn=<AddBackward0>) tensor([16.1000,  6.6000,  3.5000,  0.0310,  0.1520,  0.6260,  0.1280]) tensor(3.3019, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7119,  3.0646,  2.5468,  0.0402,  0.1546,  0.4907,  0.2303],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 0.7000, 0.0000, 0.0610, 0.1210, 0.4810, 0.0000]) tensor(19.0563, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2952,  1.9450,  3.5769,  0.0120,  0.0866,  0.4978,  0.2397],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3900e+01, 2.7000e+00, 4.9000e+00, 9.0000e-03, 8.3000e-02, 4.5300e-01,\n",
      "        2.4200e-01]) tensor(1.3011, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6845,  4.6061,  2.3164,  0.0503,  0.1516,  0.5527,  0.1403],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 1.5000, 1.1000, 0.0210, 0.0830, 0.5490, 0.1010]) tensor(5.7326, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0815,  4.8530,  2.6726,  0.0472,  0.1442,  0.5602,  0.1460],\n",
      "       grad_fn=<AddBackward0>) tensor([5.9000, 2.5000, 1.0000, 0.0140, 0.1230, 0.5440, 0.0740]) tensor(5.0270, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6296, 5.3338, 1.4481, 0.0754, 0.1947, 0.5450, 0.1060],\n",
      "       grad_fn=<AddBackward0>) tensor([3.9000, 3.9000, 1.3000, 0.1010, 0.1840, 0.5390, 0.1610]) tensor(4.9871, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.7514,  4.7064,  4.3529,  0.0228,  0.1227,  0.5705,  0.2520],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5200e+01, 2.4000e+00, 7.1000e+00, 9.0000e-03, 8.2000e-02, 5.4500e-01,\n",
      "        3.7300e-01]) tensor(2.7704, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.0389, 0.2677, 2.1506, 0.0140, 0.0714, 0.4406, 0.1800],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4000, 0.7000, 1.7000, 0.0030, 0.0860, 0.4450, 0.2720]) tensor(0.4407, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7472,  4.5017,  2.5558,  0.0500,  0.1606,  0.5357,  0.1794],\n",
      "       grad_fn=<AddBackward0>) tensor([14.2000,  4.4000,  1.6000,  0.0710,  0.1040,  0.5110,  0.1100]) tensor(0.4348, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.4227,  6.6597,  3.3315,  0.0587,  0.1840,  0.6030,  0.1834],\n",
      "       grad_fn=<AddBackward0>) tensor([8.5000, 4.3000, 1.7000, 0.0610, 0.1600, 0.4620, 0.1220]) tensor(12.5527, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1541,  4.0279,  2.1534,  0.0643,  0.1843,  0.5399,  0.1765],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 0.5000, 0.0000, 0.0000, 0.1430, 0.3330, 0.0000]) tensor(23.5551, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5122,  5.1509,  2.4776,  0.0533,  0.1677,  0.5628,  0.1603],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 2.0000, 0.9000, 0.0480, 0.0850, 0.4560, 0.0870]) tensor(9.0034, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.1236,  6.4559,  3.1438,  0.0550,  0.1781,  0.6055,  0.1748],\n",
      "       grad_fn=<AddBackward0>) tensor([16.9000,  8.8000,  7.7000,  0.0620,  0.1790,  0.5820,  0.3410]) tensor(3.7616, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.1770, 1.0636, 1.2788, 0.0331, 0.1109, 0.4520, 0.1440],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 0.2000, 0.2000, 0.0710, 0.0000, 0.7090, 0.1110]) tensor(1.4668, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.0586,  7.3376,  2.8866,  0.0688,  0.2059,  0.6221,  0.1595],\n",
      "       grad_fn=<AddBackward0>) tensor([10.2000,  3.7000,  1.1000,  0.0310,  0.1520,  0.4950,  0.0730]) tensor(11.1727, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0018,  7.1745,  1.3345,  0.0900,  0.2310,  0.6010,  0.0831],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 4.3000, 0.6000, 0.0960, 0.2030, 0.5330, 0.0540]) tensor(9.2980, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3652,  4.7028,  1.4069,  0.0675,  0.1860,  0.5364,  0.1218],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 2.6000, 0.6000, 0.0990, 0.1210, 0.5640, 0.0730]) tensor(3.9697, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2744,  4.5760,  2.9505,  0.0367,  0.1360,  0.5644,  0.1750],\n",
      "       grad_fn=<AddBackward0>) tensor([11.3000,  2.5000,  1.2000,  0.0180,  0.0850,  0.5990,  0.0740]) tensor(1.6124, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5072,  5.6687,  1.5654,  0.0719,  0.1901,  0.5697,  0.0979],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 4.5000, 0.3000, 0.1430, 0.2110, 0.5620, 0.0290]) tensor(4.9168, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4221, 2.3441, 1.6308, 0.0506, 0.1359, 0.4978, 0.1288],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4000, 1.6000, 0.2000, 0.0630, 0.2220, 0.4320, 0.0340]) tensor(1.6793, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5447, 2.6891, 2.0436, 0.0411, 0.1273, 0.5078, 0.1489],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 2.4000, 1.6000, 0.0100, 0.0820, 0.4770, 0.0840]) tensor(0.2003, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2046, 1.4641, 1.6220, 0.0339, 0.1302, 0.4574, 0.1880],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 1.4000, 0.0000, 0.0290, 0.1760, 0.5130, 0.0000]) tensor(2.0382, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5741,  3.6268,  3.4038,  0.0269,  0.1204,  0.5369,  0.2178],\n",
      "       grad_fn=<AddBackward0>) tensor([11.1000,  2.0000,  1.5000,  0.0270,  0.0800,  0.5120,  0.1350]) tensor(1.7715, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.8936e+01, 4.2593e+00, 4.6031e+00, 1.7378e-02, 1.1889e-01, 5.5075e-01,\n",
      "        2.8711e-01], grad_fn=<AddBackward0>) tensor([2.4100e+01, 3.5000e+00, 5.6000e+00, 1.5000e-02, 8.6000e-02, 5.3600e-01,\n",
      "        2.3400e-01]) tensor(4.0348, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5461,  4.2391,  2.8232,  0.0410,  0.1358,  0.5500,  0.1669],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 4.6000, 0.7000, 0.0890, 0.1810, 0.5070, 0.0630]) tensor(4.1599, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.7359, 3.5854, 0.4062, 0.0720, 0.1820, 0.5110, 0.0767],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 1.6000, 0.1000, 0.1150, 0.2050, 0.4320, 0.0190]) tensor(2.9052, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5112,  1.5293,  3.3653,  0.0146,  0.1069,  0.4679,  0.2791],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 0.0000, 0.3000, 0.0000, 0.0000, 0.7000, 0.2500]) tensor(21.2790, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6824, 2.6771, 2.2331, 0.0399, 0.1278, 0.5048, 0.1656],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 2.3000, 3.0000, 0.0440, 0.0860, 0.4100, 0.2330]) tensor(1.5534, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6552, 6.1509, 1.1406, 0.0839, 0.2089, 0.5798, 0.0693],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 5.7000, 0.5000, 0.1110, 0.2180, 0.6520, 0.0440]) tensor(1.4220, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3879,  3.1589,  3.8660,  0.0164,  0.0934,  0.5384,  0.2222],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4800e+01, 2.4000e+00, 2.2000e+00, 1.0000e-02, 7.4000e-02, 5.8200e-01,\n",
      "        1.2500e-01]) tensor(1.3116, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8714,  3.1469,  3.1453,  0.0203,  0.0995,  0.5225,  0.1942],\n",
      "       grad_fn=<AddBackward0>) tensor([10.5000,  2.5000,  4.2000,  0.0120,  0.0800,  0.5180,  0.2250]) tensor(0.2386, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.2560,  6.2320,  2.7076,  0.0661,  0.1975,  0.5949,  0.1672],\n",
      "       grad_fn=<AddBackward0>) tensor([17.1000,  6.9000,  1.3000,  0.0850,  0.2010,  0.5350,  0.0830]) tensor(0.4501, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.8965, 3.4493, 1.6474, 0.0617, 0.1643, 0.4856, 0.1470],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 3.5000, 0.3000, 0.1140, 0.2240, 0.4050, 0.0400]) tensor(6.2925, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.6167, 1.5678, 2.1902, 0.0289, 0.1031, 0.4790, 0.1708],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 0.3000, 0.4000, 0.0000, 0.0650, 0.4850, 0.0860]) tensor(4.0032, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1047,  6.3158,  2.5601,  0.0625,  0.1819,  0.5981,  0.1365],\n",
      "       grad_fn=<AddBackward0>) tensor([11.1000,  6.9000,  1.2000,  0.0810,  0.2050,  0.5870,  0.0690]) tensor(1.6035, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.2378,  4.6166,  3.5494,  0.0325,  0.1392,  0.5609,  0.2225],\n",
      "       grad_fn=<AddBackward0>) tensor([14.8000,  3.3000,  3.9000,  0.0190,  0.0980,  0.5310,  0.2420]) tensor(0.5609, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.4105, 2.1272, 1.1255, 0.0453, 0.1220, 0.4925, 0.0968],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 1.3000, 0.3000, 0.0290, 0.0900, 0.4040, 0.0380]) tensor(0.6652, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4942,  4.5277,  2.3543,  0.0469,  0.1440,  0.5580,  0.1364],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 3.0000, 1.5000, 0.0360, 0.1050, 0.4900, 0.0990]) tensor(2.8334, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5906,  4.9637,  2.0810,  0.0554,  0.1656,  0.5638,  0.1331],\n",
      "       grad_fn=<AddBackward0>) tensor([15.3000,  5.7000,  1.7000,  0.0350,  0.1520,  0.5370,  0.0790]) tensor(2.0645, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7859, 3.4270, 1.9929, 0.0476, 0.1475, 0.5278, 0.1508],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4000, 1.4000, 0.3000, 0.0580, 0.1690, 0.4090, 0.0710]) tensor(8.7924, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3701,  3.4632,  2.3985,  0.0429,  0.1439,  0.5112,  0.1839],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.3000, 0.6000, 0.0140, 0.1070, 0.4440, 0.1110]) tensor(10.4360, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0165,  5.8956,  1.7634,  0.0731,  0.2030,  0.5706,  0.1263],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 2.4000, 0.3000, 0.1110, 0.1340, 0.4600, 0.0500]) tensor(12.6612, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5285,  3.8638,  4.1774,  0.0166,  0.1035,  0.5555,  0.2418],\n",
      "       grad_fn=<AddBackward0>) tensor([14.9000,  3.8000,  3.7000,  0.0180,  0.1040,  0.4800,  0.1620]) tensor(0.0913, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.2313, 5.6128, 0.2213, 0.0906, 0.2126, 0.5559, 0.0296],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 4.9000, 1.1000, 0.0780, 0.1900, 0.4740, 0.0760]) tensor(1.3294, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1614,  4.2157,  2.7015,  0.0441,  0.1451,  0.5493,  0.1711],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 5.2000, 0.7000, 0.1070, 0.2120, 0.5330, 0.0580]) tensor(2.5256, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3014,  3.8755,  2.1206,  0.0484,  0.1501,  0.5333,  0.1510],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 2.9000, 0.5000, 0.0350, 0.1460, 0.6250, 0.0490]) tensor(0.6872, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7575, 4.0967, 1.9087, 0.0470, 0.1379, 0.5381, 0.1140],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 7.9000, 0.8000, 0.1000, 0.2070, 0.5230, 0.0450]) tensor(2.2883, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4933,  5.4715,  2.7464,  0.0504,  0.1635,  0.5798,  0.1632],\n",
      "       grad_fn=<AddBackward0>) tensor([15.3000,  5.0000,  1.1000,  0.0640,  0.1110,  0.5490,  0.0570]) tensor(0.5141, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8929,  4.4051,  2.5743,  0.0492,  0.1587,  0.5408,  0.1789],\n",
      "       grad_fn=<AddBackward0>) tensor([13.7000,  4.7000,  2.8000,  0.0180,  0.1580,  0.5890,  0.1580]) tensor(0.1133, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0704,  3.7606,  2.2954,  0.0374,  0.1308,  0.5354,  0.1517],\n",
      "       grad_fn=<AddBackward0>) tensor([12.5000,  3.5000,  1.7000,  0.0280,  0.0990,  0.5250,  0.0860]) tensor(0.9044, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4403, 1.8464, 1.8573, 0.0416, 0.1438, 0.4617, 0.2008],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 1.5000, 1.0000, 0.0000, 0.1670, 0.3070, 0.3330]) tensor(9.1352, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.5671,  3.8021, -0.2311,  0.0813,  0.1885,  0.5129,  0.0267],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 3.8000, 0.7000, 0.0880, 0.1610, 0.5310, 0.0730]) tensor(0.4600, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.3146,  5.6402,  2.7041,  0.0619,  0.1822,  0.5779,  0.1634],\n",
      "       grad_fn=<AddBackward0>) tensor([16.4000,  9.8000,  3.0000,  0.0860,  0.2290,  0.5310,  0.1590]) tensor(3.1065, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.7050,  3.4853,  3.8555,  0.0218,  0.1125,  0.5411,  0.2401],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8300e+01, 2.9000e+00, 7.7000e+00, 1.3000e-02, 8.6000e-02, 5.3900e-01,\n",
      "        4.0400e-01]) tensor(4.0106, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0066, 2.1185, 3.0041, 0.0224, 0.0971, 0.4936, 0.2021],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 0.4000, 1.3000, 0.0080, 0.0490, 0.4330, 0.2450]) tensor(6.7013, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9985, 5.8944, 0.9751, 0.0842, 0.2118, 0.5628, 0.0787],\n",
      "       grad_fn=<AddBackward0>) tensor([7.4000, 8.3000, 0.3000, 0.1320, 0.2200, 0.6260, 0.0140]) tensor(1.8579, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.8880,  5.6819,  2.6906,  0.0580,  0.1771,  0.5831,  0.1630],\n",
      "       grad_fn=<AddBackward0>) tensor([16.5000,  7.2000,  2.5000,  0.0490,  0.2040,  0.5670,  0.1240]) tensor(0.7060, grad_fn=<MseLossBackward0>)\n",
      "tensor([22.4380,  6.5674,  4.8713,  0.0374,  0.1601,  0.6132,  0.2693],\n",
      "       grad_fn=<AddBackward0>) tensor([25.2000,  6.0000,  4.8000,  0.0430,  0.1420,  0.5590,  0.2450]) tensor(1.1371, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.1460, 1.4219, 1.1318, 0.0488, 0.1362, 0.4735, 0.1355],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 1.0000, 0.0000, 0.5000, 0.0000, 1.0000, 0.0000]) tensor(1.6963, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.1953, 3.6664, 0.4479, 0.0732, 0.1831, 0.5069, 0.0756],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 1.8000, 0.2000, 0.0760, 0.2620, 0.5800, 0.0500]) tensor(1.5460, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8833,  4.3309,  2.4786,  0.0437,  0.1410,  0.5468,  0.1525],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3300e+01, 3.2000e+00, 1.7000e+00, 1.2000e-02, 9.6000e-02, 5.3300e-01,\n",
      "        7.9000e-02]) tensor(1.1049, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7452,  5.4989,  2.5568,  0.0572,  0.1686,  0.5701,  0.1473],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 4.3000, 0.8000, 0.0730, 0.1260, 0.4800, 0.0590]) tensor(5.2014, grad_fn=<MseLossBackward0>)\n",
      "tensor([21.5915,  6.2365,  4.6784,  0.0361,  0.1551,  0.6073,  0.2614],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3100e+01, 5.2000e+00, 4.3000e+00, 1.9000e-02, 1.3200e-01, 5.7700e-01,\n",
      "        2.0900e-01]) tensor(0.4997, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6156,  4.2159,  3.4589,  0.0273,  0.1229,  0.5592,  0.2085],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5500e+01, 2.6000e+00, 5.3000e+00, 1.5000e-02, 7.1000e-02, 5.4200e-01,\n",
      "        2.6800e-01]) tensor(0.9700, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4943,  6.3756,  1.7678,  0.0712,  0.1950,  0.5942,  0.1002],\n",
      "       grad_fn=<AddBackward0>) tensor([9.5000, 9.3000, 1.1000, 0.1160, 0.2440, 0.5980, 0.0530]) tensor(2.5672, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1817,  2.8324,  2.8208,  0.0267,  0.1089,  0.5212,  0.1844],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 3.3000, 2.4000, 0.0500, 0.1210, 0.5350, 0.1920]) tensor(1.3267, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.4264, 3.0329, 0.3547, 0.0650, 0.1659, 0.4982, 0.0749],\n",
      "       grad_fn=<AddBackward0>) tensor([8.3000, 8.1000, 0.5000, 0.1240, 0.1550, 0.6420, 0.0300]) tensor(5.8181, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.5404, 3.1439, 1.6124, 0.0512, 0.1510, 0.5138, 0.1387],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 1.2000, 0.3000, 0.0300, 0.1610, 0.4790, 0.0630]) tensor(5.8281, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0465,  6.4958,  1.9243,  0.0723,  0.1979,  0.5988,  0.1066],\n",
      "       grad_fn=<AddBackward0>) tensor([7.4000, 6.6000, 1.0000, 0.1090, 0.1710, 0.5450, 0.0560]) tensor(4.6793, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0530, 3.9363, 1.0858, 0.0682, 0.1745, 0.5173, 0.1001],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 1.6000, 0.2000, 0.1080, 0.1760, 0.3420, 0.0430]) tensor(6.3054, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9431,  4.1388,  2.5067,  0.0465,  0.1503,  0.5366,  0.1721],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 1.6000, 0.3000, 0.0860, 0.1030, 0.5040, 0.0570]) tensor(11.8028, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6325,  3.1737,  3.2585,  0.0275,  0.1189,  0.5094,  0.2245],\n",
      "       grad_fn=<AddBackward0>) tensor([14.3000,  3.4000,  2.3000,  0.0270,  0.1000,  0.5020,  0.1380]) tensor(0.5369, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5691, 1.9010, 2.8419, 0.0251, 0.1063, 0.4777, 0.2150],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 1.5000, 1.7000, 0.0410, 0.0630, 0.5860, 0.2150]) tensor(2.0310, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.8889, 3.3094, 0.8404, 0.0593, 0.1590, 0.5181, 0.0885],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([5.7000, 3.6000, 1.3000, 0.0590, 0.1330, 0.4480, 0.0930]) tensor(0.0481, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4752,  3.0068,  3.6424,  0.0194,  0.0990,  0.5324,  0.2205],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 1.3000, 2.0000, 0.0210, 0.0680, 0.5500, 0.2230]) tensor(6.0744, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.7450,  4.5568,  3.5201,  0.0292,  0.1255,  0.5621,  0.2045],\n",
      "       grad_fn=<AddBackward0>) tensor([12.0000,  3.4000,  6.8000,  0.0170,  0.0890,  0.5100,  0.3200]) tensor(2.8069, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9583, 2.2780, 1.8937, 0.0363, 0.1247, 0.4922, 0.1616],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 0.9000, 0.3000, 0.0540, 0.0620, 0.4210, 0.0790]) tensor(4.1485, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9183,  6.6149,  1.6348,  0.0784,  0.2099,  0.5939,  0.0997],\n",
      "       grad_fn=<AddBackward0>) tensor([12.1000,  6.9000,  1.1000,  0.0920,  0.1850,  0.5770,  0.0670]) tensor(0.1485, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0869,  2.0439,  2.7958,  0.0214,  0.1019,  0.5048,  0.2018],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 0.5000, 0.4000, 0.0130, 0.0630, 0.4760, 0.1000]) tensor(7.7426, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6095,  5.3449,  2.5315,  0.0574,  0.1677,  0.5726,  0.1459],\n",
      "       grad_fn=<AddBackward0>) tensor([11.5000,  9.4000,  1.9000,  0.0820,  0.2320,  0.4810,  0.0870]) tensor(2.5843, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.0215,  6.8758,  2.4735,  0.0767,  0.2193,  0.5944,  0.1606],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 4.4000, 0.9000, 0.0830, 0.2250, 0.4470, 0.1060]) tensor(13.3809, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4472, 4.3658, 1.2715, 0.0637, 0.1790, 0.5307, 0.1165],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 1.4000, 0.4000, 0.1200, 0.1310, 0.5540, 0.1100]) tensor(7.4897, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.6430, 1.7116, 1.9114, 0.0349, 0.1157, 0.4704, 0.1669],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 0.3000, 0.8000, 0.0090, 0.0540, 0.3930, 0.2110]) tensor(4.6953, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.5300, 2.3451, 1.1063, 0.0494, 0.1348, 0.4923, 0.1063],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 0.9000, 0.5000, 0.0000, 0.1140, 0.3400, 0.0850]) tensor(1.5811, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.3821,  4.8086,  2.7893,  0.0535,  0.1693,  0.5454,  0.1919],\n",
      "       grad_fn=<AddBackward0>) tensor([9.9000, 2.0000, 1.0000, 0.0200, 0.1010, 0.5690, 0.0880]) tensor(4.4566, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9707, 3.0624, 0.7618, 0.0614, 0.1634, 0.5079, 0.0958],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 1.4000, 0.3000, 0.0270, 0.1360, 0.5200, 0.0580]) tensor(0.9806, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1739,  5.2779,  2.5263,  0.0540,  0.1708,  0.5710,  0.1626],\n",
      "       grad_fn=<AddBackward0>) tensor([13.0000,  2.3000,  1.4000,  0.0230,  0.0840,  0.5850,  0.0980]) tensor(1.6468, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0719, 4.0356, 1.6822, 0.0571, 0.1639, 0.5175, 0.1361],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 3.8000, 0.6000, 0.0520, 0.1490, 0.5050, 0.0480]) tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7309,  5.1332,  1.8863,  0.0657,  0.1796,  0.5470,  0.1276],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 4.3000, 1.1000, 0.0850, 0.1530, 0.6190, 0.0880]) tensor(1.9696, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8361, 3.7035, 2.1064, 0.0458, 0.1385, 0.5329, 0.1372],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 2.8000, 0.4000, 0.0830, 0.1420, 0.5000, 0.0470]) tensor(3.4736, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.7990, 1.1953, 0.4774, 0.0461, 0.1367, 0.4586, 0.1158],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 2.7000, 0.9000, 0.0570, 0.1710, 0.6170, 0.1030]) tensor(0.8690, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5787, 3.1849, 0.9138, 0.0658, 0.1701, 0.5121, 0.1026],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 0.6000, 0.4000, 0.0260, 0.0490, 0.3880, 0.0650]) tensor(3.2584, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5271,  4.2379,  2.8749,  0.0394,  0.1327,  0.5453,  0.1701],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 1.8000, 3.1000, 0.0200, 0.0770, 0.5290, 0.2300]) tensor(5.0649, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2900, 3.9017, 1.3982, 0.0554, 0.1579, 0.5371, 0.1089],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 1.6000, 0.7000, 0.0210, 0.1360, 0.4770, 0.1030]) tensor(4.6753, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.7229e+01, 3.3495e+00, 4.6180e+00, 7.9738e-03, 9.5118e-02, 5.4281e-01,\n",
      "        2.8324e-01], grad_fn=<AddBackward0>) tensor([12.1000,  2.0000,  6.1000,  0.0220,  0.0740,  0.5330,  0.4340]) tensor(4.3351, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3592, 4.1976, 0.7181, 0.0757, 0.1901, 0.5348, 0.0811],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 1.6000, 0.1000, 0.1170, 0.1830, 0.5760, 0.0130]) tensor(5.1227, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9147,  4.4051,  2.7289,  0.0457,  0.1462,  0.5417,  0.1711],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 1.7000, 1.9000, 0.0130, 0.1100, 0.4530, 0.1930]) tensor(8.1744, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.4068,  5.3576,  3.8352,  0.0437,  0.1574,  0.5715,  0.2294],\n",
      "       grad_fn=<AddBackward0>) tensor([17.0000,  3.9000,  1.9000,  0.0180,  0.0990,  0.5880,  0.0910]) tensor(0.8655, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9140,  4.0068,  2.3715,  0.0417,  0.1388,  0.5472,  0.1530],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 2.7000, 0.5000, 0.0590, 0.1270, 0.5690, 0.0480]) tensor(2.9345, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9641, 3.2658, 1.8340, 0.0445, 0.1406, 0.5231, 0.1415],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 0.8000, 0.7000, 0.0560, 0.0670, 0.3480, 0.1710]) tensor(8.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9442, 2.7934, 1.4477, 0.0444, 0.1310, 0.5101, 0.1148],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 2.8000, 0.6000, 0.0910, 0.1760, 0.5090, 0.0690]) tensor(1.1024, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2666,  5.4570,  1.7449,  0.0726,  0.1996,  0.5633,  0.1287],\n",
      "       grad_fn=<AddBackward0>) tensor([13.6000,  6.7000,  2.2000,  0.0710,  0.1880,  0.4890,  0.1300]) tensor(0.5051, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.6828,  3.2298,  3.8563,  0.0204,  0.1158,  0.5218,  0.2622],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 0.8000, 1.0000, 0.0100, 0.0780, 0.4780, 0.1920]) tensor(18.3134, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6761, 4.0693, 2.5536, 0.0426, 0.1315, 0.5468, 0.1446],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 3.8000, 3.3000, 0.0330, 0.1090, 0.5360, 0.1690]) tensor(2.0207, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0977, 3.5348, 1.8611, 0.0519, 0.1519, 0.5177, 0.1449],\n",
      "       grad_fn=<AddBackward0>) tensor([12.4000,  5.3000,  1.5000,  0.0490,  0.1300,  0.5720,  0.0720]) tensor(2.0228, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.8825, 2.2996, 1.1178, 0.0485, 0.1345, 0.4960, 0.1082],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1500e+01, 3.0000e+00, 4.4000e+00, 8.0000e-03, 8.9000e-02, 5.3400e-01,\n",
      "        1.8100e-01]) tensor(7.8665, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.7737, 3.4846, 1.3914, 0.0543, 0.1492, 0.5109, 0.1125],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 1.9000, 1.1000, 0.0290, 0.0960, 0.5370, 0.0920]) tensor(1.4706, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1650,  4.6637,  2.3971,  0.0476,  0.1475,  0.5630,  0.1397],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 7.6000, 1.2000, 0.0450, 0.1980, 0.5220, 0.0560]) tensor(1.9349, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.9893, 1.6753, 0.6970, 0.0546, 0.1405, 0.4670, 0.1031],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 1.8000, 0.5000, 0.0340, 0.0650, 0.4950, 0.0450]) tensor(0.5868, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0990,  4.4436,  2.3638,  0.0419,  0.1390,  0.5573,  0.1425],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 3.3000, 0.6000, 0.0440, 0.1380, 0.5160, 0.0420]) tensor(3.9230, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2374,  4.6914,  2.8450,  0.0441,  0.1470,  0.5605,  0.1720],\n",
      "       grad_fn=<AddBackward0>) tensor([10.9000,  4.5000,  9.0000,  0.0390,  0.1060,  0.4980,  0.3700]) tensor(6.2042, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.0149,  3.9314,  3.6513,  0.0252,  0.1156,  0.5473,  0.2172],\n",
      "       grad_fn=<AddBackward0>) tensor([7.5000, 1.5000, 3.2000, 0.0080, 0.0810, 0.5750, 0.2570]) tensor(6.9376, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3793,  4.3063,  1.8162,  0.0557,  0.1646,  0.5432,  0.1341],\n",
      "       grad_fn=<AddBackward0>) tensor([8.2000, 3.6000, 0.7000, 0.0880, 0.1640, 0.5440, 0.0720]) tensor(0.9284, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9559, 4.6072, 1.5661, 0.0607, 0.1681, 0.5488, 0.1073],\n",
      "       grad_fn=<AddBackward0>) tensor([11.7000,  5.1000,  3.0000,  0.0600,  0.1350,  0.5670,  0.1450]) tensor(1.4046, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7712,  7.6136,  1.4221,  0.0944,  0.2409,  0.6090,  0.0855],\n",
      "       grad_fn=<AddBackward0>) tensor([10.6000,  7.1000,  1.1000,  0.0930,  0.2170,  0.5010,  0.0760]) tensor(1.4909, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.8972,  6.8904,  2.9069,  0.0676,  0.2052,  0.6019,  0.1760],\n",
      "       grad_fn=<AddBackward0>) tensor([19.5000,  9.7000,  3.1000,  0.0670,  0.2230,  0.5380,  0.1580]) tensor(1.5007, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0448, 1.7455, 2.6759, 0.0237, 0.0971, 0.4829, 0.1934],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 1.3000, 2.0000, 0.0090, 0.0850, 0.4490, 0.2120]) tensor(1.6921, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.6415,  6.6640,  4.2770,  0.0481,  0.1746,  0.6060,  0.2395],\n",
      "       grad_fn=<AddBackward0>) tensor([22.4000,  5.9000,  2.9000,  0.0260,  0.1600,  0.5350,  0.1410]) tensor(0.7982, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.0791,  5.6328,  3.9200,  0.0364,  0.1531,  0.5871,  0.2342],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2100e+01, 3.2000e+00, 3.4000e+00, 1.8000e-02, 8.2000e-02, 5.3100e-01,\n",
      "        1.6000e-01]) tensor(2.1898, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4763,  4.0048,  3.0705,  0.0304,  0.1226,  0.5499,  0.1851],\n",
      "       grad_fn=<AddBackward0>) tensor([13.5000,  3.7000,  1.5000,  0.0330,  0.1140,  0.5670,  0.0840]) tensor(0.5169, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.6286, 2.2940, 1.1950, 0.0449, 0.1237, 0.4893, 0.1016],\n",
      "       grad_fn=<AddBackward0>) tensor([0.7000, 1.4000, 0.3000, 0.0800, 0.1800, 0.3820, 0.0980]) tensor(1.4561, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0421,  5.7562,  2.2981,  0.0644,  0.1846,  0.5781,  0.1389],\n",
      "       grad_fn=<AddBackward0>) tensor([14.1000,  9.3000,  1.4000,  0.1320,  0.2170,  0.5220,  0.0820]) tensor(2.0709, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.1979,  6.0626,  2.3066,  0.0646,  0.1938,  0.5886,  0.1490],\n",
      "       grad_fn=<AddBackward0>) tensor([17.4000,  4.6000,  1.4000,  0.0330,  0.1200,  0.5930,  0.0730]) tensor(1.1176, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1134, 3.7522, 1.0937, 0.0549, 0.1498, 0.5280, 0.0852],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 3.7000, 1.4000, 0.0190, 0.1090, 0.5260, 0.0720]) tensor(0.4206, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.9071, -0.2723,  1.4316,  0.0225,  0.0812,  0.4292,  0.1533],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 0.6000, 1.0000, 0.0130, 0.0880, 0.3790, 0.2160]) tensor(0.1599, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5203, 2.6587, 1.2092, 0.0507, 0.1409, 0.4973, 0.1127],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 1.4000, 0.7000, 0.0250, 0.1000, 0.4700, 0.0780]) tensor(0.7373, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.6275, 1.3148, 0.9536, 0.0392, 0.1131, 0.4691, 0.1070],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 0.8000, 2.1000, 0.0290, 0.0560, 0.5750, 0.2350]) tensor(0.2947, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4088, 5.2731, 0.9939, 0.0729, 0.1939, 0.5587, 0.0829],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 4.1000, 0.2000, 0.1060, 0.1840, 0.5390, 0.0240]) tensor(1.5806, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.1452,  4.5558,  4.1540,  0.0246,  0.1242,  0.5598,  0.2478],\n",
      "       grad_fn=<AddBackward0>) tensor([17.2000,  3.9000,  4.1000,  0.0470,  0.0980,  0.5490,  0.2280]) tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.3608,  8.1953,  1.6422,  0.0962,  0.2482,  0.6234,  0.0918],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 6.3000, 1.0000, 0.1090, 0.2070, 0.5270, 0.0680]) tensor(5.9960, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0772, 2.4237, 1.7774, 0.0368, 0.1220, 0.5027, 0.1426],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 1.6000, 0.4000, 0.0400, 0.0970, 0.4280, 0.0470]) tensor(1.4720, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3444,  4.3175,  2.9221,  0.0351,  0.1297,  0.5528,  0.1741],\n",
      "       grad_fn=<AddBackward0>) tensor([13.1000,  3.4000,  4.1000,  0.0180,  0.0940,  0.5540,  0.1840]) tensor(0.4003, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.1268, 1.1052, 1.5792, 0.0317, 0.1059, 0.4700, 0.1491],\n",
      "       grad_fn=<AddBackward0>) tensor([0.8000, 0.0000, 0.3000, 0.0000, 0.0000, 0.5000, 0.1430]) tensor(3.0846, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.8786, 0.2589, 2.0074, 0.0137, 0.0721, 0.4461, 0.1710],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 0.6000, 0.8000, 0.0100, 0.0390, 0.2620, 0.1300]) tensor(1.1800, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9297, 1.1805, 1.9025, 0.0254, 0.1039, 0.4611, 0.1804],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 0.5000, 1.1000, 0.0100, 0.0650, 0.4030, 0.1970]) tensor(3.2210, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3594,  4.6363,  2.5167,  0.0519,  0.1551,  0.5530,  0.1524],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 2.6000, 1.2000, 0.0210, 0.1270, 0.5000, 0.1070]) tensor(4.7925, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7898, 1.7118, 1.9786, 0.0315, 0.1163, 0.4735, 0.1793],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 0.7000, 0.9000, 0.0210, 0.0710, 0.4700, 0.2070]) tensor(4.0138, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6066,  4.6222,  2.9589,  0.0385,  0.1349,  0.5628,  0.1685],\n",
      "       grad_fn=<AddBackward0>) tensor([8.1000, 2.1000, 2.2000, 0.0120, 0.0650, 0.5030, 0.1200]) tensor(3.8940, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.1610,  7.2389,  3.4923,  0.0602,  0.1926,  0.6189,  0.1897],\n",
      "       grad_fn=<AddBackward0>) tensor([16.3000,  7.5000,  2.0000,  0.0820,  0.1830,  0.5570,  0.1260]) tensor(1.4984, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9072,  4.2132,  2.4746,  0.0406,  0.1349,  0.5518,  0.1493],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 2.3000, 1.2000, 0.0110, 0.0900, 0.5560, 0.0740]) tensor(1.6542, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.2636, 1.0758, 2.1927, 0.0207, 0.0863, 0.4632, 0.1718],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 0.7000, 1.0000, 0.0220, 0.0790, 0.4480, 0.2620]) tensor(1.8408, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0022, 3.0032, 0.8177, 0.0651, 0.1821, 0.4824, 0.1348],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 0.4000, 0.0000, 0.0000, 0.3330, 0.5150, 0.0000]) tensor(6.9256, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5858,  5.1035,  2.9030,  0.0494,  0.1576,  0.5706,  0.1697],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 5.7000, 1.5000, 0.0940, 0.1600, 0.4940, 0.1000]) tensor(2.6034, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1348,  4.4811,  2.8951,  0.0392,  0.1452,  0.5584,  0.1870],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 2.2000, 0.9000, 0.0670, 0.0910, 0.5060, 0.1000]) tensor(8.1841, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6050, 3.0135, 2.2223, 0.0367, 0.1279, 0.5180, 0.1635],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 0.8000, 1.9000, 0.0120, 0.0860, 0.4570, 0.3150]) tensor(6.7638, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.6865, 2.8170, 0.5335, 0.0653, 0.1593, 0.5059, 0.0709],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 1.8000, 0.1000, 0.0970, 0.2020, 0.5310, 0.0140]) tensor(0.9224, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4066, 4.1573, 0.4692, 0.0776, 0.1880, 0.5237, 0.0623],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 2.8000, 0.0000, 0.0730, 0.1780, 0.4830, 0.0050]) tensor(0.9294, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.4207, 0.7445, 2.0759, 0.0206, 0.0950, 0.4482, 0.1929],\n",
      "       grad_fn=<AddBackward0>) tensor([0.3000, 0.0000, 1.3000, 0.0000, 0.0000, 0.1290, 0.6670]) tensor(5.5651, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.4148,  4.9141,  4.3603,  0.0224,  0.1248,  0.5762,  0.2526],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7300e+01, 3.5000e+00, 5.1000e+00, 1.7000e-02, 9.4000e-02, 4.8600e-01,\n",
      "        2.7000e-01]) tensor(0.5427, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 3.8068, -0.4657,  2.3686,  0.0039,  0.0523,  0.4402,  0.1911],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 1.3000, 0.3000, 0.0000, 0.1380, 0.1750, 0.0430]) tensor(2.1964, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9428,  3.7454,  4.1345,  0.0170,  0.1021,  0.5402,  0.2442],\n",
      "       grad_fn=<AddBackward0>) tensor([9.0000, 2.4000, 3.6000, 0.0110, 0.0800, 0.5070, 0.1830]) tensor(5.3455, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.1880,  4.9134,  3.0965,  0.0498,  0.1632,  0.5619,  0.1979],\n",
      "       grad_fn=<AddBackward0>) tensor([13.2000,  4.6000,  4.5000,  0.0690,  0.1200,  0.5530,  0.2320]) tensor(0.8605, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6077,  4.3964,  3.3246,  0.0378,  0.1411,  0.5428,  0.2133],\n",
      "       grad_fn=<AddBackward0>) tensor([17.5000,  4.2000,  2.8000,  0.0270,  0.0870,  0.5370,  0.1210]) tensor(1.2416, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.2794,  6.3702,  3.8859,  0.0447,  0.1653,  0.6002,  0.2187],\n",
      "       grad_fn=<AddBackward0>) tensor([13.8000,  4.1000,  4.1000,  0.0200,  0.1290,  0.5140,  0.2250]) tensor(5.0332, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4779, 4.6932, 1.2895, 0.0638, 0.1720, 0.5359, 0.0987],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 2.2000, 0.3000, 0.0310, 0.1060, 0.4730, 0.0260]) tensor(5.7992, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.6831, 0.9548, 0.4472, 0.0479, 0.1286, 0.4434, 0.1040],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 5.5000, 0.5000, 0.1180, 0.1560, 0.3920, 0.0400]) tensor(4.5250, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7513, 3.1339, 2.1165, 0.0385, 0.1285, 0.5142, 0.1541],\n",
      "       grad_fn=<AddBackward0>) tensor([10.4000,  6.6000,  1.5000,  0.0920,  0.1730,  0.5920,  0.0850]) tensor(2.1611, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3012,  4.3357,  1.7728,  0.0528,  0.1584,  0.5463,  0.1268],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 3.6000, 0.7000, 0.0640, 0.1710, 0.5170, 0.0640]) tensor(3.9599, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1934,  5.7582,  2.0932,  0.0597,  0.1789,  0.5829,  0.1284],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 4.8000, 0.8000, 0.0970, 0.1860, 0.5430, 0.0630]) tensor(7.5592, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4822,  6.1484,  2.2414,  0.0682,  0.1919,  0.5814,  0.1337],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 3.2000, 0.7000, 0.0980, 0.1460, 0.5150, 0.0810]) tensor(10.4583, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.5715,  3.5208,  3.8859,  0.0205,  0.1098,  0.5281,  0.2451],\n",
      "       grad_fn=<AddBackward0>) tensor([16.3000,  3.5000,  4.5000,  0.0350,  0.0810,  0.5770,  0.2120]) tensor(0.4814, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5728,  5.0509,  1.9508,  0.0587,  0.1718,  0.5621,  0.1288],\n",
      "       grad_fn=<AddBackward0>) tensor([13.8000,  5.5000,  1.4000,  0.0260,  0.1390,  0.5490,  0.0620]) tensor(0.7818, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.4705,  6.6062,  3.7070,  0.0519,  0.1751,  0.6094,  0.2011],\n",
      "       grad_fn=<AddBackward0>) tensor([20.5000,  9.3000,  1.9000,  0.0570,  0.2170,  0.5180,  0.0830]) tensor(2.0949, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.7865,  5.5360,  3.9622,  0.0367,  0.1474,  0.5850,  0.2257],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0400e+01, 4.5000e+00, 4.3000e+00, 1.6000e-02, 1.1700e-01, 5.2800e-01,\n",
      "        2.0300e-01]) tensor(1.1461, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3378,  2.8599,  3.2051,  0.0226,  0.1042,  0.5117,  0.2107],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 1.2000, 1.3000, 0.0180, 0.0890, 0.5120, 0.1830]) tensor(8.3959, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4592,  5.5732,  2.7907,  0.0556,  0.1784,  0.5630,  0.1850],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 3.6000, 0.9000, 0.0490, 0.1750, 0.5590, 0.0910]) tensor(5.9725, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2819, 4.9372, 1.2384, 0.0670, 0.1806, 0.5593, 0.0908],\n",
      "       grad_fn=<AddBackward0>) tensor([5.9000, 4.5000, 0.6000, 0.0610, 0.2450, 0.6310, 0.0630]) tensor(1.7209, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1119,  5.2698,  2.5652,  0.0602,  0.1762,  0.5666,  0.1584],\n",
      "       grad_fn=<AddBackward0>) tensor([10.3000,  6.2000,  1.4000,  0.0710,  0.2260,  0.4840,  0.1020]) tensor(1.4489, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5387,  3.9224,  3.2970,  0.0281,  0.1211,  0.5452,  0.2037],\n",
      "       grad_fn=<AddBackward0>) tensor([8.1000, 2.1000, 0.8000, 0.0190, 0.1060, 0.5380, 0.0680]) tensor(5.5936, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.8434, 1.2651, 1.0551, 0.0400, 0.1195, 0.4626, 0.1269],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 2.5000, 0.7000, 0.0300, 0.0750, 0.5440, 0.0410]) tensor(4.1856, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2188,  5.2647,  1.6739,  0.0661,  0.1844,  0.5535,  0.1210],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 3.6000, 0.8000, 0.0910, 0.1530, 0.5200, 0.0820]) tensor(6.9544, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1122, 3.2329, 0.4611, 0.0637, 0.1721, 0.5083, 0.0875],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.9000, 0.2000, 0.1320, 0.1480, 0.3590, 0.0600]) tensor(2.5672, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0064,  4.9481,  1.8095,  0.0621,  0.1762,  0.5409,  0.1325],\n",
      "       grad_fn=<AddBackward0>) tensor([5.9000, 4.4000, 1.0000, 0.0670, 0.2000, 0.5330, 0.0910]) tensor(3.8619, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2251,  2.7380,  1.9550,  0.0404,  0.1429,  0.5081,  0.1741],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 2.0000, 0.6000, 0.0710, 0.1410, 0.4220, 0.1110]) tensor(4.0942, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7734, 2.4185, 2.4596, 0.0364, 0.1257, 0.4874, 0.1946],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 2.3000, 3.8000, 0.0140, 0.0940, 0.5390, 0.2760]) tensor(0.4569, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9396,  6.0514,  2.6049,  0.0587,  0.1808,  0.5880,  0.1549],\n",
      "       grad_fn=<AddBackward0>) tensor([16.1000,  5.1000,  1.7000,  0.0260,  0.1550,  0.5680,  0.0870]) tensor(0.4396, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7290,  2.2394,  2.9313,  0.0199,  0.1015,  0.4976,  0.2126],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 1.9000, 2.0000, 0.0530, 0.1180, 0.4870, 0.2220]) tensor(5.6836, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.0763, 3.0379, 0.7019, 0.0574, 0.1485, 0.5000, 0.0776],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 2.9000, 0.8000, 0.0440, 0.1250, 0.4360, 0.0640]) tensor(0.1410, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0878,  5.7393,  2.0634,  0.0682,  0.1882,  0.5678,  0.1284],\n",
      "       grad_fn=<AddBackward0>) tensor([11.9000,  7.8000,  3.3000,  0.0840,  0.2190,  0.5660,  0.1730]) tensor(0.8306, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3512,  2.1775,  3.0716,  0.0200,  0.0983,  0.4931,  0.2160],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 1.1000, 1.8000, 0.0160, 0.0720, 0.5670, 0.2180]) tensor(4.3370, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1345,  3.7422,  3.5763,  0.0254,  0.1135,  0.5355,  0.2159],\n",
      "       grad_fn=<AddBackward0>) tensor([12.1000,  3.4000,  7.0000,  0.0150,  0.1050,  0.5150,  0.3410]) tensor(1.8465, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.1482, 3.0337, 0.6042, 0.0609, 0.1624, 0.4885, 0.0949],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 1.5000, 0.5000, 0.0240, 0.1110, 0.4390, 0.0780]) tensor(0.9978, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.5270, 2.1752, 2.1939, 0.0378, 0.1277, 0.4790, 0.1858],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 1.8000, 1.6000, 0.0130, 0.1130, 0.4670, 0.1550]) tensor(0.2859, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7551,  2.7282,  3.7615,  0.0123,  0.0855,  0.5165,  0.2278],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 2.2000, 4.7000, 0.0170, 0.0860, 0.4600, 0.3100]) tensor(2.2902, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5056, 2.4489, 3.1594, 0.0177, 0.0885, 0.5064, 0.1987],\n",
      "       grad_fn=<AddBackward0>) tensor([8.1000, 1.6000, 2.1000, 0.0180, 0.0620, 0.6670, 0.1400]) tensor(0.5498, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.5835,  7.3565,  3.0423,  0.0711,  0.2120,  0.6128,  0.1759],\n",
      "       grad_fn=<AddBackward0>) tensor([20.5000,  5.4000,  2.3000,  0.0580,  0.1300,  0.5780,  0.1430]) tensor(1.1516, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.2423,  4.3036,  3.8570,  0.0290,  0.1252,  0.5495,  0.2298],\n",
      "       grad_fn=<AddBackward0>) tensor([6.2000, 1.1000, 0.9000, 0.0080, 0.0830, 0.5380, 0.1110]) tensor(14.3982, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.8597,  7.1272,  3.6794,  0.0593,  0.1930,  0.6187,  0.2037],\n",
      "       grad_fn=<AddBackward0>) tensor([20.1000,  7.6000,  5.4000,  0.0340,  0.1840,  0.4930,  0.2620]) tensor(0.4659, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3460, 2.5892, 1.6955, 0.0431, 0.1328, 0.5025, 0.1413],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 1.4000, 0.5000, 0.0400, 0.1490, 0.5370, 0.1030]) tensor(2.8623, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7665,  2.3356,  2.7479,  0.0272,  0.1150,  0.4999,  0.2069],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 0.8000, 2.1000, 0.0180, 0.0610, 0.4570, 0.2950]) tensor(5.1490, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.7315,  5.7010,  3.2125,  0.0531,  0.1730,  0.5806,  0.1952],\n",
      "       grad_fn=<AddBackward0>) tensor([17.1000,  3.7000,  6.4000,  0.0340,  0.0810,  0.5000,  0.3020]) tensor(2.0467, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 8.5332,  6.5776, -0.3885,  0.1165,  0.2697,  0.5599,  0.0254],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.3000, 0.1000, 0.1670, 0.1500, 0.5530, 0.0410]) tensor(9.9278, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4269,  4.0545,  2.3641,  0.0381,  0.1332,  0.5466,  0.1485],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([9.2000, 4.0000, 1.0000, 0.0270, 0.1380, 0.4690, 0.0590]) tensor(0.4833, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0570,  4.9285,  2.4625,  0.0519,  0.1589,  0.5534,  0.1531],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 1.8000, 1.7000, 0.0200, 0.0840, 0.5010, 0.1340]) tensor(5.8941, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3458,  3.6840,  2.5415,  0.0409,  0.1342,  0.5371,  0.1625],\n",
      "       grad_fn=<AddBackward0>) tensor([10.9000,  3.8000,  4.4000,  0.0170,  0.1210,  0.5280,  0.2260]) tensor(0.5399, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.6948, 1.3595, 1.7099, 0.0335, 0.1284, 0.4334, 0.2001],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.4290, 0.0000, 0.0000]) tensor(8.9403, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.9399, 3.6471, 0.6993, 0.0648, 0.1639, 0.5176, 0.0715],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 1.4000, 0.8000, 0.0150, 0.1120, 0.4230, 0.0930]) tensor(1.5070, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9476,  3.5599,  2.1956,  0.0443,  0.1472,  0.5206,  0.1698],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 3.8000, 0.9000, 0.1040, 0.1940, 0.4570, 0.0950]) tensor(6.0062, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5816,  5.7462,  2.6936,  0.0618,  0.1873,  0.5792,  0.1721],\n",
      "       grad_fn=<AddBackward0>) tensor([20.0000, 13.0000,  4.1000,  0.0930,  0.2370,  0.4910,  0.1710]) tensor(10.5899, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.4905,  5.3920,  2.0605,  0.0574,  0.1671,  0.5763,  0.1182],\n",
      "       grad_fn=<AddBackward0>) tensor([ 9.5000, 12.4000,  0.9000,  0.1420,  0.2730,  0.6200,  0.0400]) tensor(7.7782, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.6735, 2.5782, 0.7319, 0.0531, 0.1475, 0.4880, 0.0982],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 1.1000, 0.1000, 0.1310, 0.1480, 0.3940, 0.0300]) tensor(1.9978, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0859,  5.5344,  1.7505,  0.0689,  0.1864,  0.5656,  0.1122],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 7.0000, 0.7000, 0.1540, 0.2990, 0.5500, 0.0600]) tensor(5.9343, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.7675,  5.3943,  3.9533,  0.0349,  0.1388,  0.5834,  0.2165],\n",
      "       grad_fn=<AddBackward0>) tensor([13.6000,  4.1000,  6.9000,  0.0420,  0.0950,  0.5410,  0.3150]) tensor(2.9149, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6961, 2.9774, 2.6624, 0.0305, 0.1081, 0.5228, 0.1630],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 1.1000, 2.0000, 0.0130, 0.0870, 0.4910, 0.2700]) tensor(5.2032, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1723,  5.2784,  3.0227,  0.0517,  0.1620,  0.5788,  0.1730],\n",
      "       grad_fn=<AddBackward0>) tensor([16.8000,  6.5000,  4.4000,  0.0490,  0.1660,  0.5920,  0.2000]) tensor(1.4707, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.2490, 5.6654, 0.4253, 0.0876, 0.2110, 0.5618, 0.0410],\n",
      "       grad_fn=<AddBackward0>) tensor([0.8000, 1.4000, 0.1000, 0.0620, 0.1900, 0.3320, 0.0320]) tensor(8.5633, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9232, 5.3779, 1.4883, 0.0669, 0.1815, 0.5723, 0.0931],\n",
      "       grad_fn=<AddBackward0>) tensor([15.9000, 12.9000,  2.0000,  0.1170,  0.2700,  0.6820,  0.0950]) tensor(13.2269, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.3015, 1.3665, 0.7074, 0.0430, 0.1205, 0.4636, 0.0998],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 4.0000, 2.0000, 0.0310, 0.1630, 0.5290, 0.1440]) tensor(2.5151, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1986,  5.3092,  3.0349,  0.0544,  0.1670,  0.5660,  0.1804],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 5.1000, 1.2000, 0.0990, 0.1790, 0.4790, 0.1070]) tensor(4.2030, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.5247, 2.8937, 2.2489, 0.0357, 0.1207, 0.5129, 0.1572],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 1.1000, 0.4000, 0.0200, 0.0680, 0.5180, 0.0470]) tensor(3.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.8194, 1.9640, 1.6703, 0.0404, 0.1178, 0.4827, 0.1352],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 1.2000, 0.5000, 0.0080, 0.1300, 0.5580, 0.0790]) tensor(0.9221, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4613,  3.9234,  3.2083,  0.0286,  0.1227,  0.5512,  0.1987],\n",
      "       grad_fn=<AddBackward0>) tensor([14.7000,  3.2000,  5.5000,  0.0190,  0.0850,  0.5160,  0.2780]) tensor(1.0455, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2650, 5.3555, 1.7643, 0.0647, 0.1708, 0.5676, 0.0952],\n",
      "       grad_fn=<AddBackward0>) tensor([7.4000, 7.2000, 1.2000, 0.1250, 0.1750, 0.5660, 0.0640]) tensor(1.0291, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1821, 3.8966, 1.6922, 0.0541, 0.1562, 0.5293, 0.1294],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 3.6000, 1.8000, 0.0820, 0.1620, 0.6410, 0.1660]) tensor(3.0156, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.2730,  5.4483,  3.6580,  0.0415,  0.1442,  0.5850,  0.1932],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 3.5000, 2.9000, 0.0540, 0.0910, 0.5030, 0.1570]) tensor(5.7225, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6128,  7.2014,  1.9236,  0.0816,  0.2195,  0.6079,  0.1102],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 3.5000, 0.8000, 0.0550, 0.1710, 0.4430, 0.0770]) tensor(11.7778, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5362,  6.0061,  1.3031,  0.0751,  0.1963,  0.5815,  0.0794],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 6.6000, 0.5000, 0.1020, 0.2230, 0.5420, 0.0300]) tensor(2.7069, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.4131,  5.0182,  2.3676,  0.0551,  0.1607,  0.5577,  0.1413],\n",
      "       grad_fn=<AddBackward0>) tensor([8.5000, 9.3000, 1.6000, 0.1040, 0.1990, 0.5180, 0.0710]) tensor(3.9172, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3664, 3.5061, 1.4073, 0.0572, 0.1683, 0.5076, 0.1428],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 1.0000, 0.2000, 0.0810, 0.0840, 0.5360, 0.0620]) tensor(5.5340, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1880,  3.9734,  3.0227,  0.0351,  0.1337,  0.5438,  0.1952],\n",
      "       grad_fn=<AddBackward0>) tensor([14.8000,  2.6000,  3.8000,  0.0280,  0.0770,  0.5500,  0.2190]) tensor(0.7275, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9026, 2.8959, 1.5204, 0.0447, 0.1298, 0.4992, 0.1199],\n",
      "       grad_fn=<AddBackward0>) tensor([6.2000, 3.0000, 1.9000, 0.0510, 0.1080, 0.6380, 0.1270]) tensor(0.0376, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.9694,  8.5773,  2.0358,  0.0952,  0.2555,  0.6295,  0.1205],\n",
      "       grad_fn=<AddBackward0>) tensor([13.6000,  6.0000,  0.3000,  0.0950,  0.1860,  0.6040,  0.0200]) tensor(4.1090, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.3985,  4.5446,  3.5773,  0.0335,  0.1357,  0.5400,  0.2258],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000e+01, 2.3000e+00, 1.8000e+00, 1.0000e-02, 9.0000e-02, 5.2800e-01,\n",
      "        1.2000e-01]) tensor(3.9368, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0843, 1.9436, 2.4037, 0.0231, 0.0998, 0.4838, 0.1835],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 1.4000, 3.7000, 0.0110, 0.0660, 0.5310, 0.3050]) tensor(0.6434, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2490, 2.3154, 2.5553, 0.0260, 0.1084, 0.5057, 0.1845],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 3.2000, 5.0000, 0.0370, 0.0960, 0.4580, 0.3360]) tensor(0.9696, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 7.2641,  5.9708, -0.0211,  0.0980,  0.2316,  0.5634,  0.0265],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 2.1000, 0.3000, 0.0660, 0.1130, 0.5390, 0.0300]) tensor(3.4986, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5804, 4.0217, 2.3614, 0.0444, 0.1370, 0.5412, 0.1432],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 4.4000, 1.7000, 0.0380, 0.1480, 0.5190, 0.0990]) tensor(0.8928, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6326,  3.5401,  3.3650,  0.0274,  0.1215,  0.5157,  0.2283],\n",
      "       grad_fn=<AddBackward0>) tensor([13.9000,  2.0000,  4.0000,  0.0150,  0.0740,  0.5740,  0.2430]) tensor(0.4075, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5697,  4.5425,  2.8319,  0.0470,  0.1462,  0.5440,  0.1697],\n",
      "       grad_fn=<AddBackward0>) tensor([6.9000, 3.3000, 1.1000, 0.0340, 0.1340, 0.5020, 0.0850]) tensor(3.7655, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7049,  3.6007,  2.6129,  0.0326,  0.1223,  0.5363,  0.1670],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 1.9000, 0.4000, 0.0330, 0.0890, 0.5320, 0.0380]) tensor(2.8703, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3396,  4.9805,  1.4226,  0.0639,  0.1802,  0.5559,  0.1074],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 2.6000, 0.8000, 0.0450, 0.1170, 0.4590, 0.0800]) tensor(4.7889, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1871,  5.8391,  2.1270,  0.1019,  0.2043,  0.7003,  0.0432],\n",
      "       grad_fn=<AddBackward0>) tensor([0., 0., 0., 0., 0., 0., 0.]) tensor(23.4733, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.1047, 1.9860, 1.4569, 0.0428, 0.1190, 0.4873, 0.1175],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 1.7000, 0.5000, 0.0670, 0.1230, 0.4370, 0.0680]) tensor(0.9021, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.1626,  4.9422,  3.8028,  0.0287,  0.1277,  0.5752,  0.2143],\n",
      "       grad_fn=<AddBackward0>) tensor([12.0000,  2.4000,  5.3000,  0.0140,  0.0740,  0.5260,  0.2450]) tensor(3.7197, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.9119,  1.9672, -0.1021,  0.0588,  0.1459,  0.4732,  0.0532],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 4.3000, 0.5000, 0.1190, 0.2150, 0.5660, 0.0640]) tensor(1.4545, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.6106, 3.0922, 1.9966, 0.0379, 0.1235, 0.5153, 0.1386],\n",
      "       grad_fn=<AddBackward0>) tensor([8.2000, 2.5000, 3.4000, 0.0240, 0.0850, 0.5560, 0.2040]) tensor(0.3822, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.2841,  7.2701,  3.1354,  0.0680,  0.2033,  0.6153,  0.1731],\n",
      "       grad_fn=<AddBackward0>) tensor([15.9000,  7.1000,  1.3000,  0.0790,  0.1880,  0.5560,  0.0680]) tensor(1.2995, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.6263e+01, 3.8846e+00, 4.4777e+00, 1.1264e-02, 9.7114e-02, 5.6064e-01,\n",
      "        2.5532e-01], grad_fn=<AddBackward0>) tensor([2.1600e+01, 3.5000e+00, 3.4000e+00, 2.1000e-02, 9.2000e-02, 6.0300e-01,\n",
      "        1.7000e-01]) tensor(4.2570, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1408,  4.8214,  3.2122,  0.0412,  0.1391,  0.5611,  0.1806],\n",
      "       grad_fn=<AddBackward0>) tensor([11.7000,  2.1000,  2.9000,  0.0120,  0.0690,  0.5560,  0.1570]) tensor(1.3694, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8780,  4.4844,  2.3385,  0.0588,  0.1655,  0.5186,  0.1636],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 6.4000, 3.2000, 0.0500, 0.1830, 0.5240, 0.1500]) tensor(2.5628, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.5315e+01, 3.5427e+00, 4.2273e+00, 1.3278e-02, 9.8312e-02, 5.4183e-01,\n",
      "        2.5259e-01], grad_fn=<AddBackward0>) tensor([12.3000,  2.7000,  4.8000,  0.0230,  0.0910,  0.4970,  0.3060]) tensor(1.4479, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6036, 2.4678, 3.2107, 0.0139, 0.0837, 0.5175, 0.1951],\n",
      "       grad_fn=<AddBackward0>) tensor([10.5000,  3.6000,  1.7000,  0.0140,  0.0980,  0.5710,  0.0700]) tensor(0.6266, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.4784,  7.9996,  2.1690,  0.0870,  0.2393,  0.6214,  0.1299],\n",
      "       grad_fn=<AddBackward0>) tensor([20.1000, 10.5000,  2.0000,  0.0970,  0.2360,  0.5280,  0.1010]) tensor(1.8804, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4919, 2.4546, 1.0171, 0.0562, 0.1490, 0.4995, 0.1085],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 0.3000, 0.3000, 0.0000, 0.1110, 0.5950, 0.0830]) tensor(2.6859, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2358,  4.3079,  3.2649,  0.0378,  0.1404,  0.5437,  0.2093],\n",
      "       grad_fn=<AddBackward0>) tensor([15.5000,  3.0000,  3.8000,  0.0270,  0.0750,  0.5400,  0.1830]) tensor(0.5143, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.9080, 2.6022, 0.7981, 0.0541, 0.1473, 0.4927, 0.0992],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 2.2000, 0.2000, 0.1250, 0.1290, 0.5690, 0.0240]) tensor(0.4934, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.6603, 3.3245, 1.1532, 0.0587, 0.1639, 0.5049, 0.1220],\n",
      "       grad_fn=<AddBackward0>) tensor([6.9000, 2.1000, 0.6000, 0.0560, 0.0770, 0.5040, 0.0590]) tensor(0.3421, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.3823,  6.3395,  3.2233,  0.0602,  0.1930,  0.6005,  0.1972],\n",
      "       grad_fn=<AddBackward0>) tensor([28.1000, 11.0000,  5.9000,  0.0480,  0.2640,  0.6330,  0.2820]) tensor(17.6189, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3226, 3.4106, 1.2369, 0.0572, 0.1630, 0.5214, 0.1202],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 2.4000, 0.8000, 0.0520, 0.1990, 0.5080, 0.1140]) tensor(2.3715, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.8465e+01, 3.4666e+00, 5.0866e+00, 5.3115e-03, 9.3638e-02, 5.3558e-01,\n",
      "        3.1264e-01], grad_fn=<AddBackward0>) tensor([2.2600e+01, 2.5000e+00, 5.3000e+00, 1.5000e-02, 5.9000e-02, 5.7400e-01,\n",
      "        2.5400e-01]) tensor(2.5840, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.0192, 2.8510, 0.9713, 0.0557, 0.1529, 0.4968, 0.1096],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 4.2000, 0.8000, 0.0810, 0.2180, 0.4880, 0.0890]) tensor(0.7911, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1265,  5.1812,  1.6970,  0.0661,  0.1904,  0.5481,  0.1364],\n",
      "       grad_fn=<AddBackward0>) tensor([10.0000,  5.3000,  1.7000,  0.0870,  0.1790,  0.5080,  0.1180]) tensor(0.6484, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.9759,  6.6661,  2.4474,  0.0763,  0.2185,  0.5894,  0.1644],\n",
      "       grad_fn=<AddBackward0>) tensor([11.7000,  5.7000,  1.3000,  0.1070,  0.1860,  0.5030,  0.1180]) tensor(4.2995, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.2090, 1.8336, 0.8526, 0.0542, 0.1506, 0.4657, 0.1296],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 1.4000, 0.2000, 0.1070, 0.1110, 0.7260, 0.0530]) tensor(0.1227, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3985,  4.7084,  1.7922,  0.0625,  0.1729,  0.5449,  0.1267],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 5.5000, 0.6000, 0.1230, 0.1590, 0.5480, 0.0390]) tensor(2.4654, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.3553,  4.7423,  3.4266,  0.0370,  0.1431,  0.5526,  0.2141],\n",
      "       grad_fn=<AddBackward0>) tensor([11.2000,  3.2000,  1.3000,  0.0190,  0.1180,  0.5500,  0.0730]) tensor(3.4555, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 4.7360,  4.5337, -0.4814,  0.0901,  0.2158,  0.5162,  0.0311],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 0.8000, 0.1000, 0.0750, 0.1000, 0.4010, 0.0410]) tensor(3.8298, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9057, 1.7771, 3.2290, 0.0124, 0.0829, 0.4945, 0.2195],\n",
      "       grad_fn=<AddBackward0>) tensor([12.7000,  4.0000,  3.2000,  0.0240,  0.1090,  0.5380,  0.1350]) tensor(1.8229, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.2243, 3.0759, 0.6191, 0.0679, 0.1740, 0.4845, 0.0996],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 4.8000, 1.8000, 0.0560, 0.1760, 0.4600, 0.1190]) tensor(1.4303, grad_fn=<MseLossBackward0>)\n",
      "tensor([21.9535,  6.6666,  4.4300,  0.0417,  0.1686,  0.6175,  0.2487],\n",
      "       grad_fn=<AddBackward0>) tensor([25.7000,  5.8000,  3.9000,  0.0520,  0.1200,  0.5430,  0.1950]) tensor(2.1542, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4794,  5.2785,  2.7301,  0.0565,  0.1687,  0.5702,  0.1622],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 4.4000, 2.3000, 0.0640, 0.1600, 0.5200, 0.1680]) tensor(2.8769, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4639,  6.0095,  2.8472,  0.0538,  0.1728,  0.5909,  0.1640],\n",
      "       grad_fn=<AddBackward0>) tensor([11.8000,  4.2000,  1.4000,  0.0560,  0.1290,  0.5490,  0.0920]) tensor(2.6859, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5615,  5.7029,  2.4072,  0.0581,  0.1737,  0.5801,  0.1420],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 3.3000, 3.1000, 0.0370, 0.1010, 0.4910, 0.1610]) tensor(5.6376, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.1517e+01, 2.0597e+00, 3.7911e+00, 4.1958e-03, 7.3330e-02, 5.0657e-01,\n",
      "        2.3974e-01], grad_fn=<AddBackward0>) tensor([6.3000e+00, 9.0000e-01, 2.4000e+00, 6.0000e-03, 5.8000e-02, 5.8000e-01,\n",
      "        2.6200e-01]) tensor(4.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4717,  4.8118,  4.0519,  0.0279,  0.1258,  0.5714,  0.2290],\n",
      "       grad_fn=<AddBackward0>) tensor([17.9000,  3.1000,  7.7000,  0.0200,  0.0770,  0.6020,  0.3550]) tensor(2.6141, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4358,  4.7311,  2.1073,  0.0504,  0.1516,  0.5543,  0.1281],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 2.7000, 0.8000, 0.0370, 0.1240, 0.4860, 0.0610]) tensor(6.2132, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3465, 5.6237, 1.4924, 0.0726, 0.1858, 0.5592, 0.0896],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 4.0000, 1.5000, 0.1050, 0.1770, 0.4690, 0.1350]) tensor(6.6892, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.0250, 2.8371, 1.2303, 0.0513, 0.1384, 0.5043, 0.1020],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 1.6000, 0.3000, 0.0700, 0.1140, 0.3750, 0.0390]) tensor(2.3276, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.6894, 1.6865, 1.7991, 0.0285, 0.1145, 0.4817, 0.1691],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 0.7000, 0.7000, 0.0220, 0.0670, 0.5350, 0.1350]) tensor(2.4736, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0588, 4.7353, 1.2846, 0.0629, 0.1685, 0.5496, 0.0870],\n",
      "       grad_fn=<AddBackward0>) tensor([6.8000, 5.3000, 0.9000, 0.1080, 0.1780, 0.5630, 0.0610]) tensor(0.2935, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2205,  5.1557,  2.0475,  0.0563,  0.1610,  0.5671,  0.1148],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 7.2000, 0.8000, 0.0880, 0.1680, 0.5210, 0.0430]) tensor(1.5898, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5812,  4.6610,  2.1647,  0.0510,  0.1525,  0.5545,  0.1324],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 3.6000, 0.4000, 0.1350, 0.1550, 0.5300, 0.0380]) tensor(6.4252, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5145, 2.8465, 1.6624, 0.0456, 0.1379, 0.5093, 0.1360],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 2.4000, 0.3000, 0.0890, 0.1670, 0.5290, 0.0480]) tensor(2.3738, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.1638e+01, 2.0065e+00, 3.7692e+00, 4.7446e-03, 7.2928e-02, 5.1416e-01,\n",
      "        2.3651e-01], grad_fn=<AddBackward0>) tensor([5.0000, 1.4000, 1.8000, 0.0250, 0.1070, 0.4880, 0.2300]) tensor(6.9009, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1584, 3.6284, 1.4293, 0.0550, 0.1585, 0.5113, 0.1271],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 1.5000, 0.6000, 0.0220, 0.1350, 0.6130, 0.1070]) tensor(3.2175, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3184, 2.4374, 2.1339, 0.0351, 0.1214, 0.4927, 0.1691],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 1.8000, 0.9000, 0.0630, 0.1520, 0.4890, 0.1510]) tensor(4.0184, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8409, 2.1506, 1.5662, 0.0406, 0.1287, 0.4842, 0.1470],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000e+00, 1.7000e+00, 1.1000e+00, 3.0000e-03, 8.5000e-02, 4.2300e-01,\n",
      "        9.2000e-02]) tensor(0.1625, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.1539,  7.2564,  2.7770,  0.0763,  0.2195,  0.6113,  0.1660],\n",
      "       grad_fn=<AddBackward0>) tensor([20.6000, 14.5000,  1.9000,  0.1130,  0.3180,  0.5690,  0.0980]) tensor(8.4629, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.0815,  5.2042,  3.8087,  0.0406,  0.1513,  0.5759,  0.2238],\n",
      "       grad_fn=<AddBackward0>) tensor([13.0000,  2.8000,  2.9000,  0.0260,  0.0930,  0.5110,  0.1860]) tensor(3.3248, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.6560, 1.5381, 0.3156, 0.0516, 0.1334, 0.4701, 0.0777],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.8000, 0.3000, 0.0410, 0.0990, 0.5770, 0.0340]) tensor(0.3980, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.8346, 2.8018, 1.0100, 0.0510, 0.1455, 0.5063, 0.1043],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 2.0000, 0.4000, 0.0810, 0.1450, 0.4840, 0.0760]) tensor(1.2932, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.6556, 0.2487, 1.3955, 0.0219, 0.0850, 0.4461, 0.1440],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 2.1000, 3.1000, 0.0140, 0.0970, 0.5270, 0.1980]) tensor(7.0244, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1039,  5.4833,  2.5390,  0.0582,  0.1760,  0.5635,  0.1639],\n",
      "       grad_fn=<AddBackward0>) tensor([13.8000,  7.2000,  4.6000,  0.0410,  0.1790,  0.5640,  0.2170]) tensor(1.0415, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3283,  4.3481,  1.9727,  0.0538,  0.1585,  0.5354,  0.1394],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 3.8000, 0.6000, 0.0940, 0.1630, 0.5380, 0.0540]) tensor(4.0704, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2191,  2.1117,  3.2746,  0.0113,  0.0869,  0.5071,  0.2221],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 1.1000, 1.7000, 0.0260, 0.0850, 0.5350, 0.2490]) tensor(5.6763, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3610,  3.1497,  3.2981,  0.0239,  0.1063,  0.5238,  0.2050],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 1.6000, 1.2000, 0.0180, 0.1060, 0.5090, 0.1280]) tensor(4.6320, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4479, 4.8975, 1.1462, 0.0727, 0.1928, 0.5398, 0.1030],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 2.7000, 0.2000, 0.1350, 0.1820, 0.6070, 0.0300]) tensor(4.9054, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2795,  5.2438,  1.7315,  0.0597,  0.1684,  0.5712,  0.1023],\n",
      "       grad_fn=<AddBackward0>) tensor([6.8000, 4.9000, 2.0000, 0.0820, 0.1280, 0.5120, 0.1150]) tensor(1.7576, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3328,  3.2995,  2.7945,  0.0309,  0.1217,  0.5241,  0.1887],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 2.1000, 4.5000, 0.0170, 0.0840, 0.4400, 0.2820]) tensor(2.4065, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.8796, 2.9581, 0.9140, 0.0565, 0.1545, 0.5007, 0.1033],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 1.9000, 0.2000, 0.1230, 0.1910, 0.6030, 0.0400]) tensor(1.7722, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.9470, 4.1686, 0.0825, 0.0773, 0.1894, 0.5263, 0.0453],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 5.7000, 0.9000, 0.1210, 0.2240, 0.6780, 0.0770]) tensor(1.0966, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4311,  4.0770,  1.8804,  0.0580,  0.1675,  0.5283,  0.1476],\n",
      "       grad_fn=<AddBackward0>) tensor([13.5000,  7.2000,  1.2000,  0.1030,  0.1730,  0.5320,  0.0710]) tensor(2.8060, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5581, 1.8683, 1.4060, 0.0423, 0.1321, 0.4819, 0.1458],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 1.5000, 1.0000, 0.0000, 0.1430, 0.3330, 0.2000]) tensor(1.8554, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.3547,  8.6569,  2.5230,  0.0944,  0.2557,  0.6340,  0.1454],\n",
      "       grad_fn=<AddBackward0>) tensor([21.8000, 10.8000,  2.1000,  0.0700,  0.2810,  0.5320,  0.1220]) tensor(1.5376, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.7950, 0.9333, 1.8101, 0.0251, 0.1049, 0.4550, 0.1836],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 2.1000, 1.1000, 0.0730, 0.0980, 0.4260, 0.1430]) tensor(0.5865, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9936,  6.4947,  1.4935,  0.0834,  0.2158,  0.5805,  0.0970],\n",
      "       grad_fn=<AddBackward0>) tensor([11.8000,  4.5000,  1.9000,  0.0740,  0.0930,  0.5520,  0.1190]) tensor(0.5997, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.4134, 1.6736, 1.1805, 0.0385, 0.1194, 0.4825, 0.1204],\n",
      "       grad_fn=<AddBackward0>) tensor([6.9000, 3.6000, 0.5000, 0.0730, 0.1480, 0.6630, 0.0500]) tensor(1.4852, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.6036, 3.0937, 2.0042, 0.0369, 0.1202, 0.5178, 0.1358],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 3.1000, 1.0000, 0.0630, 0.1010, 0.5310, 0.0750]) tensor(0.6625, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5837,  5.3530,  2.4896,  0.0521,  0.1661,  0.5720,  0.1539],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 3.8000, 1.5000, 0.0370, 0.1080, 0.4560, 0.0920]) tensor(2.7545, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.9088,  5.3184,  2.6660,  0.0468,  0.1571,  0.5826,  0.1560],\n",
      "       grad_fn=<AddBackward0>) tensor([7.4000, 3.1000, 0.8000, 0.0450, 0.1190, 0.5380, 0.0640]) tensor(7.2542, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1862,  7.2129,  1.7983,  0.0879,  0.2297,  0.5990,  0.1097],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 3.8000, 1.0000, 0.0710, 0.2210, 0.4690, 0.1060]) tensor(10.1972, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.0261, 0.3199, 0.8510, 0.0351, 0.1062, 0.4375, 0.1278],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 0.6000, 0.6000, 0.0370, 0.0700, 0.4150, 0.1580]) tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4649,  5.2137,  3.3630,  0.0475,  0.1577,  0.5701,  0.1988],\n",
      "       grad_fn=<AddBackward0>) tensor([14.9000,  4.6000,  5.4000,  0.0250,  0.1210,  0.5710,  0.2750]) tensor(0.6932, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1180,  3.8678,  2.8334,  0.0360,  0.1313,  0.5431,  0.1807],\n",
      "       grad_fn=<AddBackward0>) tensor([10.2000,  3.6000,  2.8000,  0.0550,  0.0940,  0.5350,  0.1620]) tensor(0.5363, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.5042,  5.4390,  4.0053,  0.0359,  0.1381,  0.5855,  0.2139],\n",
      "       grad_fn=<AddBackward0>) tensor([13.3000,  7.3000,  8.4000,  0.0330,  0.1960,  0.5260,  0.3690]) tensor(4.7250, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5213,  4.7147,  2.6818,  0.0435,  0.1404,  0.5625,  0.1505],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 4.7000, 1.7000, 0.0670, 0.1180, 0.5380, 0.1030]) tensor(0.9758, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.8899, 5.2236, 1.4988, 0.0661, 0.1801, 0.5626, 0.0995],\n",
      "       grad_fn=<AddBackward0>) tensor([6.7000, 3.8000, 1.1000, 0.1030, 0.1560, 0.5680, 0.1090]) tensor(1.7661, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9093,  5.5930,  1.6923,  0.0712,  0.1950,  0.5663,  0.1180],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 3.4000, 0.3000, 0.1130, 0.2370, 0.5030, 0.0360]) tensor(9.6782, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7468,  4.9597,  1.9937,  0.0581,  0.1652,  0.5594,  0.1237],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 3.4000, 1.4000, 0.0220, 0.1750, 0.5400, 0.1220]) tensor(5.1160, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5154,  4.2279,  2.8503,  0.0346,  0.1315,  0.5503,  0.1766],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 1.9000, 2.3000, 0.0200, 0.1050, 0.4790, 0.2360]) tensor(7.6505, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.4475,  4.9759,  1.5095,  0.0836,  0.2063,  0.5913,  0.1037],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 1.5000, 0.5000, 0.2860, 0.1110, 0.2500, 0.0830]) tensor(17.4884, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.2454, 1.0617, 1.8622, 0.0262, 0.1027, 0.4628, 0.1734],\n",
      "       grad_fn=<AddBackward0>) tensor([8.7000, 2.0000, 2.3000, 0.0180, 0.0920, 0.5860, 0.2060]) tensor(1.0162, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4264, 1.8133, 2.4480, 0.0206, 0.0976, 0.4895, 0.1867],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0400e+01, 1.6000e+00, 3.3000e+00, 1.0000e-02, 8.3000e-02, 5.9200e-01,\n",
      "        2.9900e-01]) tensor(0.6700, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4644,  4.8389,  2.3019,  0.0498,  0.1488,  0.5654,  0.1274],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 8.6000, 0.8000, 0.1350, 0.3070, 0.6320, 0.0640]) tensor(5.4570, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8005,  5.3813,  1.9479,  0.0624,  0.1841,  0.5652,  0.1365],\n",
      "       grad_fn=<AddBackward0>) tensor([14.6000,  7.0000,  1.3000,  0.0520,  0.2090,  0.5420,  0.0770]) tensor(0.8975, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3485, 4.0121, 0.9385, 0.0609, 0.1584, 0.5266, 0.0729],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([4.6000, 5.9000, 0.9000, 0.1350, 0.1930, 0.4730, 0.0740]) tensor(0.5908, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5632,  6.5087,  1.4610,  0.0833,  0.2179,  0.5841,  0.0987],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 6.7000, 0.5000, 0.1760, 0.3390, 0.6020, 0.0580]) tensor(3.6598, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.1620, 1.7346, 0.3225, 0.0575, 0.1562, 0.4599, 0.1085],\n",
      "       grad_fn=<AddBackward0>) tensor([8.8000, 3.5000, 0.3000, 0.0220, 0.1400, 0.4580, 0.0150]) tensor(3.5198, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.3560e+01, 2.8261e+00, 4.3025e+00, 3.8349e-03, 7.6126e-02, 5.2751e-01,\n",
      "        2.5328e-01], grad_fn=<AddBackward0>) tensor([7.2000, 1.4000, 1.8000, 0.0130, 0.0920, 0.5520, 0.2030]) tensor(6.9635, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9290,  6.3380,  1.8246,  0.0759,  0.2046,  0.5880,  0.1116],\n",
      "       grad_fn=<AddBackward0>) tensor([12.1000,  9.1000,  1.4000,  0.1230,  0.2130,  0.5610,  0.0770]) tensor(1.2144, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.0363, 0.9135, 0.3989, 0.0425, 0.1169, 0.4489, 0.0912],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 1.0000, 1.0000, 0.0000, 0.2220, 0.5000, 0.3330]) tensor(0.0634, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9061e+00, 1.0171e+00, 2.9662e+00, 8.0477e-03, 7.6282e-02, 4.6291e-01,\n",
      "        2.2799e-01], grad_fn=<AddBackward0>) tensor([4.5000, 1.0000, 2.2000, 0.0210, 0.0730, 0.4790, 0.3290]) tensor(2.8588, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.9296e+01, 4.0827e+00, 5.0060e+00, 1.2556e-02, 1.0814e-01, 5.4953e-01,\n",
      "        3.0360e-01], grad_fn=<AddBackward0>) tensor([1.8900e+01, 2.4000e+00, 4.1000e+00, 1.4000e-02, 6.3000e-02, 5.0900e-01,\n",
      "        2.0000e-01]) tensor(0.5463, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9441, 3.3167, 1.8836, 0.0464, 0.1432, 0.5104, 0.1491],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.6000, 0.5000, 0.0820, 0.0940, 0.4040, 0.0970]) tensor(7.3887, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9704, 5.8919, 0.5816, 0.0872, 0.2116, 0.5693, 0.0453],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 8.9000, 1.9000, 0.1320, 0.1870, 0.5960, 0.0880]) tensor(1.5613, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9795, 3.9074, 1.2768, 0.0566, 0.1549, 0.5349, 0.0943],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 2.3000, 0.7000, 0.0500, 0.1540, 0.5630, 0.0970]) tensor(1.8610, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.2751, 2.0101, 0.1553, 0.0617, 0.1623, 0.4523, 0.0971],\n",
      "       grad_fn=<AddBackward0>) tensor([0.8000, 1.8000, 0.4000, 0.1250, 0.3750, 0.3330, 0.1670]) tensor(0.8998, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.9887, 0.3475, 1.0089, 0.0277, 0.0936, 0.4387, 0.1292],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 2.4000, 3.1000, 0.0310, 0.1260, 0.4680, 0.2570]) tensor(1.9277, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9406,  4.2421,  3.1361,  0.0318,  0.1270,  0.5494,  0.1894],\n",
      "       grad_fn=<AddBackward0>) tensor([13.8000,  4.3000,  3.4000,  0.0220,  0.1220,  0.5340,  0.1670]) tensor(0.1161, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.1851, 1.7415, 0.5609, 0.0471, 0.1225, 0.4786, 0.0728],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 5.0000, 0.9000, 0.1270, 0.2350, 0.4870, 0.1000]) tensor(1.6557, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.8745, 3.5973, 0.9132, 0.0603, 0.1584, 0.5143, 0.0868],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 0.4000, 0.2000, 0.0300, 0.0430, 0.5150, 0.0450]) tensor(4.3955, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8774,  3.6435,  2.7504,  0.0416,  0.1402,  0.5369,  0.1841],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7500e+01, 3.7000e+00, 2.6000e+00, 1.1000e-02, 1.0800e-01, 5.4200e-01,\n",
      "        1.1800e-01]) tensor(4.5208, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.7811,  5.7245,  3.4893,  0.0466,  0.1569,  0.5849,  0.1914],\n",
      "       grad_fn=<AddBackward0>) tensor([12.5000,  6.9000,  1.4000,  0.0900,  0.1940,  0.5510,  0.0840]) tensor(2.3612, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4102, 2.4368, 1.8529, 0.0384, 0.1246, 0.4938, 0.1520],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 1.7000, 0.5000, 0.0680, 0.0900, 0.4330, 0.0590]) tensor(3.1196, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0927,  3.6161,  3.6689,  0.0274,  0.1240,  0.5231,  0.2450],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 2.5000, 2.8000, 0.0300, 0.1070, 0.4800, 0.2330]) tensor(7.6769, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2955, 3.0547, 1.8970, 0.0488, 0.1493, 0.5135, 0.1565],\n",
      "       grad_fn=<AddBackward0>) tensor([17.8000,  4.2000,  0.8000,  0.0270,  0.1230,  0.5430,  0.0450]) tensor(10.6938, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.0627e+01, 4.7500e+00, 5.4369e+00, 1.1463e-02, 1.0615e-01, 5.8333e-01,\n",
      "        3.0098e-01], grad_fn=<AddBackward0>) tensor([1.9500e+01, 4.2000e+00, 1.0000e+01, 1.8000e-02, 1.1900e-01, 5.7500e-01,\n",
      "        4.9300e-01]) tensor(3.2045, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.3036, 3.4329, 1.1481, 0.0599, 0.1583, 0.5160, 0.1005],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.4000, 0.4000, 0.0350, 0.1240, 0.4010, 0.0800]) tensor(3.1969, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.8520, 2.3257, 0.1213, 0.0628, 0.1593, 0.4777, 0.0739],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 0.9000, 0.3000, 0.0560, 0.1150, 0.4020, 0.0620]) tensor(0.5973, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6718,  5.1688,  2.5331,  0.0633,  0.1890,  0.5608,  0.1779],\n",
      "       grad_fn=<AddBackward0>) tensor([15.5000,  4.7000,  3.3000,  0.0320,  0.1280,  0.4760,  0.1840]) tensor(0.2151, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.3767,  6.9650,  3.4119,  0.0576,  0.1904,  0.6168,  0.1932],\n",
      "       grad_fn=<AddBackward0>) tensor([14.8000,  6.4000,  1.3000,  0.0620,  0.1900,  0.5050,  0.0760]) tensor(3.6789, grad_fn=<MseLossBackward0>)\n",
      "tensor([-4.1743,  0.9547, -1.3044,  0.0658,  0.1475,  0.4208,  0.0110],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 2.0000, 0.0000, 0.1110, 0.2500, 0.0000, 0.0000]) tensor(2.9155, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9429,  2.9623,  2.1170,  0.0411,  0.1447,  0.5017,  0.1851],\n",
      "       grad_fn=<AddBackward0>) tensor([5.4000, 1.0000, 0.8000, 0.0270, 0.0770, 0.4110, 0.1600]) tensor(5.1890, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2030,  2.5601,  2.3578,  0.0341,  0.1267,  0.4937,  0.1920],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 0.8000, 0.5000, 0.0140, 0.0680, 0.4880, 0.0850]) tensor(5.7485, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9825,  4.0229,  2.9925,  0.0346,  0.1327,  0.5444,  0.1917],\n",
      "       grad_fn=<AddBackward0>) tensor([14.2000,  2.1000,  0.9000,  0.0190,  0.0680,  0.5120,  0.0570]) tensor(1.3689, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4922, 3.8375, 1.9911, 0.0505, 0.1505, 0.5203, 0.1464],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 2.2000, 0.8000, 0.0430, 0.1470, 0.4640, 0.1070]) tensor(6.9843, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.6636,  6.7436,  3.3920,  0.0600,  0.1866,  0.6012,  0.1875],\n",
      "       grad_fn=<AddBackward0>) tensor([11.5000,  2.9000,  1.4000,  0.0220,  0.1120,  0.5390,  0.1010]) tensor(8.1071, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5047,  5.7633,  3.0241,  0.0560,  0.1731,  0.5799,  0.1785],\n",
      "       grad_fn=<AddBackward0>) tensor([14.0000,  9.5000,  7.4000,  0.0560,  0.2220,  0.5870,  0.2770]) tensor(5.0554, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2676, 3.5469, 1.4839, 0.0525, 0.1536, 0.5279, 0.1209],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 1.7000, 0.8000, 0.0180, 0.1240, 0.4480, 0.0940]) tensor(2.6923, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4237, 2.7797, 1.8989, 0.0396, 0.1315, 0.4960, 0.1575],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 2.0000, 3.2000, 0.0280, 0.0870, 0.4330, 0.2900]) tensor(1.5532, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4797, 3.0833, 0.5644, 0.0637, 0.1667, 0.4977, 0.0900],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000e+00, 1.1000e+00, 9.0000e-01, 5.0000e-03, 6.6000e-02, 5.2500e-01,\n",
      "        9.0000e-02]) tensor(0.6007, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4490,  2.6724,  3.0562,  0.0232,  0.1032,  0.5068,  0.2034],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 2.8000, 4.8000, 0.0220, 0.0960, 0.5260, 0.3090]) tensor(1.0382, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8187,  4.8946,  1.8950,  0.0634,  0.1764,  0.5556,  0.1271],\n",
      "       grad_fn=<AddBackward0>) tensor([9.0000, 5.2000, 1.2000, 0.0550, 0.1810, 0.5470, 0.0810]) tensor(0.5552, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0558, 3.1951, 1.6873, 0.0514, 0.1541, 0.5033, 0.1513],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 2.3000, 0.8000, 0.0260, 0.0870, 0.4360, 0.0600]) tensor(0.3888, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.7785, 1.9787, 0.7017, 0.0537, 0.1420, 0.4738, 0.1021],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000, 0.5000, 0.8000, 0.0000, 0.0800, 0.3620, 0.1470]) tensor(1.3417, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5198,  5.7026,  1.9324,  0.0682,  0.1855,  0.5672,  0.1186],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 2.7000, 0.6000, 0.0730, 0.1410, 0.5000, 0.0690]) tensor(8.3834, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7638, 3.7606, 1.2358, 0.0570, 0.1665, 0.5259, 0.1186],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 1.9000, 0.2000, 0.0770, 0.1710, 0.4510, 0.0380]) tensor(4.4589, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1634,  1.4402,  3.3291,  0.0194,  0.1027,  0.4646,  0.2589],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 0.7000, 0.3000, 0.0000, 0.1540, 0.4620, 0.0630]) tensor(8.7255, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3099,  4.0336,  2.2661,  0.0483,  0.1536,  0.5247,  0.1669],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 0.8000, 1.3000, 0.0290, 0.0690, 0.3890, 0.2300]) tensor(11.7351, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7383, 3.0760, 2.1176, 0.0443, 0.1422, 0.4997, 0.1723],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.8000, 0.4000, 0.0550, 0.1290, 0.4120, 0.0660]) tensor(6.5785, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.8657,  4.7307,  3.3598,  0.0395,  0.1438,  0.5596,  0.2036],\n",
      "       grad_fn=<AddBackward0>) tensor([9.5000, 2.1000, 1.9000, 0.0110, 0.0960, 0.5120, 0.1370]) tensor(5.4074, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.3392, 2.1482, 0.1063, 0.0594, 0.1455, 0.4765, 0.0583],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 0.7000, 1.0000, 0.0060, 0.0690, 0.3090, 0.1390]) tensor(0.4475, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4079,  4.6780,  3.3682,  0.0406,  0.1481,  0.5609,  0.2094],\n",
      "       grad_fn=<AddBackward0>) tensor([13.9000,  4.2000,  2.2000,  0.0500,  0.1100,  0.4860,  0.1410]) tensor(0.5541, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.8519,  5.5192,  3.9232,  0.0384,  0.1496,  0.5912,  0.2221],\n",
      "       grad_fn=<AddBackward0>) tensor([20.1000,  3.3000,  4.2000,  0.0400,  0.0610,  0.5480,  0.2030]) tensor(1.4379, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.9963, 0.4150, 1.3861, 0.0278, 0.0891, 0.4503, 0.1328],\n",
      "       grad_fn=<AddBackward0>) tensor([0.8000, 0.0000, 0.8000, 0.0000, 0.0000, 0.3880, 0.3130]) tensor(0.2846, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3162, 4.9122, 0.7284, 0.0742, 0.1891, 0.5369, 0.0715],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 3.9000, 1.0000, 0.0820, 0.1520, 0.5870, 0.0780]) tensor(0.4049, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.8308,  6.5824,  1.8151,  0.0808,  0.2156,  0.5911,  0.1158],\n",
      "       grad_fn=<AddBackward0>) tensor([9.5000, 6.1000, 1.1000, 0.0920, 0.2050, 0.5120, 0.0790]) tensor(2.7869, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5505,  3.5387,  2.7226,  0.0305,  0.1207,  0.5417,  0.1747],\n",
      "       grad_fn=<AddBackward0>) tensor([10.2000,  6.9000,  7.2000,  0.0360,  0.1530,  0.4440,  0.2880]) tensor(4.7419, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6920,  4.4157,  2.6632,  0.0484,  0.1504,  0.5522,  0.1633],\n",
      "       grad_fn=<AddBackward0>) tensor([15.0000,  7.4000,  3.7000,  0.0500,  0.1930,  0.5730,  0.1770]) tensor(2.9895, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7706,  4.8790,  2.1129,  0.0561,  0.1629,  0.5572,  0.1309],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 1.6000, 0.9000, 0.0160, 0.1150, 0.5350, 0.0990]) tensor(7.3641, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.5143,  6.4116,  3.6086,  0.0537,  0.1739,  0.6048,  0.1942],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8900e+01, 5.4000e+00, 3.3000e+00, 1.6000e-02, 1.6100e-01, 6.2000e-01,\n",
      "        1.5200e-01]) tensor(0.4346, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6606,  3.7327,  2.0957,  0.0484,  0.1519,  0.5262,  0.1585],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 3.4000, 2.2000, 0.0260, 0.1770, 0.4880, 0.2060]) tensor(1.8291, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.9002,  5.5285,  2.7135,  0.0549,  0.1688,  0.5726,  0.1610],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 2.5000, 2.3000, 0.0120, 0.0920, 0.5170, 0.1540]) tensor(3.9779, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1164, 1.9700, 2.2720, 0.0309, 0.1033, 0.4830, 0.1626],\n",
      "       grad_fn=<AddBackward0>) tensor([6.7000, 2.1000, 1.2000, 0.0140, 0.0910, 0.6200, 0.0920]) tensor(0.2187, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6952, 4.0953, 0.1396, 0.0763, 0.1913, 0.5222, 0.0572],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 1.9000, 0.2000, 0.1120, 0.1790, 0.7460, 0.0390]) tensor(1.2651, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8361,  5.1032,  2.6979,  0.0522,  0.1590,  0.5667,  0.1577],\n",
      "       grad_fn=<AddBackward0>) tensor([15.4000, 10.2000,  2.2000,  0.0690,  0.2390,  0.5700,  0.0870]) tensor(4.6873, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.9052,  5.9250,  3.3758,  0.0592,  0.1834,  0.5610,  0.2123],\n",
      "       grad_fn=<AddBackward0>) tensor([16.1000,  9.6000,  3.1000,  0.0620,  0.2230,  0.5550,  0.1440]) tensor(2.0337, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3893, 1.5926, 2.4020, 0.0264, 0.1052, 0.4891, 0.1885],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 1.4000, 0.8000, 0.0110, 0.1440, 0.5890, 0.1300]) tensor(2.7630, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6651,  7.4293,  0.8593,  0.0970,  0.2339,  0.6064,  0.0409],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 5.0000, 0.5000, 0.1440, 0.2030, 0.5360, 0.0470]) tensor(9.4765, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6420,  3.7195,  2.3630,  0.0479,  0.1535,  0.5207,  0.1801],\n",
      "       grad_fn=<AddBackward0>) tensor([14.6000,  6.6000,  1.3000,  0.0880,  0.1810,  0.5220,  0.0730]) tensor(2.5986, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2120,  3.5741,  3.3727,  0.0245,  0.1097,  0.5350,  0.2031],\n",
      "       grad_fn=<AddBackward0>) tensor([11.9000,  6.0000, 11.7000,  0.0330,  0.1500,  0.5060,  0.4730]) tensor(10.7716, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8115,  3.0456,  3.3997,  0.0219,  0.1092,  0.5291,  0.2207],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000e+01, 2.4000e+00, 5.7000e+00, 5.0000e-03, 8.4000e-02, 5.4200e-01,\n",
      "        3.0400e-01]) tensor(2.2690, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.7873, 0.5346, 1.9009, 0.0220, 0.0907, 0.4421, 0.1788],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 1.4000, 2.8000, 0.0150, 0.0860, 0.4790, 0.2970]) tensor(0.2265, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 7.7011,  7.3739, -1.0749,  0.1341,  0.2997,  0.5719, -0.0150],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 4.1000, 0.7000, 0.1290, 0.2740, 0.4600, 0.0870]) tensor(4.2716, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.8979, 3.1158, 2.0774, 0.0381, 0.1344, 0.5225, 0.1601],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 1.6000, 1.0000, 0.0440, 0.1180, 0.5340, 0.1850]) tensor(3.9212, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.1582, 1.8384, 0.9204, 0.0489, 0.1337, 0.4793, 0.1096],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 1.5000, 1.0000, 0.0200, 0.0940, 0.5060, 0.0820]) tensor(0.2750, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.6564,  5.1959,  4.5776,  0.0261,  0.1383,  0.5802,  0.2752],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2500e+01, 3.7000e+00, 5.9000e+00, 1.8000e-02, 1.0600e-01, 5.5300e-01,\n",
      "        3.1500e-01]) tensor(1.0556, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7421,  4.5945,  3.2937,  0.0314,  0.1256,  0.5710,  0.1837],\n",
      "       grad_fn=<AddBackward0>) tensor([13.4000,  6.4000,  1.5000,  0.0500,  0.1720,  0.6280,  0.0660]) tensor(0.9448, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.0371e+01, 5.1075e+00, 4.9543e+00, 1.9430e-02, 1.2181e-01, 5.8888e-01,\n",
      "        2.7855e-01], grad_fn=<AddBackward0>) tensor([2.4000e+01, 4.3000e+00, 8.5000e+00, 1.8000e-02, 1.0800e-01, 6.1000e-01,\n",
      "        3.7200e-01]) tensor(3.7723, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9828, 4.2679, 2.1642, 0.0536, 0.1544, 0.5334, 0.1436],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 7.1000, 8.9000, 0.0280, 0.1840, 0.5300, 0.3450]) tensor(8.9044, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.2869, 1.9534, 1.3297, 0.0473, 0.1293, 0.4749, 0.1250],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 3.8000, 0.6000, 0.1490, 0.2000, 0.4060, 0.0780]) tensor(1.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4965, 4.5764, 0.6827, 0.0734, 0.1895, 0.5412, 0.0740],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 2.7000, 0.3000, 0.1020, 0.1600, 0.5230, 0.0450]) tensor(2.6932, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9835, 4.6916, 1.3481, 0.0639, 0.1744, 0.5481, 0.1000],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 2.8000, 0.6000, 0.0770, 0.1650, 0.5460, 0.0650]) tensor(3.4630, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.1610,  6.1933,  2.5318,  0.0578,  0.1808,  0.5898,  0.1507],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 4.2000, 0.7000, 0.0580, 0.1640, 0.5290, 0.0510]) tensor(6.1251, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.9971,  6.7259,  1.7236,  0.0813,  0.2182,  0.5892,  0.1132],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 3.0000, 0.3000, 0.0770, 0.1860, 0.4250, 0.0430]) tensor(15.9893, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.8033, 4.2553, 0.7082, 0.0709, 0.1876, 0.5378, 0.0840],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 2.3000, 0.3000, 0.0610, 0.2080, 0.5070, 0.0490]) tensor(2.4252, grad_fn=<MseLossBackward0>)\n",
      "tensor([25.2291,  7.0510,  5.2666,  0.0403,  0.1728,  0.6275,  0.2945],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0400e+01, 5.4000e+00, 8.8000e+00, 1.7000e-02, 1.3700e-01, 6.1900e-01,\n",
      "        4.4900e-01]) tensor(5.9964, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2807,  5.4729,  2.8913,  0.0522,  0.1610,  0.5765,  0.1598],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 6.1000, 0.6000, 0.1290, 0.1970, 0.4710, 0.0500]) tensor(5.4207, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.8504, 2.1076, 0.8958, 0.0538, 0.1368, 0.4791, 0.0953],\n",
      "       grad_fn=<AddBackward0>) tensor([0.6000, 0.7000, 0.4000, 0.0390, 0.0920, 0.2520, 0.1190]) tensor(1.0494, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9998, 3.0537, 1.0609, 0.0553, 0.1648, 0.4898, 0.1352],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 1.1000, 0.3000, 0.0560, 0.1740, 0.2810, 0.0970]) tensor(7.2397, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4209, 5.1569, 1.1452, 0.0732, 0.1920, 0.5598, 0.0879],\n",
      "       grad_fn=<AddBackward0>) tensor([12.6000,  6.5000,  1.0000,  0.0430,  0.1590,  0.5990,  0.0470]) tensor(1.7053, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.5430, 1.9115, 0.3317, 0.0576, 0.1490, 0.4649, 0.0879],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 5.0000, 0.0000, 0.0000, 0.2940, 0.3330, 0.0000]) tensor(1.6887, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.8180, 3.8636, 1.1517, 0.0604, 0.1563, 0.5256, 0.0848],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 2.7000, 0.2000, 0.1280, 0.1700, 0.5640, 0.0320]) tensor(1.8969, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6407,  4.3448,  2.8840,  0.0423,  0.1424,  0.5440,  0.1822],\n",
      "       grad_fn=<AddBackward0>) tensor([11.7000,  4.2000,  2.0000,  0.0270,  0.1380,  0.5960,  0.1010]) tensor(0.2424, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.5397,  5.9616,  3.7516,  0.0432,  0.1618,  0.6004,  0.2144],\n",
      "       grad_fn=<AddBackward0>) tensor([21.7000,  6.8000,  3.5000,  0.0260,  0.1770,  0.5550,  0.1770]) tensor(1.5368, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.3499, 1.1829, 1.6780, 0.0323, 0.0986, 0.4631, 0.1385],\n",
      "       grad_fn=<AddBackward0>) tensor([ 2.1000, 11.2000,  1.3000,  0.1140,  0.3480,  0.3880,  0.0630]) tensor(14.5896, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9867, 0.1945, 1.6401, 0.0182, 0.1049, 0.4191, 0.2125],\n",
      "       grad_fn=<AddBackward0>) tensor([2., 0., 0., 0., 0., 1., 0.]) tensor(3.9984, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0753, 1.7505, 2.6088, 0.0214, 0.1003, 0.4880, 0.1991],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 2.0000, 2.6000, 0.0240, 0.0970, 0.4720, 0.2250]) tensor(0.3200, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.8084,  4.8818,  3.3393,  0.0399,  0.1395,  0.5598,  0.1903],\n",
      "       grad_fn=<AddBackward0>) tensor([11.4000,  2.4000,  4.7000,  0.0140,  0.0750,  0.5960,  0.2230]) tensor(1.9741, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6429,  4.5933,  3.7981,  0.0289,  0.1216,  0.5653,  0.2099],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 1.5000, 3.0000, 0.0080, 0.0710, 0.5090, 0.2260]) tensor(9.5868, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.3544e+01, 1.9452e+00, 4.4288e+00, 9.3504e-04, 7.0537e-02, 5.0675e-01,\n",
      "        2.7802e-01], grad_fn=<AddBackward0>) tensor([13.8000,  2.8000,  4.0000,  0.0210,  0.0830,  0.5150,  0.2090]) tensor(0.1408, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0631,  3.8991,  2.5015,  0.0430,  0.1461,  0.5255,  0.1810],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 1.7000, 0.7000, 0.0140, 0.1540, 0.4930, 0.0800]) tensor(8.9011, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.2667,  7.6071,  2.4122,  0.0820,  0.2273,  0.6137,  0.1416],\n",
      "       grad_fn=<AddBackward0>) tensor([18.5000, 11.3000,  2.5000,  0.1310,  0.1870,  0.5430,  0.1190]) tensor(2.1680, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1054,  4.2657,  3.0791,  0.0332,  0.1282,  0.5557,  0.1838],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 2.2000, 1.2000, 0.0260, 0.0830, 0.5310, 0.0760]) tensor(5.1371, grad_fn=<MseLossBackward0>)\n",
      "tensor([21.9825,  5.8534,  5.0420,  0.0282,  0.1427,  0.6048,  0.2824],\n",
      "       grad_fn=<AddBackward0>) tensor([26.8000,  5.2000,  4.9000,  0.0350,  0.1270,  0.5610,  0.2320]) tensor(3.3801, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9862,  2.8813,  2.9024,  0.0217,  0.1036,  0.5254,  0.1899],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 1.3000, 2.3000, 0.0120, 0.0640, 0.4770, 0.1910]) tensor(3.4144, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.5109, 3.1456, 0.7571, 0.0584, 0.1514, 0.5134, 0.0763],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 4.6000, 0.8000, 0.0360, 0.1960, 0.5030, 0.0580]) tensor(0.4723, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5862, 3.7079, 1.2922, 0.0645, 0.1791, 0.5183, 0.1329],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 0.8000, 0.1000, 0.1020, 0.1430, 0.3980, 0.0300]) tensor(11.2237, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9718,  3.7099,  3.5134,  0.0245,  0.1202,  0.5470,  0.2254],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 3.8000, 1.5000, 0.0930, 0.1260, 0.4740, 0.1500]) tensor(5.5080, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.2642, 2.5889, 0.0957, 0.0666, 0.1618, 0.4729, 0.0662],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 1.9000, 0.4000, 0.0370, 0.1230, 0.3710, 0.0450]) tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8720,  3.5384,  3.5635,  0.0263,  0.1120,  0.5387,  0.2128],\n",
      "       grad_fn=<AddBackward0>) tensor([9.5000, 3.3000, 4.9000, 0.0340, 0.1140, 0.5550, 0.2880]) tensor(1.8885, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.7345, 3.4143, 0.8865, 0.0642, 0.1693, 0.5068, 0.1027],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.8000, 0.8000, 0.0470, 0.1710, 0.4080, 0.1130]) tensor(3.4431, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9057, 3.0352, 1.5684, 0.0417, 0.1350, 0.5133, 0.1314],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 1.7000, 0.4000, 0.0510, 0.1210, 0.5070, 0.0610]) tensor(2.9773, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1079,  4.4920,  2.8444,  0.0483,  0.1517,  0.5499,  0.1739],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000e+00, 3.1000e+00, 7.0000e-01, 6.0000e-03, 1.8000e-01, 4.6900e-01,\n",
      "        6.8000e-02]) tensor(4.2389, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1424,  4.5177,  2.8167,  0.0405,  0.1375,  0.5608,  0.1630],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 2.1000, 1.6000, 0.0210, 0.0560, 0.5840, 0.0850]) tensor(2.2026, grad_fn=<MseLossBackward0>)\n",
      "tensor([-8.9460,  0.0789, -2.7924,  0.0643,  0.1502,  0.3561, -0.0269],\n",
      "       grad_fn=<AddBackward0>) tensor([0., 0., 0., 0., 0., 0., 0.]) tensor(12.5698, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8653, 2.5713, 1.5620, 0.0408, 0.1295, 0.4978, 0.1353],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 1.4000, 0.4000, 0.0900, 0.1260, 0.4940, 0.0920]) tensor(3.4988, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8942, 1.4473, 2.5992, 0.0177, 0.0930, 0.4874, 0.2002],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 0.8000, 0.7000, 0.0190, 0.1000, 0.4850, 0.1470]) tensor(5.5387, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.6719,  6.2396,  2.9556,  0.0551,  0.1741,  0.5978,  0.1626],\n",
      "       grad_fn=<AddBackward0>) tensor([11.5000,  4.9000,  2.9000,  0.0330,  0.1400,  0.5030,  0.1470]) tensor(2.7447, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.2081,  6.0387,  3.7041,  0.0432,  0.1598,  0.5980,  0.2085],\n",
      "       grad_fn=<AddBackward0>) tensor([16.7000,  3.7000,  3.4000,  0.0330,  0.0870,  0.5760,  0.1590]) tensor(1.1207, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.0919, 0.2765, 0.2618, 0.0417, 0.1145, 0.4207, 0.1025],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 0.9000, 0.4000, 0.0000, 0.1150, 0.2730, 0.0830]) tensor(0.1550, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.0353, 2.3525, 0.4606, 0.0645, 0.1544, 0.4659, 0.0810],\n",
      "       grad_fn=<AddBackward0>) tensor([0.8000, 1.4000, 0.1000, 0.0610, 0.1700, 0.3020, 0.0240]) tensor(0.3705, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.0399, 2.7541, 1.1555, 0.0486, 0.1348, 0.5039, 0.1003],\n",
      "       grad_fn=<AddBackward0>) tensor([0.8000, 0.7000, 0.2000, 0.0770, 0.1350, 0.5910, 0.0920]) tensor(3.3025, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5599, 3.3454, 2.6110, 0.0337, 0.1190, 0.5312, 0.1625],\n",
      "       grad_fn=<AddBackward0>) tensor([10.8000,  3.5000,  5.8000,  0.0140,  0.1180,  0.6680,  0.2900]) tensor(1.6810, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9059, 4.6055, 2.0757, 0.0562, 0.1587, 0.5476, 0.1291],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 7.3000, 0.7000, 0.0900, 0.2520, 0.4790, 0.0430]) tensor(3.4901, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9004,  4.6971,  2.2087,  0.0572,  0.1693,  0.5448,  0.1544],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 2.8000, 0.5000, 0.0740, 0.1870, 0.5250, 0.0660]) tensor(8.1348, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0694, 3.4572, 1.2811, 0.0512, 0.1480, 0.5184, 0.1082],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 2.8000, 0.4000, 0.0700, 0.1950, 0.4050, 0.0590]) tensor(2.9025, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.1267, -0.3799,  0.5281,  0.0282,  0.0911,  0.4138,  0.1179],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 1.7000, 1.3000, 0.0180, 0.1020, 0.5190, 0.1430]) tensor(3.1377, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.6813, 3.3508, 0.9535, 0.0624, 0.1662, 0.5125, 0.1021],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([3.9000, 2.3000, 0.4000, 0.1060, 0.1220, 0.4910, 0.0580]) tensor(1.3074, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0230, 3.5123, 1.6047, 0.0503, 0.1478, 0.5201, 0.1254],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 2.0000, 0.6000, 0.0770, 0.2020, 0.4320, 0.1250]) tensor(5.4843, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6448,  6.4237,  2.5038,  0.0706,  0.1981,  0.5908,  0.1455],\n",
      "       grad_fn=<AddBackward0>) tensor([16.1000, 10.5000,  2.2000,  0.1150,  0.2590,  0.5380,  0.1110]) tensor(2.6908, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.0188, 1.4964, 0.9143, 0.0402, 0.1160, 0.4762, 0.1031],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 3.5000, 0.4000, 0.0580, 0.1610, 0.5850, 0.0410]) tensor(1.0176, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1110,  5.3864,  2.4547,  0.0568,  0.1698,  0.5635,  0.1522],\n",
      "       grad_fn=<AddBackward0>) tensor([6.2000, 5.0000, 1.9000, 0.0530, 0.1970, 0.4980, 0.1270]) tensor(6.8893, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.5947e+01, 3.6352e+00, 4.2000e+00, 1.5336e-02, 1.0442e-01, 5.4529e-01,\n",
      "        2.5500e-01], grad_fn=<AddBackward0>) tensor([10.9000,  1.7000,  2.7000,  0.0230,  0.0590,  0.5240,  0.2290]) tensor(4.4959, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.1340,  6.6642,  2.5269,  0.0687,  0.1965,  0.6049,  0.1392],\n",
      "       grad_fn=<AddBackward0>) tensor([14.5000,  7.6000,  2.0000,  0.0770,  0.2040,  0.6570,  0.1040]) tensor(0.2227, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.1567,  5.7540,  3.1291,  0.0519,  0.1716,  0.5708,  0.1932],\n",
      "       grad_fn=<AddBackward0>) tensor([13.1000,  6.9000,  1.2000,  0.0870,  0.1620,  0.5600,  0.0630]) tensor(2.0567, grad_fn=<MseLossBackward0>)\n",
      "tensor([-1.0059,  1.3558,  0.3077,  0.0486,  0.1171,  0.4683,  0.0551],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 4.1000, 0.5000, 0.1400, 0.2340, 0.5330, 0.0590]) tensor(1.9118, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.5431,  7.8109,  2.1044,  0.0854,  0.2333,  0.6152,  0.1237],\n",
      "       grad_fn=<AddBackward0>) tensor([16.2000,  8.1000,  2.1000,  0.0720,  0.1850,  0.5230,  0.1060]) tensor(0.0304, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0511, 2.1794, 2.6295, 0.0232, 0.0976, 0.4974, 0.1797],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 5.3000, 8.0000, 0.0230, 0.1420, 0.4730, 0.3600]) tensor(5.7051, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.7971,  5.4708,  3.3102,  0.0475,  0.1658,  0.5828,  0.2021],\n",
      "       grad_fn=<AddBackward0>) tensor([18.8000,  9.6000,  3.9000,  0.0690,  0.2480,  0.4990,  0.1960]) tensor(3.0606, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9703,  5.9253,  1.9181,  0.0703,  0.1900,  0.5795,  0.1133],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 3.7000, 1.4000, 0.0520, 0.1630, 0.6170, 0.1140]) tensor(3.1129, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.8192,  5.3763,  2.9321,  0.0511,  0.1651,  0.5739,  0.1772],\n",
      "       grad_fn=<AddBackward0>) tensor([19.1000,  7.4000,  3.7000,  0.0670,  0.1530,  0.5370,  0.1540]) tensor(3.2875, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.6810,  2.5062, -0.0736,  0.0601,  0.1524,  0.4742,  0.0545],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 1.0000, 0.3000, 0.0510, 0.1330, 0.5290, 0.1010]) tensor(0.3779, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2921, 5.0624, 0.9283, 0.0782, 0.1957, 0.5545, 0.0747],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 7.7000, 2.1000, 0.0970, 0.2350, 0.4630, 0.1210]) tensor(2.0790, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.1017,  0.7562,  0.1150,  0.0504,  0.1282,  0.4375,  0.0846],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 1.7000, 0.2000, 0.1020, 0.2020, 0.6970, 0.0480]) tensor(1.2606, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7755,  2.4579,  2.5084,  0.0258,  0.1169,  0.5036,  0.1972],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 0.5000, 0.4000, 0.0300, 0.0920, 0.3600, 0.1350]) tensor(12.9526, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0573,  4.8698,  2.7301,  0.0500,  0.1582,  0.5547,  0.1716],\n",
      "       grad_fn=<AddBackward0>) tensor([14.1000,  5.2000,  1.9000,  0.0430,  0.1360,  0.5500,  0.0870]) tensor(0.2704, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0017, 4.6572, 1.2079, 0.0682, 0.1831, 0.5411, 0.1015],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 2.3000, 0.5000, 0.1290, 0.1360, 0.6030, 0.0840]) tensor(4.7322, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0804,  5.8388,  1.9790,  0.0639,  0.1813,  0.5826,  0.1149],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 5.5000, 0.7000, 0.0950, 0.1910, 0.5330, 0.0460]) tensor(3.7948, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.8270,  5.7788,  4.2355,  0.0342,  0.1441,  0.5914,  0.2349],\n",
      "       grad_fn=<AddBackward0>) tensor([14.2000,  3.8000,  6.7000,  0.0220,  0.1010,  0.4830,  0.3060]) tensor(4.4882, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7676,  4.0902,  1.5549,  0.0743,  0.2000,  0.5320,  0.1522],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 1.0000, 0.0000, 0.3330, 0.0000, 0.6670, 0.0000]) tensor(10.3501, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7264, 1.7796, 2.8370, 0.0180, 0.0857, 0.4887, 0.1904],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 1.5000, 3.3000, 0.0150, 0.0860, 0.3980, 0.3300]) tensor(2.7197, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.8492,  2.4574, -0.6054,  0.0737,  0.1645,  0.4798,  0.0149],\n",
      "       grad_fn=<AddBackward0>) tensor([0.5000, 1.9000, 0.2000, 0.1260, 0.1480, 0.1850, 0.0410]) tensor(0.4100, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2877, 1.6183, 3.4189, 0.0107, 0.0747, 0.4923, 0.2206],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 0.5000, 1.4000, 0.0070, 0.0680, 0.4740, 0.2470]) tensor(7.1505, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9072, 4.1301, 2.1779, 0.0524, 0.1552, 0.5253, 0.1522],\n",
      "       grad_fn=<AddBackward0>) tensor([8.7000, 4.8000, 0.9000, 0.0610, 0.1990, 0.5230, 0.0660]) tensor(0.5070, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2967,  3.8869,  3.0774,  0.0306,  0.1277,  0.5423,  0.1997],\n",
      "       grad_fn=<AddBackward0>) tensor([11.6000,  3.0000,  3.2000,  0.0290,  0.1070,  0.5120,  0.2440]) tensor(0.5262, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7397,  5.1258,  2.5997,  0.0513,  0.1588,  0.5695,  0.1526],\n",
      "       grad_fn=<AddBackward0>) tensor([10.3000,  2.5000,  1.1000,  0.0180,  0.0980,  0.6200,  0.0810]) tensor(2.1584, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7317,  4.8505,  2.4787,  0.0449,  0.1460,  0.5640,  0.1442],\n",
      "       grad_fn=<AddBackward0>) tensor([9.4000, 4.2000, 1.4000, 0.0170, 0.1330, 0.5220, 0.0700]) tensor(1.0045, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.3993e+01, 3.5420e+00, 4.1514e+00, 1.2368e-02, 9.1250e-02, 5.5286e-01,\n",
      "        2.3229e-01], grad_fn=<AddBackward0>) tensor([18.1000,  3.0000,  2.3000,  0.0190,  0.0680,  0.6030,  0.0990]) tensor(2.9441, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.4897,  6.1434,  1.5958,  0.0753,  0.2003,  0.5852,  0.0966],\n",
      "       grad_fn=<AddBackward0>) tensor([12.6000,  9.0000,  3.7000,  0.1070,  0.1970,  0.5330,  0.1720]) tensor(1.9758, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9912, 2.5258, 2.5411, 0.0261, 0.1061, 0.5056, 0.1769],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 1.1000, 2.0000, 0.0130, 0.0620, 0.5550, 0.1820]) tensor(2.0741, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7118,  3.7488,  2.7120,  0.0358,  0.1313,  0.5343,  0.1804],\n",
      "       grad_fn=<AddBackward0>) tensor([5.9000, 2.0000, 1.0000, 0.0170, 0.1220, 0.5880, 0.1020]) tensor(5.6823, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6085,  2.8784,  3.5728,  0.0208,  0.1098,  0.5062,  0.2464],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6500e+01, 2.3000e+00, 7.4000e+00, 9.0000e-03, 6.5000e-02, 4.8800e-01,\n",
      "        3.7500e-01]) tensor(3.3374, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3714,  5.9956,  2.4349,  0.0647,  0.1792,  0.5819,  0.1295],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 4.6000, 0.8000, 0.0800, 0.1800, 0.5380, 0.0620]) tensor(6.4603, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4312,  4.2159,  3.0852,  0.0359,  0.1393,  0.5453,  0.2037],\n",
      "       grad_fn=<AddBackward0>) tensor([13.4000,  1.7000,  2.8000,  0.0160,  0.0510,  0.4660,  0.1690]) tensor(1.0700, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.7356, 2.3481, 1.1051, 0.0474, 0.1336, 0.4909, 0.1087],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 0.6000, 0.6000, 0.0000, 0.0900, 0.4520, 0.1110]) tensor(1.3923, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.7814,  6.1333,  4.0992,  0.0421,  0.1625,  0.6008,  0.2338],\n",
      "       grad_fn=<AddBackward0>) tensor([23.5000,  4.5000,  4.0000,  0.0270,  0.1160,  0.5500,  0.2080]) tensor(2.3587, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9399, 3.4630, 2.4299, 0.0397, 0.1268, 0.5155, 0.1587],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 1.6000, 1.0000, 0.0130, 0.0860, 0.5330, 0.0840]) tensor(3.9985, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4361, 1.7536, 2.6862, 0.0192, 0.0877, 0.4804, 0.1884],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 1.8000, 4.1000, 0.0140, 0.0690, 0.4610, 0.2390]) tensor(1.5178, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0518, 3.2639, 2.3953, 0.0339, 0.1139, 0.5236, 0.1461],\n",
      "       grad_fn=<AddBackward0>) tensor([8.8000, 2.7000, 3.0000, 0.0170, 0.0890, 0.5090, 0.1420]) tensor(0.1778, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.4556e+01, 3.3688e+00, 3.8022e+00, 1.2388e-02, 9.8261e-02, 5.4047e-01,\n",
      "        2.3568e-01], grad_fn=<AddBackward0>) tensor([6.8000, 1.5000, 3.3000, 0.0090, 0.0790, 0.4710, 0.2800]) tensor(9.1291, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1642, 5.0970, 1.2585, 0.0757, 0.1940, 0.5411, 0.1000],\n",
      "       grad_fn=<AddBackward0>) tensor([ 8.3000, 10.0000,  1.9000,  0.1050,  0.2930,  0.6820,  0.1320]) tensor(3.6043, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9202,  5.8882,  2.6096,  0.0563,  0.1775,  0.5915,  0.1550],\n",
      "       grad_fn=<AddBackward0>) tensor([13.2000,  8.4000,  0.7000,  0.1020,  0.1970,  0.4780,  0.0420]) tensor(1.8490, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.0723,  4.7036,  4.6269,  0.0184,  0.1128,  0.5759,  0.2578],\n",
      "       grad_fn=<AddBackward0>) tensor([12.5000,  3.1000,  5.2000,  0.0310,  0.0850,  0.5080,  0.2690]) tensor(4.8510, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.9172, 2.1977, 1.6193, 0.0415, 0.1199, 0.4924, 0.1251],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 3.7000, 2.4000, 0.0400, 0.1320, 0.5070, 0.1810]) tensor(0.4244, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4927, 3.0687, 0.6888, 0.0609, 0.1608, 0.5109, 0.0867],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000, 1.7000, 0.3000, 0.0610, 0.3000, 0.4000, 0.0850]) tensor(3.0503, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2156,  5.8323,  1.9658,  0.0705,  0.1914,  0.5688,  0.1239],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 3.5000, 0.8000, 0.1150, 0.1930, 0.6050, 0.0850]) tensor(12.3273, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.4161, 2.4370, 0.6623, 0.0552, 0.1441, 0.4807, 0.0897],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 2.2000, 0.3000, 0.0970, 0.1560, 0.6480, 0.0480]) tensor(0.0560, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5548, 3.7973, 0.5494, 0.0684, 0.1741, 0.5252, 0.0689],\n",
      "       grad_fn=<AddBackward0>) tensor([13.9000, 13.8000,  1.2000,  0.1180,  0.2650,  0.6260,  0.0560]) tensor(24.3058, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2247,  4.2315,  2.2556,  0.0526,  0.1594,  0.5412,  0.1567],\n",
      "       grad_fn=<AddBackward0>) tensor([21.5000,  6.9000,  7.1000,  0.0560,  0.1530,  0.6070,  0.3450]) tensor(19.4586, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0442, 3.4602, 1.1998, 0.0546, 0.1541, 0.5183, 0.1071],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 2.7000, 0.7000, 0.0410, 0.2170, 0.4860, 0.0930]) tensor(1.4429, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0452,  3.3191,  2.7845,  0.0462,  0.1619,  0.5256,  0.2281],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 1.3000, 0.3000, 0.1430, 0.1540, 0.5970, 0.1250]) tensor(17.9619, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0022,  6.7812,  1.3290,  0.0929,  0.2328,  0.5922,  0.0867],\n",
      "       grad_fn=<AddBackward0>) tensor([10.1000,  5.4000,  1.5000,  0.0490,  0.2390,  0.5270,  0.1210]) tensor(0.7947, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4304,  5.0036,  1.8249,  0.0614,  0.1820,  0.5599,  0.1365],\n",
      "       grad_fn=<AddBackward0>) tensor([11.5000,  5.8000,  0.7000,  0.0630,  0.1900,  0.5520,  0.0500]) tensor(0.3962, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.7359, 0.7200, 1.4791, 0.0291, 0.1022, 0.4568, 0.1548],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 0.9000, 1.8000, 0.0700, 0.0620, 0.5250, 0.3600]) tensor(0.9452, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2728, 5.5092, 0.9671, 0.0801, 0.2041, 0.5599, 0.0783],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 5.2000, 0.4000, 0.0950, 0.2040, 0.5210, 0.0340]) tensor(0.7346, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.3006,  2.2879, -0.3257,  0.0647,  0.1609,  0.4695,  0.0505],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4000, 3.4000, 0.4000, 0.1550, 0.2000, 0.5740, 0.0610]) tensor(0.4276, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4025,  3.3833,  3.4642,  0.0282,  0.1205,  0.5216,  0.2282],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 1.3000, 0.9000, 0.0120, 0.1040, 0.5300, 0.1140]) tensor(10.4827, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2514, 4.9201, 1.9040, 0.0593, 0.1612, 0.5543, 0.1101],\n",
      "       grad_fn=<AddBackward0>) tensor([ 6.4000, 10.7000,  2.4000,  0.1270,  0.2200,  0.4560,  0.1010]) tensor(5.9717, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1573,  2.7792,  3.0871,  0.0241,  0.1128,  0.5032,  0.2217],\n",
      "       grad_fn=<AddBackward0>) tensor([12.1000,  2.5000,  4.0000,  0.0220,  0.0910,  0.5180,  0.2590]) tensor(0.1310, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1015,  6.4394,  1.3790,  0.0854,  0.2153,  0.5759,  0.0876],\n",
      "       grad_fn=<AddBackward0>) tensor([8.8000, 8.2000, 1.1000, 0.0970, 0.2140, 0.4780, 0.0580]) tensor(1.2122, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1491,  4.2090,  2.4240,  0.0437,  0.1420,  0.5526,  0.1511],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 3.3000, 1.4000, 0.0230, 0.1180, 0.5630, 0.0830]) tensor(0.6114, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8497, 1.0787, 2.5103, 0.0225, 0.0917, 0.4819, 0.1871],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 0.0000, 0.3000, 0.0000, 0.0000, 0.7140, 0.1000]) tensor(3.5770, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9979, 4.0202, 0.7731, 0.0687, 0.1803, 0.5215, 0.0888],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 0.7000, 0.2000, 0.0740, 0.1520, 0.4130, 0.0860]) tensor(5.9418, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2407,  3.3840,  3.7978,  0.0234,  0.1142,  0.5178,  0.2476],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4200e+01, 2.6000e+00, 6.2000e+00, 1.2000e-02, 8.9000e-02, 5.1200e-01,\n",
      "        3.5800e-01]) tensor(0.9143, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.0723e+01, 5.1633e+00, 5.0642e+00, 1.6944e-02, 1.1777e-01, 5.8966e-01,\n",
      "        2.8242e-01], grad_fn=<AddBackward0>) tensor([21.1000,  4.0000, 11.6000,  0.0240,  0.1030,  0.5760,  0.5000]) tensor(6.3228, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1760,  4.8422,  2.6930,  0.0448,  0.1506,  0.5658,  0.1644],\n",
      "       grad_fn=<AddBackward0>) tensor([5.9000, 1.0000, 1.9000, 0.0160, 0.0680, 0.5490, 0.2330]) tensor(9.7634, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4257, 2.3775, 1.3382, 0.0426, 0.1253, 0.5026, 0.1137],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 2.3000, 0.6000, 0.0610, 0.1420, 0.5070, 0.0610]) tensor(1.0641, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0932,  4.5614,  2.4266,  0.0487,  0.1553,  0.5507,  0.1589],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 1.8000, 0.8000, 0.0410, 0.1270, 0.5240, 0.0940]) tensor(8.2563, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.3965, -0.4287,  1.3109,  0.0115,  0.0772,  0.3871,  0.1797],\n",
      "       grad_fn=<AddBackward0>) tensor([0.3000, 0.3000, 0.0000, 0.0420, 0.0000, 0.0510, 0.0000]) tensor(0.9710, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.1631, 3.6426, 0.0753, 0.0735, 0.1883, 0.4995, 0.0689],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 0.9000, 0.2000, 0.0680, 0.2220, 0.4740, 0.0710]) tensor(3.2090, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7665, 4.3448, 2.0019, 0.0522, 0.1529, 0.5442, 0.1299],\n",
      "       grad_fn=<AddBackward0>) tensor([10.5000,  4.2000,  1.3000,  0.0270,  0.1010,  0.5720,  0.0610]) tensor(0.1515, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2570,  2.3999,  2.7392,  0.0182,  0.0994,  0.5067,  0.1951],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 1.2000, 1.9000, 0.0090, 0.1050, 0.4420, 0.2470]) tensor(6.4493, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.0203,  6.2162,  2.5245,  0.0677,  0.2003,  0.5799,  0.1673],\n",
      "       grad_fn=<AddBackward0>) tensor([18.5000, 10.7000,  6.1000,  0.0810,  0.2540,  0.6030,  0.2860]) tensor(5.5793, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9187, 4.7704, 1.2528, 0.0678, 0.1764, 0.5493, 0.0867],\n",
      "       grad_fn=<AddBackward0>) tensor([7.5000, 7.0000, 0.5000, 0.1290, 0.2420, 0.5510, 0.0360]) tensor(0.8177, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.1656,  5.5794,  3.3764,  0.0470,  0.1553,  0.5836,  0.1847],\n",
      "       grad_fn=<AddBackward0>) tensor([10.4000,  2.9000,  1.0000,  0.0260,  0.1180,  0.5350,  0.0630]) tensor(5.0794, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9441,  5.4049,  0.9390,  0.0935,  0.2386,  0.5408,  0.1191],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 1.5000, 0.0000, 0.6000, 0.0000, 0.0000, 0.0000]) tensor(22.7729, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5363, 3.0811, 1.6681, 0.0439, 0.1290, 0.5163, 0.1179],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 4.4000, 2.8000, 0.0260, 0.1720, 0.6760, 0.1810]) tensor(0.5360, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 7.4247,  7.6053, -0.5224,  0.1265,  0.2781,  0.5859, -0.0158],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 5.1000, 1.3000, 0.0930, 0.1670, 0.6090, 0.0950]) tensor(1.5556, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.3348, 2.0162, 0.5386, 0.0572, 0.1555, 0.4620, 0.1122],\n",
      "       grad_fn=<AddBackward0>) tensor([0.4000, 0.3000, 0.3000, 0.0670, 0.0710, 0.2220, 0.1670]) tensor(2.6504, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7630,  2.7420,  3.4793,  0.0138,  0.0902,  0.5192,  0.2183],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 2.7000, 5.2000, 0.0230, 0.0970, 0.5040, 0.3120]) tensor(3.5307, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6629,  5.9060,  1.7611,  0.0705,  0.1911,  0.5820,  0.1056],\n",
      "       grad_fn=<AddBackward0>) tensor([12.6000,  6.8000,  0.8000,  0.0650,  0.1600,  0.5330,  0.0370]) tensor(0.3727, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0635,  4.1697,  2.9893,  0.0363,  0.1304,  0.5443,  0.1805],\n",
      "       grad_fn=<AddBackward0>) tensor([11.9000,  2.9000,  4.1000,  0.0170,  0.0900,  0.5190,  0.2220]) tensor(0.4110, grad_fn=<MseLossBackward0>)\n",
      "tensor([-1.0567,  2.3054, -1.4767,  0.0858,  0.1943,  0.4512,  0.0092],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 2.8000, 0.2000, 0.1490, 0.1710, 0.4460, 0.0250]) tensor(2.7882, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0606, 4.9908, 0.7077, 0.0837, 0.2072, 0.5306, 0.0821],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 4.3000, 0.3000, 0.1140, 0.1790, 0.5160, 0.0230]) tensor(1.0293, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.8699,  5.0871,  3.0654,  0.0415,  0.1450,  0.5709,  0.1748],\n",
      "       grad_fn=<AddBackward0>) tensor([9.9000, 4.2000, 1.6000, 0.0320, 0.1290, 0.5090, 0.0840]) tensor(2.6724, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.2037, 3.8838, 0.6220, 0.0723, 0.1819, 0.5156, 0.0813],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000, 1.4000, 0.2000, 0.1350, 0.2470, 0.5010, 0.0710]) tensor(4.6292, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2144,  3.1611,  3.3146,  0.0256,  0.1070,  0.5164,  0.2066],\n",
      "       grad_fn=<AddBackward0>) tensor([3.9000, 1.3000, 1.9000, 0.0140, 0.0910, 0.5010, 0.2130]) tensor(8.4238, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6518,  6.5707,  1.9854,  0.0769,  0.2008,  0.6015,  0.1023],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 8.1000, 2.7000, 0.1010, 0.2620, 0.5650, 0.1490]) tensor(6.1719, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2996, 1.9288, 2.3378, 0.0257, 0.1105, 0.4942, 0.1894],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 0.8000, 0.3000, 0.0590, 0.0900, 0.5370, 0.0770]) tensor(5.2569, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4207, 3.8175, 0.6128, 0.0746, 0.1848, 0.5059, 0.0812],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 3.0000, 0.0000, 0.0730, 0.3000, 0.2940, 0.0000]) tensor(2.0311, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4478, 1.9586, 2.5714, 0.0232, 0.0999, 0.4969, 0.1855],\n",
      "       grad_fn=<AddBackward0>) tensor([11.9000,  3.6000,  8.4000,  0.0200,  0.1000,  0.4580,  0.3680]) tensor(6.9456, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 4.8675,  4.5995, -0.3444,  0.0942,  0.2190,  0.5171,  0.0349],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 2.4000, 0.5000, 0.0960, 0.1660, 0.5470, 0.0740]) tensor(1.3467, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.2334, 1.6254, 1.9175, 0.0325, 0.1104, 0.4829, 0.1584],\n",
      "       grad_fn=<AddBackward0>) tensor([7.5000, 1.9000, 1.6000, 0.0150, 0.0980, 0.5060, 0.1460]) tensor(0.2545, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.2014, 4.6142, 0.7571, 0.0731, 0.1855, 0.5394, 0.0723],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 1.9000, 0.6000, 0.0990, 0.1510, 0.4110, 0.1120]) tensor(5.0737, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.1001, 3.9973, 0.2644, 0.0719, 0.1747, 0.5250, 0.0438],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 2.9000, 0.7000, 0.1180, 0.1110, 0.4580, 0.0670]) tensor(0.3164, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1764, 2.0391, 1.2927, 0.0438, 0.1338, 0.4738, 0.1408],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 2.0000, 0.5000, 0.1010, 0.1460, 0.4250, 0.0830]) tensor(1.6248, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9177,  3.4912,  2.3446,  0.0393,  0.1364,  0.5298,  0.1674],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 1.3000, 0.7000, 0.0160, 0.0940, 0.5210, 0.0890]) tensor(5.5819, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1291,  4.0187,  3.1977,  0.0298,  0.1183,  0.5422,  0.1882],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 2.3000, 3.6000, 0.0170, 0.1070, 0.5100, 0.2930]) tensor(6.3516, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.3330,  3.7148,  3.1562,  0.0335,  0.1310,  0.5264,  0.2120],\n",
      "       grad_fn=<AddBackward0>) tensor([5.4000, 1.3000, 0.9000, 0.0360, 0.0650, 0.4660, 0.1050]) tensor(10.5533, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.0560,  2.4907, -0.2196,  0.0651,  0.1504,  0.4830,  0.0281],\n",
      "       grad_fn=<AddBackward0>) tensor([0.3000, 0.6000, 0.4000, 0.0150, 0.0770, 0.2200, 0.0850]) tensor(0.5951, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9547, 3.0866, 2.3994, 0.0370, 0.1230, 0.5136, 0.1628],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 0.6000, 0.5000, 0.0180, 0.0300, 0.5050, 0.0520]) tensor(3.1067, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.4051e+01, 2.6969e+00, 4.3092e+00, 6.7029e-03, 8.1295e-02, 5.1765e-01,\n",
      "        2.6489e-01], grad_fn=<AddBackward0>) tensor([1.3100e+01, 3.5000e+00, 5.3000e+00, 1.3000e-02, 1.0000e-01, 4.9200e-01,\n",
      "        2.4600e-01]) tensor(0.3617, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0959, 1.9367, 2.9933, 0.0175, 0.0895, 0.4883, 0.2072],\n",
      "       grad_fn=<AddBackward0>) tensor([11.6000,  1.6000,  1.8000,  0.0140,  0.0540,  0.5920,  0.1120]) tensor(1.1184, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4603,  2.7836,  2.9448,  0.0299,  0.1151,  0.4985,  0.2046],\n",
      "       grad_fn=<AddBackward0>) tensor([5.9000, 1.5000, 1.3000, 0.0170, 0.0750, 0.4770, 0.1210]) tensor(3.5941, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0118,  3.5206,  3.1904,  0.0242,  0.1140,  0.5352,  0.2048],\n",
      "       grad_fn=<AddBackward0>) tensor([9.4000, 2.6000, 4.2000, 0.0170, 0.0990, 0.5020, 0.2470]) tensor(2.1308, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2273,  5.8341,  1.8695,  0.0659,  0.1802,  0.5773,  0.1051],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 3.3000, 0.6000, 0.0920, 0.1190, 0.5030, 0.0490]) tensor(8.0048, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1434, 4.3103, 0.1181, 0.0802, 0.2001, 0.5264, 0.0573],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 1.6000, 0.3000, 0.0590, 0.1600, 0.4350, 0.0500]) tensor(2.3788, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2781,  6.6602,  1.9084,  0.0747,  0.2086,  0.5927,  0.1207],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 4.5000, 0.5000, 0.1240, 0.1960, 0.5550, 0.0580]) tensor(5.8872, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7102,  4.2317,  2.1193,  0.0481,  0.1511,  0.5445,  0.1438],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3500e+01, 3.6000e+00, 2.1000e+00, 1.3000e-02, 1.1000e-01, 5.7500e-01,\n",
      "        1.0700e-01]) tensor(1.1696, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0160,  5.1221,  1.4750,  0.0733,  0.1986,  0.5453,  0.1236],\n",
      "       grad_fn=<AddBackward0>) tensor([14.6000,  7.2000,  3.6000,  0.0520,  0.2080,  0.5260,  0.1800]) tensor(3.0975, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2609,  5.1792,  2.7721,  0.0447,  0.1487,  0.5747,  0.1567],\n",
      "       grad_fn=<AddBackward0>) tensor([12.3000,  5.9000,  3.1000,  0.0450,  0.1360,  0.5980,  0.1260]) tensor(0.2217, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1355,  5.0480,  2.2366,  0.0612,  0.1698,  0.5669,  0.1310],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 2.7000, 0.8000, 0.0600, 0.1410, 0.6190, 0.0910]) tensor(5.9479, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.4922, 2.2031, 0.1541, 0.0650, 0.1640, 0.4766, 0.0829],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 1.4000, 0.2000, 0.0830, 0.2550, 0.5900, 0.0650]) tensor(0.5046, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.4996,  5.2698,  4.3414,  0.0333,  0.1466,  0.5759,  0.2607],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000e+01, 3.3000e+00, 8.7000e+00, 1.3000e-02, 9.6000e-02, 5.2700e-01,\n",
      "        4.5800e-01]) tensor(3.5961, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.6011, 3.3735, 0.1396, 0.0691, 0.1760, 0.5105, 0.0614],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 3.2000, 0.6000, 0.1000, 0.1500, 0.5340, 0.0650]) tensor(0.2074, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.6693,  3.0253, -0.1228,  0.0708,  0.1730,  0.4890,  0.0518],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.9000, 0.2000, 0.0650, 0.1660, 0.5090, 0.0390]) tensor(0.2422, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.6145,  7.0898,  2.8895,  0.0703,  0.2019,  0.6080,  0.1569],\n",
      "       grad_fn=<AddBackward0>) tensor([11.9000,  4.5000,  1.4000,  0.0750,  0.1340,  0.5580,  0.1010]) tensor(4.4517, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.1515,  6.2538,  2.9292,  0.0604,  0.1798,  0.5974,  0.1583],\n",
      "       grad_fn=<AddBackward0>) tensor([12.9000,  7.4000,  4.7000,  0.0470,  0.1780,  0.5750,  0.2250]) tensor(1.3605, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.1403,  0.8050,  0.2474,  0.0424,  0.1123,  0.4480,  0.0769],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 1.0000, 1.3000, 0.0650, 0.0850, 0.0000, 0.3000]) tensor(0.2025, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5820, 2.3585, 0.7938, 0.0553, 0.1525, 0.4937, 0.1082],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 2.3000, 0.7000, 0.1430, 0.2350, 0.5710, 0.1250]) tensor(0.7486, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4577,  6.5040,  2.0179,  0.0772,  0.2064,  0.5836,  0.1227],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([12.7000, 13.8000,  1.2000,  0.1360,  0.3430,  0.6730,  0.0540]) tensor(7.7871, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.3924,  4.0215,  4.1818,  0.0209,  0.1106,  0.5454,  0.2457],\n",
      "       grad_fn=<AddBackward0>) tensor([8.7000, 2.3000, 1.4000, 0.0410, 0.0720, 0.5450, 0.0920]) tensor(7.9307, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3826,  3.0511,  3.4820,  0.0270,  0.1144,  0.5031,  0.2331],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 2.1000, 1.7000, 0.0260, 0.1050, 0.5130, 0.1410]) tensor(5.5277, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.0480, 0.2777, 1.8716, 0.0110, 0.0841, 0.4348, 0.1966],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 0.3000, 0.1000, 0.0000, 0.1180, 0.3370, 0.0670]) tensor(3.8100, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4911,  5.5061,  2.4477,  0.0526,  0.1656,  0.5761,  0.1466],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 2.9000, 0.7000, 0.0470, 0.1080, 0.5540, 0.0480]) tensor(4.8261, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4311,  7.2502,  1.8288,  0.0820,  0.2209,  0.6092,  0.1050],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 5.4000, 0.3000, 0.1360, 0.2140, 0.5310, 0.0290]) tensor(10.2700, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.8610,  4.1552,  3.5563,  0.0303,  0.1239,  0.5526,  0.2095],\n",
      "       grad_fn=<AddBackward0>) tensor([14.8000,  3.1000,  1.7000,  0.0240,  0.0980,  0.5620,  0.1040]) tensor(0.7790, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2519,  5.3478,  2.2004,  0.0628,  0.1792,  0.5676,  0.1380],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 5.2000, 1.2000, 0.1010, 0.1440, 0.5300, 0.0900]) tensor(1.0058, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1224, 3.5862, 1.4873, 0.0542, 0.1553, 0.5209, 0.1226],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 2.3000, 0.8000, 0.0290, 0.1200, 0.4560, 0.0700]) tensor(0.6811, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0818,  4.8894,  1.5765,  0.0586,  0.1678,  0.5642,  0.1042],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 3.3000, 0.8000, 0.0440, 0.1630, 0.4750, 0.0700]) tensor(5.0602, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5448,  2.6802,  3.8449,  0.0122,  0.0837,  0.5156,  0.2297],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 1.7000, 2.7000, 0.0400, 0.0940, 0.4300, 0.2950]) tensor(8.4583, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4285,  4.5785,  2.9021,  0.0383,  0.1406,  0.5653,  0.1762],\n",
      "       grad_fn=<AddBackward0>) tensor([12.8000,  4.1000,  1.0000,  0.0380,  0.1440,  0.5410,  0.0660]) tensor(0.6078, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.5948,  8.5164,  2.0504,  0.0935,  0.2496,  0.6356,  0.1138],\n",
      "       grad_fn=<AddBackward0>) tensor([16.2000, 14.8000,  0.8000,  0.1530,  0.3430,  0.4990,  0.0420]) tensor(6.1469, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.3588,  5.8825,  2.2642,  0.0712,  0.2020,  0.5760,  0.1517],\n",
      "       grad_fn=<AddBackward0>) tensor([10.4000,  5.9000,  1.5000,  0.0780,  0.2410,  0.5370,  0.1400]) tensor(2.3228, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0625, 2.4858, 2.7071, 0.0255, 0.0982, 0.5126, 0.1695],\n",
      "       grad_fn=<AddBackward0>) tensor([9.4000, 3.0000, 3.1000, 0.0380, 0.0860, 0.6590, 0.1960]) tensor(0.3186, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.2804, 1.4535, 1.2514, 0.0390, 0.1128, 0.4662, 0.1218],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 2.3000, 1.3000, 0.0580, 0.1180, 0.4640, 0.1340]) tensor(0.2695, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1254,  5.0165,  1.5245,  0.0656,  0.1857,  0.5517,  0.1199],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 3.4000, 0.4000, 0.0940, 0.1980, 0.4850, 0.0500]) tensor(6.0919, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.3183,  6.1567,  3.2888,  0.0541,  0.1769,  0.5984,  0.1892],\n",
      "       grad_fn=<AddBackward0>) tensor([16.7000,  9.5000,  2.5000,  0.0690,  0.2470,  0.5280,  0.1340]) tensor(1.7422, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.3356, 5.5816, 0.5717, 0.0802, 0.1924, 0.5667, 0.0324],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 4.0000, 0.6000, 0.0590, 0.1440, 0.4080, 0.0340]) tensor(2.9244, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.0745e+01, 4.6730e+00, 5.5981e+00, 6.4037e-03, 9.8444e-02, 5.8303e-01,\n",
      "        3.0782e-01], grad_fn=<AddBackward0>) tensor([22.1000,  4.8000,  9.0000,  0.0290,  0.1100,  0.5260,  0.3910]) tensor(1.9193, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4667, 2.6191, 2.5322, 0.0299, 0.1087, 0.5081, 0.1696],\n",
      "       grad_fn=<AddBackward0>) tensor([6.8000, 2.8000, 4.0000, 0.0170, 0.1020, 0.5440, 0.2670]) tensor(0.7108, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7688, 4.6471, 0.4280, 0.0843, 0.2098, 0.5235, 0.0794],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 2.5000, 0.4000, 0.0920, 0.1470, 0.4740, 0.0670]) tensor(2.6887, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5426,  2.3614,  2.5348,  0.0328,  0.1241,  0.5071,  0.1959],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 0.2000, 0.2000, 0.0000, 0.0450, 0.5710, 0.0770]) tensor(8.7378, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1340,  5.3404,  2.4658,  0.0544,  0.1685,  0.5684,  0.1525],\n",
      "       grad_fn=<AddBackward0>) tensor([10.5000,  5.6000,  0.7000,  0.1090,  0.1470,  0.4870,  0.0500]) tensor(1.4492, grad_fn=<MseLossBackward0>)\n",
      "tensor([22.5467,  6.2460,  5.0116,  0.0304,  0.1470,  0.6091,  0.2774],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1400e+01, 3.1000e+00, 3.6000e+00, 1.5000e-02, 9.2000e-02, 5.7200e-01,\n",
      "        1.9400e-01]) tensor(1.8881, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3413,  3.0707,  3.1748,  0.0226,  0.1115,  0.5091,  0.2199],\n",
      "       grad_fn=<AddBackward0>) tensor([13.3000,  3.1000,  5.2000,  0.0240,  0.1130,  0.5590,  0.3510]) tensor(0.7201, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.5073, 4.9304, 1.2312, 0.0675, 0.1783, 0.5489, 0.0885],\n",
      "       grad_fn=<AddBackward0>) tensor([6.7000, 4.5000, 1.3000, 0.0740, 0.1780, 0.5850, 0.0900]) tensor(0.4940, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9705,  4.4985,  3.1095,  0.0390,  0.1375,  0.5478,  0.1872],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1300e+01, 2.4000e+00, 7.1000e+00, 1.0000e-02, 9.0000e-02, 6.1600e-01,\n",
      "        3.9600e-01]) tensor(3.3100, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8098,  3.8765,  2.3220,  0.0444,  0.1429,  0.5306,  0.1611],\n",
      "       grad_fn=<AddBackward0>) tensor([8.8000, 4.2000, 2.6000, 0.0370, 0.1340, 0.5060, 0.1410]) tensor(0.6032, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.7467, 1.4264, 1.3720, 0.0412, 0.1221, 0.4621, 0.1424],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 1.8000, 0.4000, 0.0500, 0.1150, 0.3610, 0.0470]) tensor(0.5936, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.8528,  5.1934,  3.1537,  0.0483,  0.1584,  0.5735,  0.1862],\n",
      "       grad_fn=<AddBackward0>) tensor([19.3000,  5.8000,  3.0000,  0.0440,  0.1220,  0.5140,  0.1240]) tensor(2.8826, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1367, 2.4117, 1.8305, 0.0355, 0.1249, 0.5006, 0.1556],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 0.8000, 0.6000, 0.0500, 0.0520, 0.4580, 0.1280]) tensor(3.6599, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0610, 3.4156, 2.5744, 0.0345, 0.1188, 0.5252, 0.1591],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 2.6000, 1.7000, 0.0170, 0.0820, 0.5280, 0.0830]) tensor(0.3661, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2982,  3.8489,  2.5768,  0.0352,  0.1242,  0.5420,  0.1563],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 2.4000, 1.8000, 0.0200, 0.0970, 0.5450, 0.1160]) tensor(1.2780, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.3761e+01, 3.1259e+00, 5.3158e+00, 7.5003e-03, 1.2482e-01, 5.1715e-01,\n",
      "        3.8881e-01], grad_fn=<AddBackward0>) tensor([2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3470, 0.0000]) tensor(73.1064, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6010,  5.0493,  3.6469,  0.0353,  0.1316,  0.5785,  0.1931],\n",
      "       grad_fn=<AddBackward0>) tensor([10.5000,  4.1000,  2.7000,  0.0330,  0.1160,  0.5940,  0.1250]) tensor(2.6602, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5833, 3.4050, 0.9684, 0.0635, 0.1723, 0.4986, 0.1175],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 2.0000, 0.1000, 0.0850, 0.2500, 0.4110, 0.0300]) tensor(4.8462, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6375,  3.2564,  2.5813,  0.0378,  0.1321,  0.5157,  0.1831],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 2.1000, 1.2000, 0.0150, 0.1080, 0.5580, 0.0990]) tensor(0.5652, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 3.3901,  2.8863, -0.3040,  0.0767,  0.1875,  0.4870,  0.0580],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 1.9000, 0.3000, 0.1320, 0.2220, 0.5550, 0.0550]) tensor(0.3621, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.2398, 1.7757, 0.9176, 0.0437, 0.1284, 0.4786, 0.1120],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 3.4000, 1.1000, 0.0280, 0.1750, 0.5050, 0.0760]) tensor(0.6466, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9959, 4.2766, 0.2466, 0.0822, 0.2060, 0.5087, 0.0792],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 2.1000, 0.5000, 0.1280, 0.1380, 0.3890, 0.1030]) tensor(3.5766, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.1835,  5.2200,  3.9659,  0.0381,  0.1462,  0.5606,  0.2361],\n",
      "       grad_fn=<AddBackward0>) tensor([14.7000,  5.4000,  2.9000,  0.0890,  0.1170,  0.5780,  0.1540]) tensor(1.0495, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.1057, 4.4498, 1.0325, 0.0687, 0.1774, 0.5299, 0.0868],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.6000, 0.3000, 0.0920, 0.1250, 0.4310, 0.0550]) tensor(4.8185, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2439,  4.3623,  2.2805,  0.0445,  0.1465,  0.5488,  0.1491],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 2.6000, 0.6000, 0.0420, 0.1340, 0.5040, 0.0620]) tensor(6.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6837,  3.1922,  3.2165,  0.0213,  0.1032,  0.5372,  0.1965],\n",
      "       grad_fn=<AddBackward0>) tensor([10.6000,  4.4000, 11.2000,  0.0450,  0.0960,  0.4950,  0.4480]) tensor(9.4907, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4209,  5.8093,  1.6814,  0.0711,  0.1995,  0.5689,  0.1208],\n",
      "       grad_fn=<AddBackward0>) tensor([17.5000,  7.0000,  3.2000,  0.0640,  0.1550,  0.5080,  0.1480]) tensor(4.2182, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9541,  4.5999,  2.2558,  0.0529,  0.1586,  0.5414,  0.1481],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 3.1000, 1.9000, 0.0390, 0.1160, 0.4720, 0.1400]) tensor(1.7616, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4675,  6.1760,  1.4372,  0.0790,  0.2014,  0.5884,  0.0791],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 2.6000, 0.3000, 0.1000, 0.1480, 0.5230, 0.0380]) tensor(9.3519, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7031, 2.1034, 3.1115, 0.0111, 0.0798, 0.5100, 0.1999],\n",
      "       grad_fn=<AddBackward0>) tensor([10.4000,  3.1000,  4.8000,  0.0180,  0.0910,  0.5260,  0.2390]) tensor(0.6188, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.6427, 0.1198, 2.0469, 0.0102, 0.0932, 0.4071, 0.2362],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 1.7000, 0.0000, 0.1430, 0.1670, 0.1380, 0.0000]) tensor(7.2805, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3499, 4.2821, 1.1718, 0.0626, 0.1715, 0.5315, 0.1023],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 7.0000, 0.6000, 0.1220, 0.1600, 0.7260, 0.0390]) tensor(1.3689, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.7336,  2.4198, -0.3887,  0.0687,  0.1691,  0.4720,  0.0504],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 1.3000, 0.0000, 0.0480, 0.2220, 0.4600, 0.0000]) tensor(0.2284, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9126, 3.1576, 1.1312, 0.0578, 0.1639, 0.5161, 0.1197],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 2.3000, 0.0000, 0.1850, 0.2810, 0.7460, 0.0000]) tensor(3.3411, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.6778, 0.9831, 1.4760, 0.0296, 0.1026, 0.4644, 0.1462],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 1.5000, 0.9000, 0.0190, 0.0990, 0.4820, 0.1230]) tensor(0.2517, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3460,  4.9861,  2.7421,  0.0482,  0.1516,  0.5610,  0.1590],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 1.9000, 0.6000, 0.0500, 0.0880, 0.5390, 0.0560]) tensor(8.3282, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4415, 3.5217, 1.8481, 0.0431, 0.1344, 0.5278, 0.1294],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 1.1000, 2.6000, 0.0150, 0.0890, 0.5230, 0.2760]) tensor(4.4104, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.1075,  4.6060,  4.4073,  0.0257,  0.1242,  0.5588,  0.2565],\n",
      "       grad_fn=<AddBackward0>) tensor([10.0000,  1.9000,  0.8000,  0.0250,  0.0780,  0.5660,  0.0670]) tensor(10.1271, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7807,  2.5908,  3.6407,  0.0162,  0.0943,  0.5151,  0.2305],\n",
      "       grad_fn=<AddBackward0>) tensor([9.5000, 1.5000, 3.7000, 0.0130, 0.0670, 0.5120, 0.3310]) tensor(0.9151, grad_fn=<MseLossBackward0>)\n",
      "tensor([-1.0485,  1.6812, -0.3273,  0.0634,  0.1480,  0.4580,  0.0419],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 1.4000, 0.0000, 0.0730, 0.0980, 0.5950, 0.0000]) tensor(0.8863, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.0637, 1.1746, 1.1574, 0.0336, 0.1103, 0.4625, 0.1312],\n",
      "       grad_fn=<AddBackward0>) tensor([0.5000, 0.2000, 0.1000, 0.0000, 0.0490, 0.2020, 0.0380]) tensor(2.1213, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0010,  4.0672,  1.7502,  0.0512,  0.1569,  0.5366,  0.1337],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4000, 1.0000, 0.2000, 0.0510, 0.1460, 0.5050, 0.0640]) tensor(9.9418, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5389,  4.8287,  2.9697,  0.0483,  0.1552,  0.5611,  0.1794],\n",
      "       grad_fn=<AddBackward0>) tensor([14.5000,  6.5000,  5.9000,  0.0400,  0.1410,  0.5300,  0.2480]) tensor(1.7586, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1403,  3.6317,  3.1325,  0.0352,  0.1326,  0.5228,  0.2129],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 1.3000, 1.7000, 0.0330, 0.0940, 0.5260, 0.2250]) tensor(11.9834, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.8920,  1.8040, -0.0714,  0.0577,  0.1425,  0.4734,  0.0546],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 0.9000, 1.3000, 0.0310, 0.0900, 0.5450, 0.2930]) tensor(0.4880, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7095,  3.3377,  3.7858,  0.0218,  0.1081,  0.5362,  0.2317],\n",
      "       grad_fn=<AddBackward0>) tensor([13.3000,  4.7000,  5.8000,  0.0300,  0.1180,  0.4630,  0.3080]) tensor(0.8702, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.7615, -0.0772,  1.6241,  0.0182,  0.0781,  0.4336,  0.1615],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 3.0000, 1.0000, 0.0560, 0.0310, 0.4050, 0.0530]) tensor(5.3311, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.3974,  2.9718, -0.4742,  0.0776,  0.1786,  0.4985,  0.0244],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 2.5000, 0.4000, 0.0890, 0.1450, 0.4950, 0.0370]) tensor(0.5553, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.7538,  5.7071,  3.4673,  0.0513,  0.1735,  0.5809,  0.2129],\n",
      "       grad_fn=<AddBackward0>) tensor([19.2000,  3.8000,  7.9000,  0.0220,  0.1040,  0.5150,  0.3910]) tensor(3.6313, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1460,  3.9798,  2.6547,  0.0358,  0.1281,  0.5472,  0.1625],\n",
      "       grad_fn=<AddBackward0>) tensor([8.5000, 2.6000, 3.1000, 0.0210, 0.0920, 0.5120, 0.1990]) tensor(1.3011, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5856, 3.2548, 1.1286, 0.0569, 0.1569, 0.4974, 0.1151],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 1.8000, 0.7000, 0.0160, 0.0950, 0.5910, 0.0680]) tensor(0.3426, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3599, 3.9454, 1.4599, 0.0559, 0.1575, 0.5320, 0.1125],\n",
      "       grad_fn=<AddBackward0>) tensor([6.8000, 3.0000, 1.7000, 0.0350, 0.1250, 0.5160, 0.1200]) tensor(0.4838, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4710, 5.5149, 0.6694, 0.0816, 0.2047, 0.5590, 0.0614],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 7.5000, 0.7000, 0.1410, 0.2370, 0.5120, 0.0490]) tensor(1.0641, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5005, 1.5651, 1.2770, 0.0374, 0.1237, 0.4707, 0.1425],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 0.7000, 0.3000, 0.0650, 0.0430, 0.5290, 0.0770]) tensor(0.8757, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.2711,  5.1078,  3.3435,  0.0381,  0.1435,  0.5733,  0.1942],\n",
      "       grad_fn=<AddBackward0>) tensor([13.8000,  3.3000,  2.5000,  0.0220,  0.0870,  0.5190,  0.1110]) tensor(0.8796, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5494,  3.8121,  2.7473,  0.0426,  0.1473,  0.5272,  0.1950],\n",
      "       grad_fn=<AddBackward0>) tensor([15.0000,  3.9000,  1.2000,  0.0250,  0.1380,  0.5170,  0.0820]) tensor(1.2030, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.6892,  6.1236,  2.8191,  0.0624,  0.1875,  0.5905,  0.1670],\n",
      "       grad_fn=<AddBackward0>) tensor([12.6000,  6.7000,  2.9000,  0.0690,  0.2040,  0.5380,  0.1840]) tensor(1.4122, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7568,  4.0524,  3.4813,  0.0311,  0.1205,  0.5462,  0.2006],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 1.6000, 3.5000, 0.0150, 0.0550, 0.4910, 0.2330]) tensor(3.3289, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.1515,  6.0139,  3.7127,  0.0499,  0.1671,  0.5941,  0.2050],\n",
      "       grad_fn=<AddBackward0>) tensor([18.3000,  6.5000,  1.8000,  0.0250,  0.2020,  0.5550,  0.0960]) tensor(0.7470, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.6934,  7.5046,  3.6698,  0.0654,  0.2010,  0.6267,  0.1940],\n",
      "       grad_fn=<AddBackward0>) tensor([17.9000, 10.1000,  3.2000,  0.1090,  0.2650,  0.5600,  0.1750]) tensor(1.4548, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0027, 3.4407, 1.9268, 0.0488, 0.1420, 0.5218, 0.1341],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 2.7000, 1.3000, 0.0120, 0.1160, 0.5430, 0.0760]) tensor(0.2274, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4272,  5.9676,  2.3190,  0.0715,  0.2013,  0.5663,  0.1572],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 4.8000, 1.0000, 0.1010, 0.1850, 0.5260, 0.1000]) tensor(6.5305, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.6181,  4.7859,  3.2355,  0.0372,  0.1461,  0.5601,  0.2058],\n",
      "       grad_fn=<AddBackward0>) tensor([6.7000, 1.0000, 1.4000, 0.0080, 0.0690, 0.4770, 0.1780]) tensor(13.8926, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0770,  5.1823,  2.7212,  0.0525,  0.1560,  0.5663,  0.1506],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 2.5000, 0.8000, 0.0220, 0.1370, 0.5770, 0.0730]) tensor(7.1849, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.1946, 1.9693, 0.6555, 0.0470, 0.1311, 0.4812, 0.0911],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 2.2000, 0.2000, 0.0570, 0.1740, 0.4360, 0.0300]) tensor(0.1797, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7692, 2.8197, 2.0535, 0.0370, 0.1219, 0.5100, 0.1476],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 1.7000, 1.3000, 0.0310, 0.1200, 0.4930, 0.1570]) tensor(2.2899, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8304,  7.0601,  1.7946,  0.0849,  0.2177,  0.5930,  0.0995],\n",
      "       grad_fn=<AddBackward0>) tensor([6.8000, 5.7000, 0.7000, 0.0990, 0.2240, 0.4680, 0.0550]) tensor(5.6331, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5963,  4.0049,  3.1132,  0.0369,  0.1386,  0.5360,  0.2066],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0500e+01, 1.7000e+00, 1.6000e+00, 1.0000e-02, 8.5000e-02, 5.3000e-01,\n",
      "        1.3700e-01]) tensor(2.4569, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.2061,  4.4585,  3.7037,  0.0264,  0.1286,  0.5591,  0.2290],\n",
      "       grad_fn=<AddBackward0>) tensor([20.6000,  4.9000,  5.5000,  0.0340,  0.1060,  0.4850,  0.2580]) tensor(3.2478, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9507, 1.5870, 2.4868, 0.0170, 0.0936, 0.4959, 0.1919],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 1.1000, 1.3000, 0.0230, 0.1170, 0.5030, 0.2360]) tensor(4.6369, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.8364, 5.6730, 0.8756, 0.0825, 0.2123, 0.5593, 0.0803],\n",
      "       grad_fn=<AddBackward0>) tensor([7.4000, 5.8000, 1.4000, 0.0900, 0.1760, 0.4320, 0.0960]) tensor(0.8922, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.2787,  6.0899,  2.9145,  0.0545,  0.1719,  0.5979,  0.1598],\n",
      "       grad_fn=<AddBackward0>) tensor([14.8000,  9.2000,  5.5000,  0.0670,  0.2000,  0.5580,  0.2200]) tensor(2.3704, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.0792, 2.7540, 0.5450, 0.0591, 0.1617, 0.4811, 0.1000],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 0.6000, 0.2000, 0.0140, 0.1670, 0.3650, 0.1030]) tensor(2.8318, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6451,  3.0048,  2.7886,  0.0322,  0.1208,  0.5238,  0.1872],\n",
      "       grad_fn=<AddBackward0>) tensor([12.1000,  3.2000,  0.6000,  0.0160,  0.1240,  0.5660,  0.0450]) tensor(0.9953, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3388, 3.3438, 1.6892, 0.0424, 0.1318, 0.5260, 0.1194],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 1.2000, 0.6000, 0.0450, 0.1100, 0.5180, 0.1130]) tensor(3.7691, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7892,  5.9175,  1.3520,  0.0741,  0.1968,  0.5750,  0.0891],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 3.4000, 0.6000, 0.0580, 0.1450, 0.5610, 0.0470]) tensor(2.4398, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.5724, 1.7109, 0.2931, 0.0547, 0.1448, 0.4567, 0.0928],\n",
      "       grad_fn=<AddBackward0>) tensor([0.6000, 0.7000, 0.2000, 0.0430, 0.1430, 0.4570, 0.1070]) tensor(0.7030, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7682,  5.2900,  1.8157,  0.0584,  0.1746,  0.5679,  0.1215],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 6.2000, 1.7000, 0.0800, 0.1780, 0.5060, 0.1000]) tensor(0.7320, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1646,  6.9071,  0.9479,  0.0955,  0.2407,  0.5865,  0.0772],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 2.4000, 0.3000, 0.1020, 0.1570, 0.5330, 0.0540]) tensor(10.9235, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3466, 1.9394, 2.0126, 0.0355, 0.1239, 0.4916, 0.1738],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 0.6000, 0.9000, 0.0430, 0.1500, 0.3220, 0.3750]) tensor(8.1534, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1335,  2.7366,  2.8583,  0.0319,  0.1183,  0.5123,  0.1940],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 4.9000, 1.0000, 0.1080, 0.1690, 0.4740, 0.0940]) tensor(1.5941, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.0405,  2.6314, -0.4076,  0.0706,  0.1718,  0.4927,  0.0398],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 2.2000, 0.2000, 0.1090, 0.2530, 0.4630, 0.0460]) tensor(0.0842, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9190e+00, 1.1640e+00, 3.3928e+00, 6.2321e-03, 6.7850e-02, 4.8355e-01,\n",
      "        2.2689e-01], grad_fn=<AddBackward0>) tensor([6.7000e+00, 2.0000e+00, 2.7000e+00, 2.0000e-03, 1.0900e-01, 4.7300e-01,\n",
      "        2.2100e-01]) tensor(0.8721, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5181,  2.0774,  3.9285,  0.0223,  0.1133,  0.5166,  0.2798],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 0.0000, 2.0000, 0.0000, 0.0000, 1.0640, 0.6670]) tensor(20.1662, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3322, 4.1218, 1.5700, 0.0513, 0.1499, 0.5387, 0.1093],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 6.1000, 1.0000, 0.1360, 0.2330, 0.4920, 0.0830]) tensor(2.1941, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3153,  4.9013,  2.2235,  0.0505,  0.1547,  0.5666,  0.1321],\n",
      "       grad_fn=<AddBackward0>) tensor([10.0000,  4.4000,  1.1000,  0.0620,  0.1070,  0.5700,  0.0560]) tensor(0.4645, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8164,  6.1658,  1.7758,  0.0710,  0.1929,  0.5801,  0.1048],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 9.4000, 1.4000, 0.1260, 0.2360, 0.5370, 0.0770]) tensor(2.0964, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.6334, 3.4799, 1.4478, 0.0519, 0.1500, 0.5171, 0.1192],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 1.3000, 0.8000, 0.0170, 0.1020, 0.5220, 0.1110]) tensor(2.5229, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6792, 4.0425, 1.8535, 0.0517, 0.1485, 0.5375, 0.1222],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 5.4000, 1.7000, 0.0520, 0.1160, 0.6090, 0.0700]) tensor(0.4473, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5952, 5.8977, 1.1810, 0.0743, 0.1935, 0.5781, 0.0711],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 7.7000, 1.0000, 0.1170, 0.2460, 0.5960, 0.0630]) tensor(1.9279, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.3615,  2.9219, -0.3862,  0.0747,  0.1754,  0.4938,  0.0309],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 1.2000, 0.0000, 0.1400, 0.1650, 0.3320, 0.0180]) tensor(0.4797, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4457, 1.9462, 1.4333, 0.0362, 0.1168, 0.4855, 0.1308],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 1.1000, 0.5000, 0.0150, 0.1150, 0.5180, 0.0780]) tensor(1.0818, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5685, 2.7841, 1.2697, 0.0499, 0.1403, 0.5049, 0.1106],\n",
      "       grad_fn=<AddBackward0>) tensor([3.9000, 3.6000, 1.0000, 0.0810, 0.2160, 0.5590, 0.1370]) tensor(0.5047, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5686,  2.5968,  2.8821,  0.0239,  0.1054,  0.5107,  0.1968],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 3.4000, 7.7000, 0.0170, 0.1180, 0.4950, 0.4420]) tensor(3.5508, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5622, 2.4459, 1.4659, 0.0460, 0.1432, 0.4902, 0.1475],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 0.9000, 0.2000, 0.1150, 0.1130, 0.4320, 0.0940]) tensor(5.1522, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8257,  3.5140,  3.4317,  0.0235,  0.1101,  0.5254,  0.2159],\n",
      "       grad_fn=<AddBackward0>) tensor([11.1000,  3.1000,  7.2000,  0.0160,  0.0780,  0.5690,  0.2790]) tensor(2.4795, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1579,  4.2448,  3.2321,  0.0313,  0.1251,  0.5611,  0.1877],\n",
      "       grad_fn=<AddBackward0>) tensor([7.7000, 2.8000, 1.3000, 0.0350, 0.1260, 0.5050, 0.1080]) tensor(5.0884, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9311, 2.6569, 1.9839, 0.0303, 0.1131, 0.5148, 0.1452],\n",
      "       grad_fn=<AddBackward0>) tensor([12.2000,  4.1000,  3.8000,  0.0190,  0.1060,  0.5360,  0.1710]) tensor(3.3723, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1857,  5.9921,  2.1665,  0.0683,  0.1860,  0.5826,  0.1200],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 3.3000, 1.0000, 0.0330, 0.1920, 0.5860, 0.0860]) tensor(6.5209, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0038,  5.6618,  1.4410,  0.0707,  0.1877,  0.5699,  0.0898],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([10.7000,  8.9000,  0.8000,  0.1160,  0.2350,  0.5810,  0.0410]) tensor(1.6269, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 3.2261,  3.8829, -0.1752,  0.0819,  0.1913,  0.5159,  0.0319],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 4.8000, 0.6000, 0.0980, 0.2040, 0.6010, 0.0500]) tensor(1.0122, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.7975,  4.3588,  4.0292,  0.0252,  0.1193,  0.5625,  0.2318],\n",
      "       grad_fn=<AddBackward0>) tensor([13.0000,  4.7000,  8.8000,  0.0230,  0.1280,  0.5350,  0.3770]) tensor(4.3893, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1911,  4.5803,  2.9324,  0.0429,  0.1397,  0.5548,  0.1694],\n",
      "       grad_fn=<AddBackward0>) tensor([11.9000,  3.7000,  2.2000,  0.0220,  0.0890,  0.5610,  0.0850]) tensor(0.2009, grad_fn=<MseLossBackward0>)\n",
      "tensor([-4.0269,  0.7363, -0.7608,  0.0564,  0.1280,  0.4373,  0.0216],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 3.5000, 0.0000, 0.0570, 0.1350, 0.5250, 0.0000]) tensor(7.2608, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3124, 3.8037, 2.2375, 0.0431, 0.1369, 0.5315, 0.1455],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.8000, 0.2000, 0.0950, 0.1890, 0.4810, 0.0440]) tensor(8.6005, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9459, 4.6220, 1.5549, 0.0639, 0.1718, 0.5376, 0.1118],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 7.3000, 0.6000, 0.1080, 0.2470, 0.6030, 0.0350]) tensor(1.2851, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.5929,  4.2215,  4.2057,  0.0233,  0.1200,  0.5571,  0.2505],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000e+01, 2.0000e+00, 3.2000e+00, 1.1000e-02, 7.6000e-02, 5.0600e-01,\n",
      "        2.0900e-01]) tensor(1.8108, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0398,  3.6112,  3.1763,  0.0247,  0.1100,  0.5366,  0.1932],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 1.4000, 3.2000, 0.0210, 0.0880, 0.3900, 0.3050]) tensor(12.8998, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.4246, 3.0738, 0.4141, 0.0601, 0.1520, 0.5061, 0.0600],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 2.4000, 0.1000, 0.1210, 0.1730, 0.4770, 0.0210]) tensor(0.6090, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.8579, 1.5537, 0.9616, 0.0413, 0.1215, 0.4825, 0.1098],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 2.1000, 0.5000, 0.0220, 0.1520, 0.3520, 0.0630]) tensor(0.3020, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0227, 2.8305, 1.7980, 0.0394, 0.1350, 0.5166, 0.1504],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 1.3000, 0.6000, 0.0360, 0.0960, 0.5410, 0.0930]) tensor(2.6280, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1735,  6.5908,  1.5055,  0.0827,  0.2193,  0.5930,  0.1004],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 5.9000, 1.5000, 0.0930, 0.1880, 0.5510, 0.1030]) tensor(1.8928, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3349,  3.4085,  2.1347,  0.0457,  0.1451,  0.5170,  0.1646],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 1.3000, 0.4000, 0.0750, 0.1250, 0.4950, 0.0750]) tensor(9.8359, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0783, 4.0122, 0.2968, 0.0790, 0.2009, 0.5223, 0.0781],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 0.9000, 0.2000, 0.0170, 0.1590, 0.6140, 0.0500]) tensor(3.2163, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.7753,  4.8766,  3.2130,  0.0446,  0.1531,  0.5518,  0.2018],\n",
      "       grad_fn=<AddBackward0>) tensor([11.6000,  3.9000,  2.0000,  0.0790,  0.0820,  0.5580,  0.1200]) tensor(1.7886, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.8013,  4.5651,  3.0315,  0.0373,  0.1457,  0.5539,  0.1996],\n",
      "       grad_fn=<AddBackward0>) tensor([13.0000,  3.2000,  3.0000,  0.0410,  0.0990,  0.4930,  0.2100]) tensor(0.7308, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1874,  3.2390,  2.7130,  0.0316,  0.1186,  0.5177,  0.1807],\n",
      "       grad_fn=<AddBackward0>) tensor([11.2000,  3.3000,  0.9000,  0.0230,  0.1010,  0.5410,  0.0470]) tensor(0.6193, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9861, 2.3072, 1.2754, 0.0457, 0.1348, 0.4937, 0.1245],\n",
      "       grad_fn=<AddBackward0>) tensor([6.2000, 2.1000, 1.3000, 0.0300, 0.0860, 0.5950, 0.1220]) tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.1626,  7.8950,  2.6127,  0.0813,  0.2290,  0.6210,  0.1489],\n",
      "       grad_fn=<AddBackward0>) tensor([20.0000, 10.3000,  3.3000,  0.1150,  0.1900,  0.5670,  0.1570]) tensor(1.3768, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.9530, 1.8167, 0.6996, 0.0496, 0.1322, 0.4734, 0.0966],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 2.8000, 0.6000, 0.0860, 0.1280, 0.5330, 0.0600]) tensor(0.4396, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.5388, 2.1331, 1.1718, 0.0540, 0.1333, 0.5301, 0.0887],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 0.8000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000]) tensor(3.4354, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.8555,  5.4049,  3.7983,  0.0387,  0.1417,  0.5882,  0.2020],\n",
      "       grad_fn=<AddBackward0>) tensor([11.3000,  3.0000,  2.3000,  0.0220,  0.1070,  0.6070,  0.1350]) tensor(4.1125, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3211, 3.8542, 1.3519, 0.0588, 0.1628, 0.5257, 0.1136],\n",
      "       grad_fn=<AddBackward0>) tensor([6.8000, 2.3000, 0.8000, 0.0300, 0.0660, 0.5060, 0.0450]) tensor(0.7213, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7433,  4.8691,  2.5259,  0.0515,  0.1549,  0.5546,  0.1517],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 1.9000, 0.4000, 0.0120, 0.1020, 0.6020, 0.0320]) tensor(5.6871, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.2176,  5.6171,  3.4987,  0.0465,  0.1541,  0.5845,  0.1882],\n",
      "       grad_fn=<AddBackward0>) tensor([9.4000, 4.8000, 0.7000, 0.0750, 0.1670, 0.5610, 0.0510]) tensor(6.0521, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.8864,  4.3473,  3.9175,  0.0288,  0.1328,  0.5416,  0.2523],\n",
      "       grad_fn=<AddBackward0>) tensor([11.3000,  2.1000,  1.5000,  0.0270,  0.0760,  0.4580,  0.1110]) tensor(6.0190, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2894,  3.9081,  2.9191,  0.0337,  0.1169,  0.5469,  0.1613],\n",
      "       grad_fn=<AddBackward0>) tensor([9.4000, 4.3000, 4.7000, 0.0260, 0.1070, 0.5900, 0.1850]) tensor(0.5884, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.7069, 0.4290, 1.3343, 0.0247, 0.0928, 0.4477, 0.1480],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 0.9000, 0.5000, 0.0380, 0.1450, 0.3820, 0.1060]) tensor(0.9600, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7469,  3.6693,  2.9937,  0.0264,  0.1134,  0.5435,  0.1822],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 1.6000, 1.5000, 0.0110, 0.0880, 0.5350, 0.1390]) tensor(3.8844, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4462,  5.5680,  2.6653,  0.0541,  0.1650,  0.5833,  0.1489],\n",
      "       grad_fn=<AddBackward0>) tensor([10.4000,  6.7000,  0.8000,  0.1050,  0.1910,  0.5660,  0.0520]) tensor(2.0076, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4760,  3.0472,  2.0640,  0.0421,  0.1441,  0.5074,  0.1751],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 1.2000, 0.1000, 0.0570, 0.1260, 0.4030, 0.0310]) tensor(9.6811, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2234,  2.6529,  3.6813,  0.0136,  0.0966,  0.5070,  0.2473],\n",
      "       grad_fn=<AddBackward0>) tensor([6.8000, 2.3000, 2.5000, 0.0260, 0.1110, 0.4360, 0.2280]) tensor(6.1122, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5097,  3.9173,  3.1805,  0.0334,  0.1212,  0.5359,  0.1872],\n",
      "       grad_fn=<AddBackward0>) tensor([9.9000, 2.5000, 2.9000, 0.0120, 0.0750, 0.5460, 0.1470]) tensor(0.6690, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7668,  4.7798,  2.5533,  0.0465,  0.1516,  0.5668,  0.1559],\n",
      "       grad_fn=<AddBackward0>) tensor([12.1000,  5.6000,  2.4000,  0.0330,  0.1750,  0.5550,  0.1330]) tensor(0.1632, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8500, 0.8922, 0.3182, 0.0400, 0.1151, 0.4549, 0.0861],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4000, 2.3000, 0.2000, 0.0920, 0.1750, 0.5540, 0.0370]) tensor(0.6310, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8683,  5.5961,  1.8765,  0.0611,  0.1767,  0.5808,  0.1131],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 4.2000, 0.9000, 0.0780, 0.1440, 0.5620, 0.0760]) tensor(4.3801, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7916,  5.1937,  2.5867,  0.0532,  0.1627,  0.5724,  0.1512],\n",
      "       grad_fn=<AddBackward0>) tensor([11.2000,  4.5000,  1.7000,  0.0190,  0.1580,  0.5930,  0.0970]) tensor(0.5436, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1677,  4.7558,  2.0674,  0.0528,  0.1558,  0.5588,  0.1242],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 3.1000, 1.6000, 0.0180, 0.1260, 0.5410, 0.1090]) tensor(1.1579, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4002,  4.8362,  2.9710,  0.0405,  0.1423,  0.5597,  0.1761],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 3.5000, 2.5000, 0.0230, 0.1310, 0.4900, 0.1610]) tensor(5.0935, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.2969,  7.2808,  3.3396,  0.0639,  0.1997,  0.6219,  0.1850],\n",
      "       grad_fn=<AddBackward0>) tensor([20.4000,  9.6000,  1.0000,  0.1030,  0.2280,  0.6370,  0.0490]) tensor(1.7272, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5419, 5.7005, 1.2149, 0.0738, 0.1922, 0.5661, 0.0801],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 5.4000, 0.4000, 0.1450, 0.2500, 0.4710, 0.0400]) tensor(6.4126, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.9600, 1.5496, 0.6909, 0.0432, 0.1262, 0.4636, 0.1049],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000, 0.5000, 0.1000, 0.0180, 0.1050, 0.4000, 0.0670]) tensor(0.7024, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5827,  3.4096,  2.4851,  0.0370,  0.1303,  0.5217,  0.1730],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 1.1000, 1.3000, 0.0160, 0.0680, 0.4980, 0.1610]) tensor(5.7405, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3259, 3.7671, 1.4008, 0.0492, 0.1443, 0.5346, 0.1018],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 2.1000, 0.5000, 0.0800, 0.1350, 0.4260, 0.0590]) tensor(3.3134, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5022,  4.1638,  3.1089,  0.0348,  0.1344,  0.5502,  0.1955],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8500e+01, 2.6000e+00, 3.2000e+00, 8.0000e-03, 7.4000e-02, 5.9500e-01,\n",
      "        1.6000e-01]) tensor(3.9199, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4057,  5.9448,  3.1052,  0.0487,  0.1672,  0.5886,  0.1817],\n",
      "       grad_fn=<AddBackward0>) tensor([17.2000,  3.8000,  1.8000,  0.0300,  0.1040,  0.5300,  0.0850]) tensor(0.9931, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.8860, 3.1810, 0.0805, 0.0664, 0.1696, 0.5033, 0.0585],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 3.4000, 0.5000, 0.0860, 0.1890, 0.4490, 0.0520]) tensor(0.0864, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3898,  5.6651,  1.9465,  0.0675,  0.1842,  0.5740,  0.1154],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 4.8000, 1.2000, 0.1010, 0.1700, 0.5650, 0.0860]) tensor(2.0278, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7327,  4.4346,  2.8169,  0.0343,  0.1318,  0.5621,  0.1683],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 2.3000, 0.8000, 0.0460, 0.1050, 0.4630, 0.0790]) tensor(9.5573, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0329,  3.9627,  2.1270,  0.0514,  0.1573,  0.5378,  0.1551],\n",
      "       grad_fn=<AddBackward0>) tensor([12.5000,  3.9000,  2.4000,  0.0340,  0.1310,  0.5080,  0.1550]) tensor(0.3190, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.8659,  4.9043,  2.7169,  0.0488,  0.1650,  0.5666,  0.1816],\n",
      "       grad_fn=<AddBackward0>) tensor([19.9000,  5.3000,  3.8000,  0.0330,  0.1280,  0.5490,  0.1760]) tensor(3.8106, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.6255,  5.0449,  3.2722,  0.0394,  0.1477,  0.5731,  0.1968],\n",
      "       grad_fn=<AddBackward0>) tensor([7.7000, 1.9000, 2.3000, 0.0230, 0.0910, 0.4820, 0.1930]) tensor(10.5229, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5085,  6.3830,  1.6071,  0.0839,  0.2162,  0.5891,  0.1014],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 5.7000, 0.6000, 0.1040, 0.1800, 0.4460, 0.0410]) tensor(5.3727, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5300, 3.6351, 2.0229, 0.0464, 0.1442, 0.5274, 0.1456],\n",
      "       grad_fn=<AddBackward0>) tensor([9.5000, 3.5000, 1.9000, 0.0210, 0.1320, 0.5400, 0.1210]) tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5681,  3.0174,  3.1887,  0.0267,  0.1188,  0.5057,  0.2260],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000e+01, 2.6000e+00, 6.6000e+00, 1.3000e-02, 8.6000e-02, 5.0300e-01,\n",
      "        3.6100e-01]) tensor(1.9830, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4146,  4.0235,  3.0881,  0.0320,  0.1242,  0.5411,  0.1887],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 1.5000, 1.1000, 0.0200, 0.1170, 0.4840, 0.1610]) tensor(12.3241, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0383, 2.2543, 1.5352, 0.0420, 0.1325, 0.4833, 0.1486],\n",
      "       grad_fn=<AddBackward0>) tensor([9.4000, 2.2000, 2.6000, 0.0180, 0.0930, 0.6170, 0.1820]) tensor(0.9622, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7174, 4.5217, 1.4282, 0.0643, 0.1791, 0.5312, 0.1213],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 2.8000, 0.6000, 0.0810, 0.1550, 0.4990, 0.0770]) tensor(5.1918, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5463, 3.6141, 2.1058, 0.0393, 0.1319, 0.5348, 0.1427],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 1.1000, 0.5000, 0.0230, 0.0950, 0.5090, 0.0770]) tensor(7.0259, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.0683, 2.4592, 0.3257, 0.0579, 0.1574, 0.4737, 0.0910],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 0.7000, 0.1000, 0.0950, 0.1540, 0.4370, 0.0690]) tensor(1.8839, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.2657, 3.4431, 1.1640, 0.0636, 0.1672, 0.5231, 0.1068],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 2.0000, 0.0000, 0.2050, 0.1330, 0.4320, 0.0000]) tensor(4.9221, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1453,  1.6899,  3.2190,  0.0167,  0.0973,  0.4877,  0.2379],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 0.6000, 0.6000, 0.0320, 0.0620, 0.4140, 0.2140]) tensor(11.0997, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4870, 1.5934, 2.7854, 0.0157, 0.0809, 0.4865, 0.1893],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 1.0000, 1.4000, 0.0120, 0.0870, 0.5370, 0.2060]) tensor(1.8683, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7133,  5.2649,  1.5950,  0.0736,  0.1992,  0.5469,  0.1288],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1800e+01, 1.0000e+01, 1.0000e-01, 1.5400e-01, 3.3000e-01, 6.1900e-01,\n",
      "        9.0000e-03]) tensor(3.5295, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7713,  4.7198,  2.5919,  0.0467,  0.1522,  0.5590,  0.1624],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7500e+01, 2.6000e+00, 2.9000e+00, 1.6000e-02, 6.1000e-02, 5.9700e-01,\n",
      "        1.2800e-01]) tensor(3.8516, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.6914, 3.3545, 1.1721, 0.0567, 0.1572, 0.5025, 0.1144],\n",
      "       grad_fn=<AddBackward0>) tensor([11.1000,  8.5000,  1.2000,  0.0770,  0.2430,  0.5430,  0.0630]) tensor(6.5606, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.2217, 2.7992, 0.9462, 0.0561, 0.1643, 0.4777, 0.1336],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 2.3000, 0.3000, 0.1130, 0.2500, 0.3190, 0.0710]) tensor(2.6470, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.4264,  3.6496,  3.1334,  0.0323,  0.1191,  0.5378,  0.1874],\n",
      "       grad_fn=<AddBackward0>) tensor([14.2000,  4.8000,  8.7000,  0.0190,  0.1140,  0.5620,  0.3290]) tensor(5.7177, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.4020,  5.3960,  4.0114,  0.0384,  0.1525,  0.5808,  0.2376],\n",
      "       grad_fn=<AddBackward0>) tensor([19.3000,  4.3000,  3.7000,  0.0330,  0.0990,  0.4880,  0.1860]) tensor(0.3027, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5208,  4.5711,  3.8675,  0.0290,  0.1246,  0.5651,  0.2205],\n",
      "       grad_fn=<AddBackward0>) tensor([8.1000e+00, 1.4000e+00, 1.8000e+00, 6.0000e-03, 7.9000e-02, 5.7200e-01,\n",
      "        1.7700e-01]) tensor(9.9147, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0352,  3.2153,  2.5906,  0.0355,  0.1243,  0.5122,  0.1781],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 0.8000, 0.5000, 0.0130, 0.1230, 0.5030, 0.1070]) tensor(10.2287, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5775, 3.8640, 0.5131, 0.0676, 0.1779, 0.5216, 0.0773],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 3.4000, 0.2000, 0.1050, 0.2600, 0.4750, 0.0290]) tensor(1.5811, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7661,  6.5724,  1.7447,  0.0806,  0.2106,  0.5849,  0.1068],\n",
      "       grad_fn=<AddBackward0>) tensor([11.5000, 15.0000,  0.7000,  0.1620,  0.3230,  0.6380,  0.0300]) tensor(10.5353, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.2257, 0.8647, 0.6006, 0.0354, 0.1059, 0.4550, 0.0960],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 1.1000, 0.2000, 0.0310, 0.0870, 0.5340, 0.0430]) tensor(0.1678, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.0262, 2.3135, 1.3273, 0.0436, 0.1314, 0.4875, 0.1293],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 2.8000, 0.8000, 0.0190, 0.1380, 0.5030, 0.0620]) tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1828, 4.9162, 1.2884, 0.0657, 0.1775, 0.5578, 0.0919],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 7.5000, 0.4000, 0.1310, 0.1760, 0.5560, 0.0240]) tensor(1.0687, grad_fn=<MseLossBackward0>)\n",
      "tensor([23.6186,  6.7727,  4.8424,  0.0433,  0.1749,  0.6184,  0.2761],\n",
      "       grad_fn=<AddBackward0>) tensor([28.7000,  6.9000,  2.6000,  0.0610,  0.1580,  0.5600,  0.1370]) tensor(4.4127, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.6520, 0.5584, 0.6499, 0.0388, 0.1125, 0.4355, 0.1165],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 0.8000, 0.7000, 0.0000, 0.0930, 0.4620, 0.1030]) tensor(0.1974, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0319, 3.3011, 2.3358, 0.0339, 0.1187, 0.5315, 0.1490],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 2.5000, 1.2000, 0.0320, 0.0950, 0.4650, 0.0770]) tensor(2.9581, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5369,  5.3326,  1.2946,  0.0765,  0.2017,  0.5518,  0.1072],\n",
      "       grad_fn=<AddBackward0>) tensor([14.6000,  8.9000,  3.1000,  0.0590,  0.2300,  0.5410,  0.1380]) tensor(4.6423, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.8923,  2.4134, -0.1026,  0.0622,  0.1561,  0.4791,  0.0541],\n",
      "       grad_fn=<AddBackward0>) tensor([0.5000, 0.6000, 0.3000, 0.0690, 0.1200, 0.2730, 0.0910]) tensor(0.7763, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.9426,  4.1531,  4.3237,  0.0167,  0.1046,  0.5593,  0.2448],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 1.4000, 2.6000, 0.0220, 0.1060, 0.5450, 0.3440]) tensor(17.9950, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7319, 3.3255, 2.3161, 0.0377, 0.1288, 0.5147, 0.1647],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 1.0000, 2.4000, 0.0270, 0.0850, 0.5150, 0.3390]) tensor(7.2524, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.0661, 3.5935, 0.2239, 0.0721, 0.1815, 0.5077, 0.0667],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 3.9000, 0.7000, 0.0990, 0.2030, 0.6820, 0.0890]) tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6017,  4.1725,  2.6702,  0.0417,  0.1397,  0.5457,  0.1672],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 2.5000, 1.5000, 0.0220, 0.1060, 0.5530, 0.1050]) tensor(1.4201, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0093,  5.1710,  1.4562,  0.0627,  0.1757,  0.5618,  0.0989],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 4.4000, 0.4000, 0.1050, 0.1920, 0.5630, 0.0370]) tensor(3.0226, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5767, 2.4991, 1.4934, 0.0472, 0.1340, 0.4914, 0.1264],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 1.3000, 0.6000, 0.0290, 0.1150, 0.4900, 0.0990]) tensor(0.8228, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3737, 5.3579, 0.9232, 0.0808, 0.2053, 0.5573, 0.0810],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 4.3000, 0.1000, 0.1440, 0.2190, 0.5500, 0.0100]) tensor(3.2464, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.4671,  5.3179,  3.9736,  0.0352,  0.1420,  0.5793,  0.2266],\n",
      "       grad_fn=<AddBackward0>) tensor([11.4000,  2.6000,  2.8000,  0.0210,  0.1030,  0.5290,  0.1940]) tensor(6.5114, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1599, 4.5334, 0.3041, 0.0781, 0.1941, 0.5346, 0.0549],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 3.4000, 0.4000, 0.0770, 0.1990, 0.5420, 0.0510]) tensor(0.2151, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4388,  6.1170,  3.4334,  0.0499,  0.1652,  0.6024,  0.1830],\n",
      "       grad_fn=<AddBackward0>) tensor([14.4000,  8.7000,  3.0000,  0.0790,  0.2230,  0.5890,  0.1540]) tensor(1.5745, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6095, 3.9014, 0.6517, 0.0641, 0.1661, 0.5281, 0.0676],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 3.4000, 0.3000, 0.0830, 0.1680, 0.5230, 0.0280]) tensor(0.3377, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5684,  2.4700,  2.9469,  0.0215,  0.1030,  0.5084,  0.2027],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 1.5000, 3.0000, 0.0250, 0.0600, 0.4970, 0.2460]) tensor(1.7563, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8370,  4.3811,  2.7880,  0.0447,  0.1430,  0.5575,  0.1641],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 3.3000, 1.6000, 0.0160, 0.1290, 0.5440, 0.1150]) tensor(2.5834, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.4141,  1.8637, -0.1355,  0.0630,  0.1550,  0.4600,  0.0667],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 2.0000, 0.0000, 0.0310, 0.1870, 0.4040, 0.0000]) tensor(0.0557, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3420,  5.7345,  1.1019,  0.0848,  0.2204,  0.5587,  0.1014],\n",
      "       grad_fn=<AddBackward0>) tensor([10.8000,  5.5000,  1.5000,  0.0820,  0.1490,  0.4490,  0.0990]) tensor(0.0749, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.2251,  6.5388,  2.5954,  0.0638,  0.1882,  0.6003,  0.1439],\n",
      "       grad_fn=<AddBackward0>) tensor([10.7000,  8.4000,  0.7000,  0.1260,  0.2360,  0.5520,  0.0440]) tensor(3.9359, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.0289, 1.5236, 1.6748, 0.0277, 0.1064, 0.4739, 0.1547],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 0.3000, 0.1000, 0.0080, 0.0790, 0.4570, 0.0280]) tensor(3.5008, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.0191,  3.1480,  3.6072,  0.0204,  0.1101,  0.5145,  0.2432],\n",
      "       grad_fn=<AddBackward0>) tensor([17.3000,  4.8000,  6.1000,  0.0260,  0.1320,  0.5190,  0.2660]) tensor(2.8155, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.6188, 2.6014, 2.0636, 0.0342, 0.1125, 0.5050, 0.1413],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 1.5000, 0.5000, 0.0470, 0.1100, 0.4120, 0.0740]) tensor(3.3138, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0709, 3.1240, 1.8065, 0.0434, 0.1305, 0.5171, 0.1266],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 1.9000, 0.6000, 0.0200, 0.1040, 0.4690, 0.0540]) tensor(2.3484, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9410,  5.6591,  2.9105,  0.0578,  0.1771,  0.5778,  0.1745],\n",
      "       grad_fn=<AddBackward0>) tensor([18.9000,  7.4000,  2.4000,  0.0580,  0.1620,  0.5710,  0.1170]) tensor(2.7099, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2601,  3.6767,  3.9324,  0.0204,  0.1060,  0.5469,  0.2299],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2300e+01, 2.4000e+00, 4.9000e+00, 4.0000e-03, 7.8000e-02, 5.6100e-01,\n",
      "        2.3400e-01]) tensor(0.9157, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.1251, 1.2813, 2.1902, 0.0261, 0.0933, 0.4707, 0.1644],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 1.1000, 0.7000, 0.0090, 0.1190, 0.2100, 0.1050]) tensor(2.7633, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.6990, 2.1384, 0.7798, 0.0471, 0.1327, 0.4904, 0.0931],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 3.2000, 0.9000, 0.0920, 0.1060, 0.5330, 0.0800]) tensor(0.5299, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.4359,  5.4579,  3.4777,  0.0482,  0.1670,  0.5762,  0.2150],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5800e+01, 3.3000e+00, 4.3000e+00, 1.5000e-02, 1.0700e-01, 5.0700e-01,\n",
      "        2.3400e-01]) tensor(1.1455, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5360,  5.4396,  1.5429,  0.0657,  0.1810,  0.5711,  0.0980],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 5.3000, 0.6000, 0.1050, 0.1660, 0.5880, 0.0380]) tensor(1.9168, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5108, 3.6519, 0.8628, 0.0615, 0.1602, 0.5155, 0.0818],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.2000, 0.6000, 0.0130, 0.1060, 0.4280, 0.0850]) tensor(2.5325, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5598, 4.3469, 0.4810, 0.0784, 0.1947, 0.5291, 0.0694],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 3.7000, 0.4000, 0.1430, 0.2190, 0.4470, 0.0580]) tensor(2.0818, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4630,  6.5201,  2.8327,  0.0642,  0.1931,  0.6033,  0.1630],\n",
      "       grad_fn=<AddBackward0>) tensor([18.9000,  8.4000,  3.2000,  0.0790,  0.1870,  0.5940,  0.1480]) tensor(1.3726, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.7845, 2.9459, 1.4205, 0.0402, 0.1189, 0.5224, 0.0930],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 3.7000, 6.6000, 0.0300, 0.1060, 0.5830, 0.3370]) tensor(6.0025, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.8461,  6.2822,  2.3946,  0.0648,  0.1859,  0.5904,  0.1336],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 3.6000, 1.1000, 0.0360, 0.1380, 0.5150, 0.0720]) tensor(7.5791, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.8865,  4.1140, -0.6120,  0.0914,  0.2062,  0.5222,  0.0093],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 5.1000, 0.3000, 0.1520, 0.1830, 0.5760, 0.0240]) tensor(0.9587, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6400,  4.2202,  2.5362,  0.0398,  0.1327,  0.5479,  0.1509],\n",
      "       grad_fn=<AddBackward0>) tensor([9.5000, 7.8000, 2.6000, 0.1010, 0.1540, 0.5750, 0.1220]) tensor(2.0178, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7882,  6.7805,  1.3771,  0.0866,  0.2204,  0.5892,  0.0839],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 4.4000, 0.4000, 0.0970, 0.1870, 0.5930, 0.0390]) tensor(5.2494, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.6646,  7.1084,  3.0612,  0.0670,  0.1994,  0.6154,  0.1668],\n",
      "       grad_fn=<AddBackward0>) tensor([16.0000,  9.9000,  2.4000,  0.0710,  0.2870,  0.5530,  0.1190]) tensor(1.5735, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3797,  4.7461,  2.7032,  0.0422,  0.1431,  0.5616,  0.1598],\n",
      "       grad_fn=<AddBackward0>) tensor([10.8000,  5.1000,  2.2000,  0.0410,  0.1570,  0.5190,  0.1260]) tensor(0.4110, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0214,  5.4520,  3.0277,  0.0451,  0.1565,  0.5779,  0.1778],\n",
      "       grad_fn=<AddBackward0>) tensor([14.1000,  4.7000,  2.6000,  0.0200,  0.1380,  0.5490,  0.1350]) tensor(0.2287, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6005, 1.4933, 1.8109, 0.0266, 0.0996, 0.4782, 0.1513],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 1.6000, 1.8000, 0.0150, 0.1150, 0.4820, 0.1960]) tensor(0.1178, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0357, 2.0895, 2.7517, 0.0234, 0.0961, 0.4914, 0.1868],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 0.7000, 2.3000, 0.0090, 0.0530, 0.4770, 0.2200]) tensor(2.6321, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9416, 4.6407, 0.8880, 0.0806, 0.2094, 0.5308, 0.1098],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 1.0000, 0.0000, 0.1000, 0.2260, 0.4130, 0.0280]) tensor(13.4311, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7812,  5.6148,  2.4423,  0.0581,  0.1751,  0.5784,  0.1478],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 4.2000, 3.0000, 0.0470, 0.1170, 0.4630, 0.1620]) tensor(3.4634, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.7712,  5.1055,  3.1273,  0.0480,  0.1585,  0.5695,  0.1880],\n",
      "       grad_fn=<AddBackward0>) tensor([11.5000,  5.1000,  1.3000,  0.0600,  0.1750,  0.5450,  0.0970]) tensor(2.0070, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8123,  4.4160,  2.2049,  0.0491,  0.1513,  0.5431,  0.1447],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([5.5000, 2.2000, 4.5000, 0.0240, 0.0900, 0.4950, 0.3010]) tensor(5.4899, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4441, 5.3324, 0.1937, 0.0976, 0.2324, 0.5248, 0.0639],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 2.3000, 0.6000, 0.0700, 0.1230, 0.4730, 0.0690]) tensor(3.0340, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0044,  4.8027,  1.9765,  0.0600,  0.1662,  0.5583,  0.1207],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 2.1000, 0.7000, 0.0680, 0.1190, 0.5380, 0.0790]) tensor(5.4495, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.5311,  4.7211,  4.3335,  0.0236,  0.1237,  0.5697,  0.2507],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0400e+01, 3.3000e+00, 2.5000e+00, 1.5000e-02, 8.6000e-02, 5.4000e-01,\n",
      "        1.2300e-01]) tensor(1.9473, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9284,  6.4217,  1.9846,  0.0710,  0.1955,  0.6003,  0.1081],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 6.5000, 0.8000, 0.0930, 0.1790, 0.5210, 0.0490]) tensor(3.1324, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6828, 1.5871, 3.0316, 0.0109, 0.0811, 0.4892, 0.2154],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 0.9000, 0.8000, 0.0280, 0.1080, 0.4220, 0.1810]) tensor(9.2120, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.6949, 3.9680, 1.4492, 0.0598, 0.1639, 0.5209, 0.1145],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 1.4000, 0.6000, 0.0100, 0.1580, 0.5240, 0.1000]) tensor(3.4409, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1998,  5.3692,  2.4033,  0.0516,  0.1634,  0.5732,  0.1453],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 4.6000, 1.4000, 0.0330, 0.1310, 0.5620, 0.0680]) tensor(2.5149, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.7119,  5.3263,  2.6708,  0.0550,  0.1747,  0.5636,  0.1761],\n",
      "       grad_fn=<AddBackward0>) tensor([12.2000,  3.9000,  1.2000,  0.0490,  0.1300,  0.4730,  0.0890]) tensor(1.5036, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9582, 3.1374, 2.5248, 0.0317, 0.1190, 0.5239, 0.1702],\n",
      "       grad_fn=<AddBackward0>) tensor([7.4000, 2.2000, 1.3000, 0.0340, 0.0980, 0.4830, 0.1210]) tensor(1.2754, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1860,  2.5271,  2.3608,  0.0365,  0.1364,  0.4878,  0.2059],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 0.2000, 1.0000, 0.0000, 0.0560, 0.5810, 0.6000]) tensor(15.0246, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.8083,  4.2856,  3.9976,  0.0215,  0.1203,  0.5528,  0.2466],\n",
      "       grad_fn=<AddBackward0>) tensor([12.1000,  3.5000,  4.4000,  0.0180,  0.1360,  0.4640,  0.3070]) tensor(3.2798, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.6695,  6.8288,  3.9828,  0.0511,  0.1817,  0.6119,  0.2264],\n",
      "       grad_fn=<AddBackward0>) tensor([23.0000,  6.5000,  5.1000,  0.0260,  0.1630,  0.5170,  0.2490]) tensor(0.9711, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9990, 2.0815, 2.5333, 0.0289, 0.1156, 0.4832, 0.2046],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 1.1000, 1.0000, 0.0300, 0.0700, 0.5060, 0.1440]) tensor(4.6385, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4588, 4.4940, 0.4826, 0.0736, 0.1815, 0.5390, 0.0504],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 5.8000, 0.5000, 0.1010, 0.2410, 0.5160, 0.0450]) tensor(0.4362, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.0336,  4.9500,  4.3972,  0.0205,  0.1248,  0.5791,  0.2577],\n",
      "       grad_fn=<AddBackward0>) tensor([15.0000,  4.3000,  2.5000,  0.0490,  0.1160,  0.4670,  0.1620]) tensor(2.9021, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0456,  4.8434,  3.1510,  0.0432,  0.1528,  0.5567,  0.2008],\n",
      "       grad_fn=<AddBackward0>) tensor([11.8000,  3.6000,  2.3000,  0.0360,  0.1100,  0.5260,  0.1360]) tensor(1.8302, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9651,  4.5411,  2.8870,  0.0413,  0.1421,  0.5462,  0.1804],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0700e+01, 2.9000e+00, 1.2000e+00, 9.0000e-03, 1.0500e-01, 6.0200e-01,\n",
      "        6.3000e-02]) tensor(1.5270, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.6633,  6.1890,  4.0117,  0.0417,  0.1628,  0.6085,  0.2259],\n",
      "       grad_fn=<AddBackward0>) tensor([22.0000, 11.4000,  5.0000,  0.0780,  0.2520,  0.5310,  0.2150]) tensor(4.8009, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4333, 3.0048, 0.6974, 0.0575, 0.1574, 0.5028, 0.0922],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 2.2000, 0.2000, 0.0740, 0.1850, 0.4810, 0.0430]) tensor(0.9743, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9948,  6.6006,  2.1828,  0.0721,  0.2054,  0.5917,  0.1366],\n",
      "       grad_fn=<AddBackward0>) tensor([19.3000, 10.9000,  2.0000,  0.1030,  0.2640,  0.5480,  0.1060]) tensor(5.2943, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6071,  5.1144,  1.4499,  0.0726,  0.1998,  0.5540,  0.1226],\n",
      "       grad_fn=<AddBackward0>) tensor([8.5000, 4.8000, 1.0000, 0.1200, 0.1730, 0.4480, 0.0970]) tensor(1.4244, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.0518,  4.8847,  3.3817,  0.0444,  0.1575,  0.5648,  0.2131],\n",
      "       grad_fn=<AddBackward0>) tensor([21.1000,  7.0000,  2.2000,  0.0360,  0.2010,  0.5480,  0.1210]) tensor(4.4808, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0776,  6.4041,  2.5521,  0.0657,  0.1912,  0.5921,  0.1478],\n",
      "       grad_fn=<AddBackward0>) tensor([12.0000,  8.6000,  1.2000,  0.0920,  0.2300,  0.4950,  0.0620]) tensor(2.3059, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.5600,  6.1886,  3.5027,  0.0483,  0.1723,  0.5942,  0.2076],\n",
      "       grad_fn=<AddBackward0>) tensor([20.1000,  5.3000,  5.8000,  0.0270,  0.1360,  0.5290,  0.2680]) tensor(1.2069, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0989,  4.5053,  2.8768,  0.0403,  0.1428,  0.5483,  0.1816],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 1.5000, 4.0000, 0.0200, 0.0670, 0.4660, 0.3320]) tensor(7.5092, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.2541, 2.3700, 1.4081, 0.0435, 0.1273, 0.4941, 0.1203],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 2.7000, 0.6000, 0.0410, 0.1170, 0.5270, 0.0490]) tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4264, 3.1073, 2.9191, 0.0263, 0.1034, 0.5227, 0.1755],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 1.0000, 2.1000, 0.0130, 0.0630, 0.4810, 0.2320]) tensor(4.7841, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6333, 2.0014, 2.1761, 0.0360, 0.1302, 0.4738, 0.1996],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 1.5000, 0.5000, 0.0070, 0.0930, 0.5130, 0.0650]) tensor(1.4310, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.6369, 1.8742, 1.6640, 0.0381, 0.1201, 0.4943, 0.1440],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 0.5000, 0.6000, 0.0390, 0.0610, 0.5480, 0.2000]) tensor(3.3733, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7823,  3.1909,  3.8033,  0.0139,  0.0975,  0.5262,  0.2391],\n",
      "       grad_fn=<AddBackward0>) tensor([12.9000,  2.7000,  5.5000,  0.0170,  0.0710,  0.5210,  0.2700]) tensor(0.5571, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.6586, 2.1247, 2.4125, 0.0263, 0.1027, 0.4932, 0.1731],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 1.9000, 2.7000, 0.0230, 0.1000, 0.4680, 0.2600]) tensor(0.6256, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0478,  3.0733,  2.6504,  0.0333,  0.1249,  0.5113,  0.1912],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 1.7000, 0.3000, 0.0330, 0.1430, 0.5550, 0.0400]) tensor(7.7611, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4484,  4.7124,  1.7449,  0.0577,  0.1662,  0.5562,  0.1186],\n",
      "       grad_fn=<AddBackward0>) tensor([3.9000, 2.1000, 0.7000, 0.0610, 0.1560, 0.5810, 0.1000]) tensor(7.2570, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5942,  2.9438,  3.1144,  0.0255,  0.1115,  0.5105,  0.2110],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 1.1000, 1.7000, 0.0310, 0.0850, 0.5140, 0.2580]) tensor(9.9014, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5156, 3.5011, 0.4183, 0.0688, 0.1766, 0.5073, 0.0780],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 1.7000, 0.2000, 0.0650, 0.1810, 0.5450, 0.0370]) tensor(2.0411, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6813, 6.1623, 0.7868, 0.0939, 0.2291, 0.5727, 0.0665],\n",
      "       grad_fn=<AddBackward0>) tensor([12.1000,  7.9000,  2.4000,  0.0780,  0.2540,  0.5460,  0.1560]) tensor(1.6402, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6300,  3.5695,  2.5182,  0.0390,  0.1326,  0.5237,  0.1725],\n",
      "       grad_fn=<AddBackward0>) tensor([9.9000, 2.3000, 1.3000, 0.0150, 0.1030, 0.5850, 0.0950]) tensor(0.5200, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3440,  3.6939,  3.1291,  0.0296,  0.1162,  0.5322,  0.1882],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 2.9000, 2.2000, 0.0220, 0.1030, 0.5620, 0.1540]) tensor(1.9081, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7312,  4.9990,  1.7034,  0.0585,  0.1703,  0.5675,  0.1122],\n",
      "       grad_fn=<AddBackward0>) tensor([3.9000, 3.3000, 0.3000, 0.1370, 0.1730, 0.5150, 0.0360]) tensor(7.3623, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4841,  2.2876,  3.2922,  0.0136,  0.0871,  0.5048,  0.2149],\n",
      "       grad_fn=<AddBackward0>) tensor([11.4000,  1.9000,  3.6000,  0.0160,  0.0720,  0.5340,  0.2420]) tensor(0.1551, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8604, 4.9626, 1.3023, 0.0688, 0.1809, 0.5413, 0.0974],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 2.1000, 0.5000, 0.0240, 0.1090, 0.5620, 0.0510]) tensor(5.5235, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.4060e+01, 3.1330e+00, 3.8800e+00, 9.0637e-03, 8.9878e-02, 5.3525e-01,\n",
      "        2.3826e-01], grad_fn=<AddBackward0>) tensor([12.4000,  2.5000,  5.7000,  0.0230,  0.0860,  0.5460,  0.3230]) tensor(0.9250, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.7880,  3.9494,  4.1400,  0.0225,  0.1196,  0.5470,  0.2584],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 1.3000, 2.8000, 0.0280, 0.0880, 0.5200, 0.3430]) tensor(14.3935, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.4344, 4.3110, 0.7956, 0.0652, 0.1699, 0.5387, 0.0692],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 4.2000, 0.5000, 0.1230, 0.1900, 0.5130, 0.0520]) tensor(0.7281, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4509, 3.0057, 1.9653, 0.0391, 0.1244, 0.5042, 0.1427],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 2.1000, 0.4000, 0.0630, 0.1770, 0.5520, 0.0570]) tensor(3.4277, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9688, 3.9551, 1.6725, 0.0560, 0.1535, 0.5273, 0.1171],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 2.2000, 1.6000, 0.0210, 0.0900, 0.5070, 0.1130]) tensor(1.7870, grad_fn=<MseLossBackward0>)\n",
      "tensor([-2.5224, -0.1629, -0.9084,  0.0365,  0.1197,  0.3647,  0.0836],\n",
      "       grad_fn=<AddBackward0>) tensor([0., 0., 0., 0., 0., 0., 0.]) tensor(1.0529, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0768,  4.6207,  1.8957,  0.0515,  0.1549,  0.5529,  0.1222],\n",
      "       grad_fn=<AddBackward0>) tensor([8.3000, 6.0000, 1.4000, 0.0700, 0.2000, 0.5010, 0.0850]) tensor(0.7588, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3084, 2.5169, 2.6165, 0.0280, 0.1082, 0.5021, 0.1837],\n",
      "       grad_fn=<AddBackward0>) tensor([10.2000,  3.5000,  2.7000,  0.0510,  0.0760,  0.5450,  0.1230]) tensor(0.2536, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8960, 3.3360, 1.4361, 0.0578, 0.1649, 0.5165, 0.1354],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 0.8000, 0.6000, 0.0770, 0.1890, 0.7740, 0.3080]) tensor(7.4377, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8027,  2.9305,  2.5241,  0.0345,  0.1278,  0.5042,  0.1915],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 1.7000, 1.8000, 0.0230, 0.0900, 0.4320, 0.1840]) tensor(6.1486, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4663,  5.7925,  2.7498,  0.0577,  0.1802,  0.5748,  0.1740],\n",
      "       grad_fn=<AddBackward0>) tensor([10.4000,  6.4000,  1.0000,  0.0890,  0.2210,  0.5260,  0.0720]) tensor(4.1591, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5363,  3.6994,  2.9546,  0.0332,  0.1185,  0.5385,  0.1724],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 1.4000, 1.2000, 0.0300, 0.1080, 0.5550, 0.1940]) tensor(9.0951, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.9566,  8.1739,  2.8453,  0.0802,  0.2334,  0.6281,  0.1657],\n",
      "       grad_fn=<AddBackward0>) tensor([17.1000,  9.9000,  2.7000,  0.1090,  0.2780,  0.5240,  0.1540]) tensor(1.5964, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8117, 2.2533, 0.1405, 0.0619, 0.1467, 0.4910, 0.0473],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000, 1.4000, 0.2000, 0.1400, 0.2930, 0.6510, 0.0530]) tensor(0.1240, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.3918,  3.9551,  3.6285,  0.0221,  0.1133,  0.5509,  0.2181],\n",
      "       grad_fn=<AddBackward0>) tensor([13.9000,  3.1000,  1.8000,  0.0200,  0.0890,  0.5070,  0.0870]) tensor(0.6194, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5246, 4.9948, 0.2594, 0.0848, 0.2002, 0.5478, 0.0351],\n",
      "       grad_fn=<AddBackward0>) tensor([3.9000, 4.2000, 0.4000, 0.1220, 0.1710, 0.5060, 0.0340]) tensor(0.4707, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2603, 4.5727, 1.8660, 0.0546, 0.1560, 0.5521, 0.1155],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 3.5000, 0.5000, 0.0710, 0.2030, 0.5980, 0.0520]) tensor(2.2430, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.5877,  3.5045, -0.6371,  0.0862,  0.1905,  0.4960,  0.0082],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 2.8000, 0.4000, 0.1020, 0.1740, 0.4760, 0.0500]) tensor(0.4017, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2321, 4.5747, 1.4171, 0.0671, 0.1799, 0.5432, 0.1098],\n",
      "       grad_fn=<AddBackward0>) tensor([9.0000, 2.0000, 2.0000, 0.0330, 0.0780, 0.5340, 0.1600]) tensor(1.0053, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5232,  5.9624,  1.2326,  0.0781,  0.2036,  0.5772,  0.0827],\n",
      "       grad_fn=<AddBackward0>) tensor([8.7000, 6.0000, 0.4000, 0.0780, 0.1910, 0.5740, 0.0270]) tensor(0.5746, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0834,  4.9086,  2.1274,  0.0617,  0.1737,  0.5489,  0.1398],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 1.8000, 0.6000, 0.0810, 0.1330, 0.5340, 0.1070]) tensor(8.6810, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9315,  6.4723,  1.8439,  0.0721,  0.1987,  0.5888,  0.1087],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 6.6000, 0.6000, 0.1040, 0.2510, 0.5530, 0.0430]) tensor(3.6990, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4371,  5.3754,  2.9123,  0.0469,  0.1619,  0.5795,  0.1785],\n",
      "       grad_fn=<AddBackward0>) tensor([13.5000,  8.2000,  1.0000,  0.1060,  0.2360,  0.5590,  0.0600]) tensor(2.2016, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.8455,  5.9612,  2.4057,  0.0596,  0.1795,  0.5841,  0.1417],\n",
      "       grad_fn=<AddBackward0>) tensor([13.2000,  8.3000,  3.0000,  0.0570,  0.2280,  0.5540,  0.1520]) tensor(0.8919, grad_fn=<MseLossBackward0>)\n",
      "tensor([-4.5499e-01,  2.6921e+00, -1.1599e+00,  8.1552e-02,  1.8467e-01,\n",
      "         4.8422e-01, -6.5480e-04], grad_fn=<AddBackward0>) tensor([1.2000, 0.5000, 0.0000, 0.0000, 0.0770, 0.4510, 0.0000]) tensor(1.2728, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4159,  5.0432,  1.7058,  0.0635,  0.1765,  0.5643,  0.1114],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 4.3000, 1.9000, 0.0530, 0.2010, 0.5080, 0.1710]) tensor(2.8711, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7602, 5.0890, 0.8001, 0.0734, 0.1880, 0.5529, 0.0658],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 2.8000, 1.0000, 0.0400, 0.1130, 0.5460, 0.0790]) tensor(2.0930, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3761, 2.2513, 3.1199, 0.0185, 0.0915, 0.5041, 0.2020],\n",
      "       grad_fn=<AddBackward0>) tensor([7.7000e+00, 1.7000e+00, 3.5000e+00, 7.0000e-03, 8.3000e-02, 4.9800e-01,\n",
      "        2.8200e-01]) tensor(0.4664, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9066, 2.6496, 2.1162, 0.0411, 0.1347, 0.4928, 0.1743],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 2.7000, 0.5000, 0.0720, 0.1480, 0.5260, 0.0670]) tensor(1.2029, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4074, 4.4685, 0.3155, 0.0852, 0.2102, 0.5252, 0.0738],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 3.8000, 0.5000, 0.1840, 0.3870, 0.4810, 0.0960]) tensor(2.9773, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2271,  5.1764,  2.1052,  0.0628,  0.1744,  0.5627,  0.1290],\n",
      "       grad_fn=<AddBackward0>) tensor([5.4000, 4.4000, 1.0000, 0.0990, 0.1890, 0.4690, 0.0890]) tensor(5.1130, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.9218, 1.3049, 0.6605, 0.0435, 0.1204, 0.4555, 0.1009],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 1.5000, 0.4000, 0.0850, 0.1540, 0.3930, 0.1090]) tensor(0.0905, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.7198, 2.1432, 0.8538, 0.0481, 0.1383, 0.4814, 0.1095],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 0.4000, 0.1000, 0.0150, 0.0780, 0.5100, 0.0500]) tensor(2.2865, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1778, 1.7267, 2.2120, 0.0288, 0.1114, 0.4879, 0.1818],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4100e+01, 1.8000e+00, 3.0000e+00, 1.0000e-02, 7.7000e-02, 5.8200e-01,\n",
      "        1.9200e-01]) tensor(5.1014, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.7811,  6.5367,  3.8783,  0.0492,  0.1753,  0.6063,  0.2203],\n",
      "       grad_fn=<AddBackward0>) tensor([23.2000,  5.8000,  3.1000,  0.0400,  0.1300,  0.5380,  0.1490]) tensor(1.8356, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.1785, 3.2279, 2.2204, 0.0353, 0.1139, 0.5236, 0.1335],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 2.1000, 1.9000, 0.0130, 0.0980, 0.4620, 0.1350]) tensor(0.7562, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4390, 2.2723, 2.1413, 0.0340, 0.1200, 0.4934, 0.1715],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 0.8000, 0.7000, 0.0340, 0.1080, 0.5580, 0.2190]) tensor(5.9913, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.5128e+01,  4.7593e+00,  6.6478e+00, -3.8215e-03,  9.3367e-02,\n",
      "         5.8604e-01,  3.7620e-01], grad_fn=<AddBackward0>) tensor([3.3000e+01, 3.2000e+00, 7.4000e+00, 1.6000e-02, 7.1000e-02, 5.4300e-01,\n",
      "        3.3100e-01]) tensor(9.2813, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.2938, 2.4504, 0.2385, 0.0612, 0.1530, 0.4871, 0.0642],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 0.8000, 0.1000, 0.0800, 0.1300, 0.4750, 0.0300]) tensor(0.6313, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7173, 3.3668, 2.2984, 0.0407, 0.1291, 0.5168, 0.1544],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 2.4000, 1.1000, 0.0320, 0.1290, 0.6640, 0.1020]) tensor(1.5580, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.3526,  5.5014,  3.9269,  0.0417,  0.1539,  0.5771,  0.2264],\n",
      "       grad_fn=<AddBackward0>) tensor([17.2000,  6.3000,  2.0000,  0.0610,  0.1580,  0.4810,  0.1100]) tensor(0.6282, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2899, 5.4124, 0.7953, 0.0780, 0.2025, 0.5629, 0.0728],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 2.0000, 0.4000, 0.0520, 0.1460, 0.5230, 0.0660]) tensor(6.9847, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.1149,  1.5340, -0.0847,  0.0613,  0.1536,  0.4400,  0.0813],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 1.0000, 0.0000, 0.0000, 0.2730, 0.4360, 0.0000]) tensor(0.1572, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.1017,  7.5434,  2.4336,  0.0810,  0.2256,  0.6131,  0.1428],\n",
      "       grad_fn=<AddBackward0>) tensor([17.2000,  7.4000,  0.7000,  0.1130,  0.1700,  0.5580,  0.0440]) tensor(0.4361, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.0120,  4.3766,  3.3693,  0.0306,  0.1257,  0.5655,  0.1948],\n",
      "       grad_fn=<AddBackward0>) tensor([9.9000, 2.5000, 0.8000, 0.0140, 0.1040, 0.5510, 0.0530]) tensor(3.8646, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5244,  4.7885,  1.7951,  0.0626,  0.1802,  0.5377,  0.1415],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 5.5000, 1.0000, 0.0870, 0.1980, 0.6110, 0.0810]) tensor(0.5888, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.1027,  5.0424,  4.0125,  0.0314,  0.1322,  0.5716,  0.2238],\n",
      "       grad_fn=<AddBackward0>) tensor([16.6000,  3.6000,  2.0000,  0.0390,  0.0740,  0.5370,  0.1000]) tensor(0.9140, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4253,  3.7496,  3.2186,  0.0361,  0.1400,  0.5249,  0.2262],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 1.2000, 1.8000, 0.0240, 0.0830, 0.4550, 0.2410]) tensor(13.3754, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2734,  4.3781,  1.7733,  0.0610,  0.1725,  0.5369,  0.1348],\n",
      "       grad_fn=<AddBackward0>) tensor([8.5000, 4.9000, 1.2000, 0.0340, 0.1970, 0.5000, 0.0870]) tensor(0.5358, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.2294,  6.8619,  2.6887,  0.0666,  0.1970,  0.6087,  0.1493],\n",
      "       grad_fn=<AddBackward0>) tensor([15.3000,  9.3000,  3.5000,  0.0810,  0.2330,  0.5870,  0.1690]) tensor(1.0670, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3723,  4.3978,  1.5125,  0.0601,  0.1735,  0.5429,  0.1237],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 2.4000, 0.5000, 0.0690, 0.1690, 0.5400, 0.0750]) tensor(4.8400, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1755,  4.4424,  3.0547,  0.0425,  0.1457,  0.5363,  0.1964],\n",
      "       grad_fn=<AddBackward0>) tensor([10.6000,  2.2000,  2.2000,  0.0310,  0.0640,  0.5030,  0.1430]) tensor(1.7718, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4449,  3.9247,  2.2977,  0.0415,  0.1369,  0.5436,  0.1484],\n",
      "       grad_fn=<AddBackward0>) tensor([13.0000,  6.2000,  2.3000,  0.0720,  0.1280,  0.5860,  0.1080]) tensor(1.6729, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0408,  4.9944,  2.1508,  0.0643,  0.1849,  0.5592,  0.1524],\n",
      "       grad_fn=<AddBackward0>) tensor([14.8000,  6.1000,  3.1000,  0.0370,  0.1620,  0.5190,  0.1640]) tensor(0.7458, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2780, 2.4955, 1.5875, 0.0481, 0.1461, 0.4960, 0.1529],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 0.3000, 0.3000, 0.1670, 0.0000, 0.6000, 0.2000]) tensor(6.5629, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4990,  4.4500,  2.5436,  0.0467,  0.1410,  0.5588,  0.1418],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 4.0000, 3.4000, 0.0340, 0.1270, 0.5650, 0.1650]) tensor(1.8829, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4602, 3.9422, 1.3679, 0.0542, 0.1581, 0.5343, 0.1112],\n",
      "       grad_fn=<AddBackward0>) tensor([8.7000, 4.8000, 1.2000, 0.0780, 0.1570, 0.5130, 0.0880]) tensor(0.1176, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9222, 2.4266, 2.9664, 0.0203, 0.0977, 0.5080, 0.1978],\n",
      "       grad_fn=<AddBackward0>) tensor([11.0000,  1.6000,  3.1000,  0.0120,  0.0690,  0.5850,  0.2250]) tensor(0.2672, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6046, 5.6411, 0.4382, 0.0870, 0.2179, 0.5526, 0.0609],\n",
      "       grad_fn=<AddBackward0>) tensor([10.6000,  6.4000,  1.5000,  0.0530,  0.2190,  0.4650,  0.1020]) tensor(0.8136, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1137, 3.8632, 1.5474, 0.0572, 0.1633, 0.5201, 0.1309],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 4.4000, 1.3000, 0.0740, 0.1820, 0.5850, 0.1220]) tensor(1.0265, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7290, 2.1545, 2.2255, 0.0290, 0.1140, 0.4940, 0.1779],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 2.2000, 2.1000, 0.0280, 0.1120, 0.4230, 0.2240]) tensor(1.4022, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3556, 3.2913, 2.3041, 0.0363, 0.1248, 0.5267, 0.1548],\n",
      "       grad_fn=<AddBackward0>) tensor([11.4000,  2.7000,  1.5000,  0.0170,  0.0910,  0.5340,  0.0790]) tensor(0.7405, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9895,  5.3141,  1.8760,  0.0632,  0.1812,  0.5696,  0.1251],\n",
      "       grad_fn=<AddBackward0>) tensor([17.6000,  7.4000,  1.2000,  0.0640,  0.1830,  0.6450,  0.0650]) tensor(5.1850, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5188, 2.2531, 1.8297, 0.0422, 0.1298, 0.5011, 0.1508],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 1.3000, 4.3000, 0.0530, 0.0570, 0.3870, 0.4190]) tensor(3.0978, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9928, 2.0489, 2.5958, 0.0203, 0.0922, 0.5017, 0.1763],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000e+00, 1.7000e+00, 1.8000e+00, 2.0000e-03, 7.2000e-02, 5.4500e-01,\n",
      "        1.0800e-01]) tensor(0.2840, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4734,  3.9825,  3.1679,  0.0329,  0.1290,  0.5437,  0.1998],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2800e+01, 3.1000e+00, 6.0000e+00, 8.0000e-03, 8.0000e-02, 4.8300e-01,\n",
      "        2.4300e-01]) tensor(1.3231, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9828,  6.5529,  1.4856,  0.0829,  0.2134,  0.5830,  0.0934],\n",
      "       grad_fn=<AddBackward0>) tensor([10.4000, 13.6000,  0.9000,  0.1330,  0.2890,  0.6300,  0.0360]) tensor(7.5034, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2538,  2.4331,  3.6985,  0.0130,  0.0907,  0.5123,  0.2388],\n",
      "       grad_fn=<AddBackward0>) tensor([13.8000,  3.3000,  4.7000,  0.0200,  0.0860,  0.4470,  0.2450]) tensor(0.5928, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5179, 3.5235, 1.3206, 0.0555, 0.1563, 0.5192, 0.1135],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 2.0000, 0.8000, 0.0350, 0.1190, 0.4380, 0.0870]) tensor(2.3463, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.0185, 0.1407, 2.0715, 0.0178, 0.0758, 0.4534, 0.1722],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 1.0000, 0.0000, 0.0000, 0.2500, 0.3750, 0.0000]) tensor(1.6341, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.8464, 2.0802, 2.2062, 0.0279, 0.1063, 0.4978, 0.1660],\n",
      "       grad_fn=<AddBackward0>) tensor([8.7000, 3.0000, 2.2000, 0.0380, 0.1040, 0.5790, 0.1510]) tensor(0.2259, grad_fn=<MseLossBackward0>)\n",
      "tensor([21.8804,  6.7834,  4.4378,  0.0431,  0.1704,  0.6176,  0.2471],\n",
      "       grad_fn=<AddBackward0>) tensor([23.6000,  3.8000,  4.5000,  0.0430,  0.0720,  0.5450,  0.2000]) tensor(1.6970, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9916,  6.0572,  1.5417,  0.0750,  0.1969,  0.5775,  0.0931],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 5.5000, 1.0000, 0.1000, 0.2240, 0.4650, 0.0800]) tensor(6.2953, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0779,  2.6819,  2.8065,  0.0249,  0.1067,  0.5046,  0.1931],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 0.5000, 0.7000, 0.0160, 0.0890, 0.4820, 0.2340]) tensor(11.3414, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2294,  5.5555,  1.5479,  0.0662,  0.1803,  0.5738,  0.0927],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 2.6000, 0.4000, 0.0980, 0.1350, 0.5350, 0.0550]) tensor(8.2963, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0147, 3.4552, 1.6219, 0.0531, 0.1468, 0.5149, 0.1208],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 2.1000, 0.5000, 0.0260, 0.1840, 0.5820, 0.0630]) tensor(2.5224, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.8930, 3.4848, 1.7576, 0.0505, 0.1456, 0.5184, 0.1296],\n",
      "       grad_fn=<AddBackward0>) tensor([7.5000, 4.0000, 1.2000, 0.0440, 0.1190, 0.4560, 0.0650]) tensor(0.1057, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6578, 1.3060, 2.1611, 0.0236, 0.0915, 0.4808, 0.1627],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 2.2000, 3.8000, 0.0240, 0.1270, 0.5290, 0.3890]) tensor(0.6108, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.8914,  5.6871,  2.7211,  0.0555,  0.1738,  0.5819,  0.1639],\n",
      "       grad_fn=<AddBackward0>) tensor([12.6000,  5.4000,  4.3000,  0.0310,  0.1410,  0.5130,  0.2130]) tensor(1.1192, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1326,  3.0958,  3.3740,  0.0278,  0.1149,  0.5145,  0.2206],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 1.5000, 0.8000, 0.0220, 0.0840, 0.5260, 0.0870]) tensor(6.5119, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9477, 1.3405, 2.6494, 0.0201, 0.0879, 0.4709, 0.1934],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 1.4000, 1.4000, 0.0290, 0.1510, 0.4090, 0.2960]) tensor(3.7232, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8886,  1.7258,  2.9816,  0.0225,  0.1063,  0.4939,  0.2259],\n",
      "       grad_fn=<AddBackward0>) tensor([15.9000,  1.9000,  4.5000,  0.0280,  0.0570,  0.5520,  0.3300]) tensor(3.9239, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0833, 2.6177, 1.3406, 0.0575, 0.1618, 0.4912, 0.1474],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 1.4000, 1.2000, 0.0270, 0.1330, 0.2760, 0.2500]) tensor(5.8629, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4940, 2.8949, 1.2046, 0.0548, 0.1570, 0.4987, 0.1291],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([2.1000, 0.4000, 0.4000, 0.0600, 0.0260, 0.5060, 0.1540]) tensor(5.1408, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 5.3793,  4.4900, -0.3769,  0.0900,  0.2199,  0.5027,  0.0487],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 1.3000, 0.0000, 0.0000, 0.3330, 0.7530, 0.0000]) tensor(3.4202, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.5276, 3.0184, 2.4049, 0.0367, 0.1203, 0.5125, 0.1610],\n",
      "       grad_fn=<AddBackward0>) tensor([11.7000,  4.4000,  1.6000,  0.0140,  0.1160,  0.5670,  0.0660]) tensor(1.8048, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4915, 3.8869, 1.8006, 0.0526, 0.1569, 0.5246, 0.1389],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.4000, 0.5000, 0.0320, 0.1110, 0.4450, 0.0810]) tensor(6.6034, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9385,  3.0094,  3.4871,  0.0163,  0.0950,  0.5263,  0.2142],\n",
      "       grad_fn=<AddBackward0>) tensor([12.2000,  4.0000,  2.1000,  0.0130,  0.1140,  0.5130,  0.0970]) tensor(0.4269, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.6208, 4.6974, 1.0340, 0.0697, 0.1727, 0.5464, 0.0677],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 3.9000, 0.1000, 0.0940, 0.1260, 0.5980, 0.0090]) tensor(1.6986, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9153, 2.8224, 3.1619, 0.0275, 0.1056, 0.5188, 0.1938],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 1.3000, 2.2000, 0.0360, 0.1010, 0.6000, 0.2760]) tensor(5.9837, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4046, 1.5209, 1.2923, 0.0373, 0.1214, 0.4639, 0.1438],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000, 0.7000, 0.3000, 0.0630, 0.0750, 0.3370, 0.0740]) tensor(2.8874, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9344, 4.2934, 1.8367, 0.0560, 0.1529, 0.5343, 0.1144],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 6.0000, 2.0000, 0.0580, 0.2410, 0.3920, 0.1340]) tensor(2.8658, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.1418,  4.9080,  3.7273,  0.0342,  0.1382,  0.5678,  0.2192],\n",
      "       grad_fn=<AddBackward0>) tensor([11.7000,  3.2000,  3.0000,  0.0220,  0.1110,  0.5340,  0.1920]) tensor(3.3112, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.4927,  2.4862, -0.5023,  0.0722,  0.1696,  0.4650,  0.0382],\n",
      "       grad_fn=<AddBackward0>) tensor([0.5000, 0.8000, 0.3000, 0.0690, 0.2110, 0.5000, 0.1430]) tensor(0.5001, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7927, 3.9638, 1.3172, 0.0613, 0.1701, 0.5312, 0.1149],\n",
      "       grad_fn=<AddBackward0>) tensor([3.9000, 2.4000, 0.3000, 0.1050, 0.2050, 0.5130, 0.0620]) tensor(3.9179, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3593, 4.0944, 2.2225, 0.0461, 0.1352, 0.5397, 0.1270],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 4.2000, 0.7000, 0.0840, 0.2370, 0.4890, 0.0690]) tensor(4.4385, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.2200,  6.6299,  2.6891,  0.0697,  0.1971,  0.5954,  0.1503],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 4.4000, 2.0000, 0.0830, 0.1760, 0.4920, 0.1730]) tensor(8.6450, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 4.3604,  3.5068, -0.0255,  0.0760,  0.1852,  0.5096,  0.0531],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 2.2000, 0.1000, 0.1370, 0.2190, 0.5320, 0.0140]) tensor(1.4161, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.0793,  4.8275,  3.5694,  0.0324,  0.1367,  0.5662,  0.2147],\n",
      "       grad_fn=<AddBackward0>) tensor([12.8000,  3.0000,  5.7000,  0.0180,  0.0900,  0.4730,  0.2800]) tensor(2.6640, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.5393, 2.5220, 0.6476, 0.0566, 0.1523, 0.4897, 0.0947],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 2.3000, 0.3000, 0.0110, 0.2340, 0.4250, 0.0580]) tensor(0.5096, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2526,  2.6442,  3.8482,  0.0146,  0.0953,  0.5033,  0.2534],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 1.2000, 2.5000, 0.0210, 0.0910, 0.5060, 0.3200]) tensor(10.2876, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.5993, -1.0706,  1.4479,  0.0115,  0.0622,  0.4006,  0.1661],\n",
      "       grad_fn=<AddBackward0>) tensor([0.3000, 0.3000, 0.4000, 0.0330, 0.0310, 0.1110, 0.1880]) tensor(0.4503, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.1310e+01, 5.4352e+00, 4.8450e+00, 2.0381e-02, 1.3012e-01, 5.9134e-01,\n",
      "        2.7964e-01], grad_fn=<AddBackward0>) tensor([19.1000,  3.7000,  8.1000,  0.0240,  0.0910,  0.5390,  0.3900]) tensor(2.6435, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3564, 3.0424, 2.4469, 0.0343, 0.1208, 0.5223, 0.1647],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 1.9000, 1.3000, 0.0370, 0.0880, 0.5260, 0.1110]) tensor(2.3907, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.9846e+01, 4.8130e+00, 4.7337e+00, 1.7723e-02, 1.2021e-01, 5.8151e-01,\n",
      "        2.7534e-01], grad_fn=<AddBackward0>) tensor([11.0000,  2.6000,  4.4000,  0.0180,  0.1230,  0.5100,  0.3680]) tensor(11.8966, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.0110,  8.8535,  1.9100,  0.0987,  0.2565,  0.6381,  0.0994],\n",
      "       grad_fn=<AddBackward0>) tensor([8.5000, 6.4000, 0.6000, 0.0930, 0.1920, 0.4510, 0.0380]) tensor(11.4595, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8318,  5.1586,  2.5128,  0.0527,  0.1633,  0.5562,  0.1582],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 3.7000, 0.9000, 0.0520, 0.0910, 0.5740, 0.0470]) tensor(2.1701, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6536,  6.7023,  1.9370,  0.0762,  0.2128,  0.5944,  0.1245],\n",
      "       grad_fn=<AddBackward0>) tensor([16.0000, 11.0000,  1.3000,  0.1230,  0.2670,  0.5470,  0.0710]) tensor(2.9570, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0687,  5.7060,  2.3746,  0.0650,  0.1777,  0.5662,  0.1358],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 5.6000, 0.9000, 0.0410, 0.1600, 0.5370, 0.0470]) tensor(5.4029, grad_fn=<MseLossBackward0>)\n",
      "tensor([21.1284,  8.4633,  3.1486,  0.0772,  0.2314,  0.6348,  0.1778],\n",
      "       grad_fn=<AddBackward0>) tensor([20.1000, 10.0000,  1.9000,  0.1310,  0.1840,  0.5280,  0.1180]) tensor(0.7141, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9012, 2.7904, 1.1894, 0.0570, 0.1568, 0.4891, 0.1293],\n",
      "       grad_fn=<AddBackward0>) tensor([6.8000, 2.9000, 3.3000, 0.0120, 0.1490, 0.4850, 0.2540]) tensor(0.6421, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7131,  6.3089,  2.0307,  0.0704,  0.1983,  0.5840,  0.1263],\n",
      "       grad_fn=<AddBackward0>) tensor([6.9000, 3.3000, 1.3000, 0.0250, 0.1420, 0.4850, 0.1180]) tensor(8.0030, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7721, 3.1195, 1.3007, 0.0504, 0.1511, 0.5152, 0.1237],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 1.6000, 0.2000, 0.0780, 0.1360, 0.4560, 0.0400]) tensor(3.4909, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.2248,  2.1285, -1.0471,  0.0793,  0.1817,  0.4632,  0.0236],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 0.4000, 0.5000, 0.0000, 0.1140, 0.6480, 0.2000]) tensor(1.2553, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8229,  3.7844,  3.1738,  0.0292,  0.1209,  0.5477,  0.1944],\n",
      "       grad_fn=<AddBackward0>) tensor([13.9000,  5.6000,  2.1000,  0.0310,  0.1530,  0.5690,  0.1000]) tensor(0.8029, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9703, 2.3490, 2.5101, 0.0266, 0.1064, 0.5097, 0.1762],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 2.0000, 1.1000, 0.0420, 0.1390, 0.5590, 0.1470]) tensor(2.6688, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6038,  3.0015,  3.4466,  0.0171,  0.0941,  0.5356,  0.2053],\n",
      "       grad_fn=<AddBackward0>) tensor([14.0000,  2.3000,  1.4000,  0.0190,  0.0760,  0.6080,  0.0810]) tensor(1.4919, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5576,  5.7641,  2.9594,  0.0499,  0.1672,  0.5825,  0.1756],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 4.5000, 2.9000, 0.0290, 0.2020, 0.4590, 0.2240]) tensor(7.5503, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.1481,  3.1296, -1.1102,  0.0883,  0.1953,  0.4922, -0.0033],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 1.5000, 0.0000, 0.0400, 0.1470, 0.3330, 0.0000]) tensor(0.6634, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9333,  5.7661,  1.4031,  0.0789,  0.2066,  0.5580,  0.1054],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 2.0000, 0.5000, 0.0990, 0.1190, 0.4120, 0.0910]) tensor(10.6904, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7765, 1.2532, 3.0219, 0.0126, 0.0794, 0.4879, 0.2115],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 1.3000, 3.2000, 0.0190, 0.0890, 0.5190, 0.3490]) tensor(3.8358, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.6005,  5.9768,  3.1875,  0.0499,  0.1679,  0.5949,  0.1820],\n",
      "       grad_fn=<AddBackward0>) tensor([13.0000,  4.2000,  1.4000,  0.0320,  0.1220,  0.5470,  0.0790]) tensor(2.7616, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.6968, 3.9216, 0.8827, 0.0642, 0.1602, 0.5236, 0.0679],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 1.8000, 0.9000, 0.0640, 0.1270, 0.6540, 0.1120]) tensor(2.1059, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0314,  4.7360,  2.5171,  0.0528,  0.1601,  0.5541,  0.1575],\n",
      "       grad_fn=<AddBackward0>) tensor([9.4000, 2.8000, 1.0000, 0.0240, 0.1080, 0.5900, 0.0820]) tensor(1.8550, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0137,  2.9568,  3.2655,  0.0217,  0.1059,  0.5235,  0.2120],\n",
      "       grad_fn=<AddBackward0>) tensor([9.4000, 1.8000, 2.6000, 0.0170, 0.0790, 0.5340, 0.2350]) tensor(1.2305, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8774,  5.5144,  2.7725,  0.0535,  0.1599,  0.5799,  0.1482],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 3.6000, 0.4000, 0.0880, 0.1700, 0.5180, 0.0440]) tensor(10.6505, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6727,  1.9045,  3.3183,  0.0142,  0.0894,  0.4926,  0.2293],\n",
      "       grad_fn=<AddBackward0>) tensor([5.9000, 1.4000, 2.1000, 0.0180, 0.0880, 0.4830, 0.2270]) tensor(3.5025, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1083,  5.6664,  2.5510,  0.0617,  0.1838,  0.5736,  0.1597],\n",
      "       grad_fn=<AddBackward0>) tensor([13.3000,  7.0000,  1.4000,  0.1020,  0.1790,  0.5140,  0.0890]) tensor(0.5381, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9456, 3.0713, 1.3544, 0.0535, 0.1551, 0.5081, 0.1302],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 2.0000, 1.2000, 0.0150, 0.1260, 0.4900, 0.1670]) tensor(1.8639, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0469, 5.6396, 1.0752, 0.0787, 0.1987, 0.5756, 0.0696],\n",
      "       grad_fn=<AddBackward0>) tensor([14.3000, 13.5000,  1.3000,  0.1100,  0.2880,  0.6830,  0.0590]) tensor(12.7789, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6029,  2.5597,  3.4295,  0.0141,  0.0917,  0.5057,  0.2262],\n",
      "       grad_fn=<AddBackward0>) tensor([13.2000,  3.1000,  5.3000,  0.0230,  0.0870,  0.5080,  0.2660]) tensor(0.9062, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9389, 2.0234, 2.6861, 0.0239, 0.1030, 0.4852, 0.1997],\n",
      "       grad_fn=<AddBackward0>) tensor([8.3000, 2.3000, 1.4000, 0.0270, 0.0950, 0.5100, 0.1100]) tensor(0.3068, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9283, 3.6339, 1.7991, 0.0479, 0.1407, 0.5287, 0.1234],\n",
      "       grad_fn=<AddBackward0>) tensor([10.3000,  6.4000,  1.1000,  0.1070,  0.1490,  0.5450,  0.0710]) tensor(1.9674, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.6874,  6.0749,  3.9446,  0.0416,  0.1583,  0.5989,  0.2201],\n",
      "       grad_fn=<AddBackward0>) tensor([22.2000,  7.0000,  1.9000,  0.0640,  0.1250,  0.5420,  0.0850]) tensor(2.4854, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.8525,  4.8513,  3.6108,  0.0389,  0.1456,  0.5601,  0.2195],\n",
      "       grad_fn=<AddBackward0>) tensor([14.2000,  5.1000,  2.1000,  0.0790,  0.1290,  0.5380,  0.1170]) tensor(0.7269, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.8704, 4.6158, 0.4484, 0.0811, 0.1951, 0.5349, 0.0546],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 2.0000, 0.2000, 0.0670, 0.1840, 0.4230, 0.0380]) tensor(3.4728, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2807,  3.8877,  2.1518,  0.0410,  0.1377,  0.5406,  0.1452],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 4.1000, 1.4000, 0.0380, 0.2130, 0.4890, 0.1260]) tensor(2.7062, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.8053, 0.5654, 1.9104, 0.0199, 0.0853, 0.4546, 0.1695],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 0.3000, 1.0000, 0.0450, 0.0000, 0.3540, 0.1960]) tensor(1.5087, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1791,  4.3660,  3.4385,  0.0324,  0.1285,  0.5588,  0.2023],\n",
      "       grad_fn=<AddBackward0>) tensor([15.6000,  3.2000,  7.3000,  0.0230,  0.0780,  0.6030,  0.3350]) tensor(2.6160, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2165,  2.6111,  3.1214,  0.0193,  0.0999,  0.5173,  0.2072],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 1.2000, 1.3000, 0.0190, 0.1220, 0.5330, 0.2000]) tensor(7.9933, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.5063, 1.1700, 0.8225, 0.0408, 0.1306, 0.4434, 0.1402],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 2.0000, 0.0000, 0.1880, 0.2000, 0.0000, 0.0000]) tensor(3.1308, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 4.4889,  4.0490, -0.0249,  0.0826,  0.1975,  0.5055,  0.0521],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 3.2000, 0.8000, 0.1070, 0.2070, 0.5440, 0.1030]) tensor(0.6084, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1179, 4.0392, 2.0249, 0.0483, 0.1448, 0.5342, 0.1330],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 2.4000, 1.1000, 0.0590, 0.0830, 0.4290, 0.0900]) tensor(4.2503, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1328, 3.0505, 1.8129, 0.0454, 0.1392, 0.5076, 0.1440],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 0.5000, 1.6000, 0.0090, 0.0630, 0.4480, 0.3180]) tensor(5.1582, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9009, 1.1384, 2.4392, 0.0168, 0.0794, 0.4745, 0.1765],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 2.0000, 2.7000, 0.0170, 0.0940, 0.4610, 0.1800]) tensor(0.4819, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.2081, 1.2819, 1.5622, 0.0265, 0.0958, 0.4748, 0.1353],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.4000, 0.7000, 0.0200, 0.1170, 0.4160, 0.1090]) tensor(0.7437, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.0174, 2.5517, 1.8030, 0.0402, 0.1205, 0.5015, 0.1314],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.7000, 0.8000, 0.0480, 0.0920, 0.4980, 0.0930]) tensor(1.3026, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5286, 3.8629, 0.7189, 0.0675, 0.1756, 0.5142, 0.0869],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 2.5000, 0.0000, 0.0810, 0.2320, 0.7050, 0.0040]) tensor(2.4399, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1617,  5.1040,  1.7648,  0.0637,  0.1788,  0.5620,  0.1200],\n",
      "       grad_fn=<AddBackward0>) tensor([16.1000,  7.7000,  0.9000,  0.1270,  0.1480,  0.5890,  0.0470]) tensor(4.5549, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.2058,  1.3951,  0.1621,  0.0554,  0.1311,  0.4718,  0.0569],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 0.8000, 0.3000, 0.0000, 0.1580, 0.4000, 0.0530]) tensor(0.2623, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.5333,  3.6581,  3.2762,  0.0367,  0.1516,  0.5320,  0.2476],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 1.3000, 0.4000, 0.1250, 0.1250, 0.5550, 0.2310]) tensor(23.0086, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.2241,  1.7109, -0.1196,  0.0663,  0.1526,  0.4622,  0.0560],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.1820]) tensor(0.6412, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.7546,  6.7431,  3.1104,  0.0607,  0.1914,  0.6060,  0.1785],\n",
      "       grad_fn=<AddBackward0>) tensor([21.0000, 10.3000,  2.7000,  0.1070,  0.2060,  0.5530,  0.1200]) tensor(3.3372, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.6821,  7.6339,  3.4753,  0.0647,  0.2072,  0.6289,  0.1952],\n",
      "       grad_fn=<AddBackward0>) tensor([20.5000,  8.0000,  3.7000,  0.0710,  0.1800,  0.4940,  0.1780]) tensor(0.0339, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1109, 4.1038, 1.2982, 0.0633, 0.1581, 0.5370, 0.0821],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 6.5000, 0.8000, 0.1260, 0.1980, 0.4500, 0.0450]) tensor(2.3307, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0651, 3.5114, 1.6268, 0.0582, 0.1651, 0.5133, 0.1430],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 4.0000, 0.4000, 0.1000, 0.1910, 0.4300, 0.0470]) tensor(1.5077, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6855,  5.3004,  1.9732,  0.0621,  0.1771,  0.5675,  0.1259],\n",
      "       grad_fn=<AddBackward0>) tensor([10.6000,  5.5000,  1.1000,  0.0400,  0.2080,  0.5460,  0.0690]) tensor(0.2837, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9189,  6.0792,  1.8331,  0.0694,  0.1898,  0.5881,  0.1043],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 6.5000, 0.9000, 0.1050, 0.1710, 0.4990, 0.0490]) tensor(1.9204, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.3640,  4.2540,  2.8534,  0.0411,  0.1446,  0.5366,  0.1922],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 1.4000, 2.0000, 0.0180, 0.1190, 0.4680, 0.2710]) tensor(14.8888, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2523,  3.4231,  3.3973,  0.0214,  0.1082,  0.5409,  0.2113],\n",
      "       grad_fn=<AddBackward0>) tensor([13.5000,  3.3000,  4.5000,  0.0280,  0.0790,  0.4960,  0.2150]) tensor(0.1850, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2253,  5.5207,  1.2717,  0.0752,  0.1979,  0.5494,  0.1006],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 5.0000, 0.9000, 0.1050, 0.1660, 0.4740, 0.0680]) tensor(2.8573, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4570,  5.4505,  1.9404,  0.0663,  0.1883,  0.5504,  0.1400],\n",
      "       grad_fn=<AddBackward0>) tensor([6.8000, 5.3000, 0.6000, 0.0760, 0.2250, 0.5070, 0.0580]) tensor(4.8331, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9589, 1.5304, 2.1012, 0.0258, 0.1016, 0.4730, 0.1753],\n",
      "       grad_fn=<AddBackward0>) tensor([3.9000, 1.4000, 2.0000, 0.0240, 0.1040, 0.4540, 0.2640]) tensor(1.3417, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.2360,  5.6910,  3.3629,  0.0489,  0.1683,  0.5854,  0.2021],\n",
      "       grad_fn=<AddBackward0>) tensor([18.5000,  4.0000,  4.8000,  0.0240,  0.1010,  0.5590,  0.2280]) tensor(0.9327, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.2591, 2.9221, 0.8806, 0.0593, 0.1632, 0.4788, 0.1187],\n",
      "       grad_fn=<AddBackward0>) tensor([0.3000, 0.4000, 0.3000, 0.0000, 0.1430, 0.1250, 0.2000]) tensor(6.0492, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1989,  6.3222,  2.6045,  0.0631,  0.1827,  0.6021,  0.1372],\n",
      "       grad_fn=<AddBackward0>) tensor([13.3000, 11.6000,  1.4000,  0.1120,  0.2560,  0.5840,  0.0660]) tensor(4.3039, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.2220, -0.1892,  0.6419,  0.0347,  0.0941,  0.4415,  0.1024],\n",
      "       grad_fn=<AddBackward0>) tensor([0.7000, 0.3000, 0.3000, 0.0000, 0.0450, 0.5000, 0.1110]) tensor(0.1733, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.5959,  6.6074,  2.6716,  0.0712,  0.1956,  0.5993,  0.1417],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 5.5000, 0.2000, 0.1170, 0.1940, 0.6150, 0.0140]) tensor(7.6482, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.5647,  4.5596,  4.1097,  0.0253,  0.1272,  0.5651,  0.2475],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000e+01, 3.9000e+00, 5.2000e+00, 1.5000e-02, 1.1900e-01, 5.4300e-01,\n",
      "        2.7200e-01]) tensor(0.2777, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0648, 2.8955, 1.6436, 0.0414, 0.1290, 0.5111, 0.1279],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 1.9000, 0.6000, 0.0160, 0.0980, 0.4870, 0.0570]) tensor(0.4312, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7348, 5.3196, 1.1184, 0.0742, 0.1899, 0.5528, 0.0815],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 4.9000, 0.5000, 0.1150, 0.1950, 0.5120, 0.0440]) tensor(4.1461, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1509, 1.8799, 2.4488, 0.0196, 0.0935, 0.4950, 0.1786],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 0.9000, 0.4000, 0.0320, 0.0800, 0.4490, 0.0710]) tensor(4.8291, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5879,  7.2147,  2.4333,  0.0769,  0.2120,  0.6101,  0.1324],\n",
      "       grad_fn=<AddBackward0>) tensor([13.7000, 11.8000,  1.4000,  0.1170,  0.2930,  0.6040,  0.0710]) tensor(3.6669, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5662,  3.5340,  3.0142,  0.0260,  0.1117,  0.5402,  0.1843],\n",
      "       grad_fn=<AddBackward0>) tensor([12.4000,  2.6000,  2.9000,  0.0220,  0.0850,  0.5720,  0.1540]) tensor(0.2262, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1645, 3.8036, 1.3645, 0.0604, 0.1712, 0.5224, 0.1274],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 1.5000, 0.2000, 0.1120, 0.1430, 0.3570, 0.0450]) tensor(7.6888, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4681,  4.5818,  2.7449,  0.0480,  0.1528,  0.5440,  0.1750],\n",
      "       grad_fn=<AddBackward0>) tensor([11.5000,  6.4000,  2.8000,  0.0540,  0.1770,  0.6020,  0.1410]) tensor(0.6074, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.5861,  6.3915,  3.5659,  0.0568,  0.1806,  0.5998,  0.1982],\n",
      "       grad_fn=<AddBackward0>) tensor([14.8000,  7.8000,  2.3000,  0.0460,  0.2590,  0.5350,  0.1470]) tensor(1.6231, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2790,  3.0456,  2.2748,  0.0421,  0.1446,  0.5051,  0.1895],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 0.7000, 0.2000, 0.0080, 0.1200, 0.3470, 0.0790]) tensor(14.7900, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.0919, 1.6313, 1.5324, 0.0344, 0.1166, 0.4778, 0.1479],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000, 0.4000, 0.4000, 0.0170, 0.0770, 0.3860, 0.1790]) tensor(3.9613, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9344,  4.1351,  2.4787,  0.0411,  0.1375,  0.5416,  0.1572],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 2.0000, 3.7000, 0.0160, 0.0930, 0.5110, 0.3050]) tensor(3.9361, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.0368, 1.6619, 1.2968, 0.0365, 0.1180, 0.4662, 0.1376],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 1.7000, 1.6000, 0.0390, 0.0880, 0.4860, 0.1560]) tensor(0.0408, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7315,  5.1776,  1.6156,  0.0638,  0.1800,  0.5586,  0.1133],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 1.7000, 0.6000, 0.0440, 0.1450, 0.4830, 0.1050]) tensor(9.1415, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6998,  3.9456,  3.3822,  0.0331,  0.1207,  0.5402,  0.1935],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 2.1000, 0.8000, 0.0190, 0.1140, 0.4700, 0.0810]) tensor(7.8541, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.2019, 3.1271, 1.4332, 0.0606, 0.1630, 0.4737, 0.1450],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 2.9000, 0.1000, 0.1610, 0.1900, 0.3490, 0.0170]) tensor(5.4134, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4110, 3.2844, 1.7391, 0.0459, 0.1417, 0.5177, 0.1365],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 2.1000, 1.0000, 0.0490, 0.0730, 0.4940, 0.0940]) tensor(0.4558, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.3445e+01, 2.5384e+00, 3.8768e+00, 1.0152e-02, 9.1327e-02, 5.0404e-01,\n",
      "        2.5853e-01], grad_fn=<AddBackward0>) tensor([15.3000,  2.5000,  2.5000,  0.0170,  0.0800,  0.5840,  0.1490]) tensor(0.7652, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.0761, 2.6211, 1.2114, 0.0472, 0.1380, 0.4923, 0.1198],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 1.8000, 0.5000, 0.0470, 0.1040, 0.4480, 0.0600]) tensor(1.9965, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1835,  4.2840,  3.1638,  0.0341,  0.1253,  0.5545,  0.1794],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 1.6000, 0.5000, 0.0150, 0.0710, 0.5620, 0.0380]) tensor(5.4531, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3647, 1.3793, 2.3763, 0.0225, 0.1078, 0.4772, 0.2080],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4000, 0.4000, 0.8000, 0.0000, 0.0860, 0.4320, 0.2500]) tensor(7.4222, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.1075e+01, 1.2508e+00, 3.6343e+00, 1.0211e-03, 6.7889e-02, 4.7960e-01,\n",
      "        2.5494e-01], grad_fn=<AddBackward0>) tensor([5.3000, 1.1000, 2.7000, 0.0340, 0.0680, 0.5230, 0.3340]) tensor(4.8936, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.2092,  4.4256,  3.6106,  0.0378,  0.1458,  0.5528,  0.2313],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9800e+01, 4.8000e+00, 4.6000e+00, 1.0000e-02, 1.3700e-01, 5.1200e-01,\n",
      "        2.3300e-01]) tensor(2.0021, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.8688,  4.8076,  3.8876,  0.0301,  0.1335,  0.5643,  0.2323],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 1.0000, 1.3000, 0.0220, 0.0640, 0.5460, 0.1540]) tensor(16.3845, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1444,  4.9275,  3.4438,  0.0369,  0.1359,  0.5684,  0.1915],\n",
      "       grad_fn=<AddBackward0>) tensor([14.5000,  4.0000,  4.1000,  0.0330,  0.1060,  0.6450,  0.1950]) tensor(0.2035, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 5.3156,  4.9632, -0.0756,  0.0873,  0.2065,  0.5449,  0.0253],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 2.0000, 0.1000, 0.1130, 0.1910, 0.4060, 0.0130]) tensor(3.4519, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2345,  4.8876,  2.8390,  0.0435,  0.1487,  0.5649,  0.1697],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 3.7000, 0.9000, 0.0730, 0.1760, 0.5810, 0.0810]) tensor(4.0789, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.7072,  5.9355,  3.6699,  0.0446,  0.1620,  0.5897,  0.2106],\n",
      "       grad_fn=<AddBackward0>) tensor([22.6000,  4.2000,  3.1000,  0.0340,  0.0910,  0.5710,  0.1460]) tensor(3.8980, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6487,  3.7823,  3.0328,  0.0338,  0.1285,  0.5458,  0.1911],\n",
      "       grad_fn=<AddBackward0>) tensor([15.6000,  3.9000,  3.7000,  0.0320,  0.0860,  0.5460,  0.1700]) tensor(1.3102, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.3008,  5.7265,  3.2853,  0.0461,  0.1600,  0.5902,  0.1870],\n",
      "       grad_fn=<AddBackward0>) tensor([17.3000, 10.7000,  2.0000,  0.0860,  0.2540,  0.5320,  0.1020]) tensor(3.9153, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.2678, 1.4711, 0.9050, 0.0449, 0.1317, 0.4612, 0.1266],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 5.0000, 0.8000, 0.1270, 0.3200, 0.6380, 0.1100]) tensor(1.9153, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2786, 2.0383, 1.9830, 0.0466, 0.1340, 0.5069, 0.1618],\n",
      "       grad_fn=<AddBackward0>) tensor([0., 0., 0., 0., 0., 0., 0.]) tensor(10.9893, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.3709,  4.9731,  3.0556,  0.0430,  0.1451,  0.5707,  0.1719],\n",
      "       grad_fn=<AddBackward0>) tensor([12.3000,  6.0000,  1.9000,  0.0520,  0.1550,  0.5490,  0.0870]) tensor(0.5064, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1046,  7.4906,  1.2667,  0.0951,  0.2452,  0.6053,  0.0877],\n",
      "       grad_fn=<AddBackward0>) tensor([10.2000,  7.2000,  1.9000,  0.1240,  0.2360,  0.5280,  0.1420]) tensor(2.2487, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.5180,  6.6729,  1.8201,  0.0785,  0.2180,  0.5871,  0.1251],\n",
      "       grad_fn=<AddBackward0>) tensor([17.1000,  6.5000,  1.6000,  0.0470,  0.1560,  0.4920,  0.0850]) tensor(0.9658, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 4.4654,  3.9860, -0.0050,  0.0737,  0.1828,  0.5213,  0.0420],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 1.5000, 0.2000, 0.0340, 0.1290, 0.6150, 0.0370]) tensor(1.1195, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.9292,  5.0622,  4.3923,  0.0247,  0.1229,  0.5837,  0.2415],\n",
      "       grad_fn=<AddBackward0>) tensor([14.6000,  2.8000,  6.1000,  0.0180,  0.0790,  0.5490,  0.2870]) tensor(2.7318, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.3251,  0.2749,  0.0380,  0.0409,  0.1130,  0.4366,  0.0842],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 4.1000, 0.8000, 0.0500, 0.1720, 0.5740, 0.0690]) tensor(7.1917, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.7007e+01, 4.2108e+00, 4.1862e+00, 1.5883e-02, 1.0999e-01, 5.5911e-01,\n",
      "        2.4971e-01], grad_fn=<AddBackward0>) tensor([11.6000,  2.8000,  4.4000,  0.0160,  0.0870,  0.4780,  0.2400]) tensor(4.4680, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.4293, 3.2723, 0.5190, 0.0675, 0.1683, 0.4989, 0.0771],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 1.6000, 0.4000, 0.0870, 0.1640, 0.4510, 0.0750]) tensor(1.7129, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.2496, 3.8176, 0.7612, 0.0653, 0.1697, 0.5219, 0.0812],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([4.7000, 3.3000, 0.7000, 0.0810, 0.1710, 0.5670, 0.0760]) tensor(0.3822, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2219,  3.2948,  2.3476,  0.0356,  0.1329,  0.5310,  0.1728],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 1.8000, 1.3000, 0.0900, 0.0910, 0.4620, 0.1860]) tensor(6.9323, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2296, 3.4328, 1.9681, 0.0425, 0.1373, 0.5312, 0.1414],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 2.1000, 1.5000, 0.0150, 0.0900, 0.5440, 0.1110]) tensor(0.2881, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.3856,  5.9220,  4.0563,  0.0401,  0.1527,  0.5943,  0.2234],\n",
      "       grad_fn=<AddBackward0>) tensor([16.9000,  6.4000,  9.8000,  0.0330,  0.1510,  0.5100,  0.4190]) tensor(5.0673, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1635,  3.2600,  2.5395,  0.0362,  0.1269,  0.5127,  0.1773],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 1.2000, 0.7000, 0.0160, 0.0650, 0.5270, 0.0630]) tensor(3.9382, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6917, 3.8283, 1.4160, 0.0613, 0.1691, 0.5120, 0.1285],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 3.0000, 0.5000, 0.0410, 0.1920, 0.5650, 0.0530]) tensor(1.4138, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0949,  4.1654,  3.4495,  0.0297,  0.1294,  0.5483,  0.2185],\n",
      "       grad_fn=<AddBackward0>) tensor([13.2000,  2.7000,  6.5000,  0.0200,  0.0720,  0.4870,  0.3430]) tensor(2.1524, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9211,  4.3221,  2.2884,  0.0444,  0.1450,  0.5481,  0.1473],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 1.5000, 0.5000, 0.0240, 0.1080, 0.4620, 0.0750]) tensor(8.4397, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.6004,  4.8616,  3.5818,  0.0335,  0.1405,  0.5687,  0.2185],\n",
      "       grad_fn=<AddBackward0>) tensor([13.2000,  3.5000,  2.7000,  0.0150,  0.1280,  0.5060,  0.1620]) tensor(2.0289, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9319,  4.7738,  0.6611,  0.0850,  0.2260,  0.5280,  0.1156],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 0.7000, 0.0000, 0.0480, 0.2500, 0.5060, 0.0000]) tensor(14.3485, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8229,  4.7734,  2.0155,  0.0544,  0.1618,  0.5572,  0.1299],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 2.5000, 1.2000, 0.0640, 0.0910, 0.5050, 0.1150]) tensor(3.7568, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.8884, 0.7850, 0.7351, 0.0380, 0.1186, 0.4332, 0.1308],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 0.9000, 0.3000, 0.0580, 0.1030, 0.4950, 0.0910]) tensor(0.1694, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4244, 4.0641, 0.4409, 0.0787, 0.1890, 0.5213, 0.0636],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 3.1000, 0.1000, 0.1040, 0.2560, 0.5600, 0.0100]) tensor(1.0611, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.7291, 3.7699, 0.6784, 0.0599, 0.1606, 0.5246, 0.0722],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 1.5000, 0.2000, 0.0580, 0.1190, 0.5450, 0.0280]) tensor(1.9128, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6932,  3.4175,  2.3625,  0.0401,  0.1360,  0.5177,  0.1732],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 3.7000, 2.9000, 0.0350, 0.1370, 0.5010, 0.1960]) tensor(1.4196, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.4646, 3.6727, 0.4506, 0.0726, 0.1763, 0.5180, 0.0615],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 5.4000, 0.4000, 0.0800, 0.1960, 0.4680, 0.0290]) tensor(0.4681, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.6817,  5.2122,  4.9542,  0.0239,  0.1312,  0.5849,  0.2842],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8800e+01, 4.2000e+00, 7.5000e+00, 1.2000e-02, 1.0400e-01, 6.2300e-01,\n",
      "        3.3400e-01]) tensor(10.4882, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9518,  5.9649,  2.9455,  0.0557,  0.1718,  0.5906,  0.1632],\n",
      "       grad_fn=<AddBackward0>) tensor([11.0000,  5.0000,  2.3000,  0.0380,  0.1760,  0.5320,  0.1420]) tensor(2.4240, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5546,  5.7707,  2.2092,  0.0675,  0.1861,  0.5650,  0.1364],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 4.4000, 0.7000, 0.1220, 0.1790, 0.5620, 0.0610]) tensor(9.4086, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0932, 6.3772, 0.5919, 0.0920, 0.2177, 0.5799, 0.0353],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 6.0000, 1.4000, 0.0700, 0.1870, 0.4790, 0.0780]) tensor(2.2808, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2587, 2.3644, 1.9572, 0.0445, 0.1411, 0.4985, 0.1732],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 0.7000, 0.0000, 0.0000, 0.1540, 0.4370, 0.0000]) tensor(8.0660, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.3643,  5.6315,  2.3494,  0.0649,  0.1863,  0.5722,  0.1482],\n",
      "       grad_fn=<AddBackward0>) tensor([14.2000,  6.1000,  1.4000,  0.0490,  0.2060,  0.5350,  0.0830]) tensor(0.2608, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4655,  4.9657,  3.6120,  0.0354,  0.1371,  0.5720,  0.2061],\n",
      "       grad_fn=<AddBackward0>) tensor([19.0000,  4.5000,  6.0000,  0.0220,  0.0950,  0.5700,  0.2430]) tensor(2.6308, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3235,  3.0871,  2.3806,  0.0409,  0.1472,  0.5195,  0.1955],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 0.5000, 0.4000, 0.1670, 0.0540, 0.3930, 0.3330]) tensor(20.1675, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9732, 3.6130, 1.7394, 0.0498, 0.1452, 0.5175, 0.1292],\n",
      "       grad_fn=<AddBackward0>) tensor([8.2000e+00, 1.9000e+00, 2.1000e+00, 7.0000e-03, 5.9000e-02, 5.5000e-01,\n",
      "        1.0700e-01]) tensor(0.4467, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.8288, 4.3656, 1.2930, 0.0635, 0.1697, 0.5315, 0.1012],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 6.4000, 1.7000, 0.1040, 0.1940, 0.6020, 0.1100]) tensor(0.6559, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6993, 2.5643, 1.0243, 0.0505, 0.1432, 0.4922, 0.1124],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 1.0000, 0.3000, 0.0170, 0.1130, 0.5840, 0.0430]) tensor(1.3919, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1273, 3.9599, 2.3462, 0.0407, 0.1292, 0.5402, 0.1387],\n",
      "       grad_fn=<AddBackward0>) tensor([8.5000, 6.0000, 1.0000, 0.0820, 0.1640, 0.6130, 0.0530]) tensor(0.9119, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2251,  4.6457,  2.9603,  0.0433,  0.1468,  0.5439,  0.1861],\n",
      "       grad_fn=<AddBackward0>) tensor([8.3000, 3.0000, 4.1000, 0.0190, 0.1010, 0.5180, 0.2130]) tensor(4.0383, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.6253,  4.8072,  4.0984,  0.0276,  0.1271,  0.5725,  0.2326],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4200e+01, 1.7000e+00, 3.2000e+00, 1.2000e-02, 5.7000e-02, 5.4500e-01,\n",
      "        1.7800e-01]) tensor(2.3362, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7984,  5.6146,  2.4740,  0.0565,  0.1674,  0.5876,  0.1345],\n",
      "       grad_fn=<AddBackward0>) tensor([10.1000,  9.4000,  0.4000,  0.1230,  0.2540,  0.6970,  0.0240]) tensor(3.7070, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4076, 4.3769, 1.6037, 0.0566, 0.1577, 0.5404, 0.1078],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 5.8000, 1.3000, 0.1010, 0.1380, 0.5520, 0.0850]) tensor(0.5112, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.2956, 4.8919, 0.5462, 0.0733, 0.1787, 0.5480, 0.0396],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 2.4000, 0.3000, 0.0640, 0.1200, 0.4810, 0.0330]) tensor(2.5441, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.8452, 2.6763, 0.8394, 0.0561, 0.1485, 0.5026, 0.0923],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 2.8000, 2.2000, 0.0150, 0.1290, 0.4280, 0.1550]) tensor(0.4244, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9054, 1.7196, 1.3815, 0.0420, 0.1277, 0.4720, 0.1445],\n",
      "       grad_fn=<AddBackward0>) tensor([3.9000, 3.0000, 0.7000, 0.0480, 0.1630, 0.4300, 0.0800]) tensor(0.8761, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.8068,  4.8807,  3.0570,  0.0469,  0.1523,  0.5616,  0.1821],\n",
      "       grad_fn=<AddBackward0>) tensor([11.6000,  3.6000,  2.0000,  0.0250,  0.1140,  0.5020,  0.1120]) tensor(1.0911, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.8139, 0.2003, 1.3103, 0.0263, 0.0887, 0.4315, 0.1422],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 1.0000, 0.9000, 0.0230, 0.0710, 0.3410, 0.1400]) tensor(0.1177, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.8910,  6.6040,  2.0941,  0.0742,  0.2046,  0.5841,  0.1273],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 3.9000, 0.9000, 0.0910, 0.1200, 0.5200, 0.0730]) tensor(6.2079, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5562, 4.7784, 0.5826, 0.0811, 0.2015, 0.5449, 0.0686],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 2.0000, 0.2000, 0.1540, 0.2250, 0.5890, 0.0640]) tensor(5.5349, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.3835, 3.3231, 0.2855, 0.0730, 0.1773, 0.5099, 0.0637],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 2.2000, 0.4000, 0.1720, 0.1670, 0.3130, 0.0880]) tensor(1.6369, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.8531,  2.9274, -0.3501,  0.0763,  0.1791,  0.4870,  0.0400],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 4.1000, 0.1000, 0.1480, 0.2420, 0.4760, 0.0180]) tensor(0.2553, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.4884,  4.3551,  2.2951,  0.0460,  0.1501,  0.5463,  0.1534],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 3.7000, 0.6000, 0.0550, 0.1790, 0.4180, 0.0590]) tensor(5.4288, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.2372,  4.5867,  3.8801,  0.0291,  0.1350,  0.5615,  0.2411],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3700e+01, 4.7000e+00, 4.5000e+00, 1.8000e-02, 1.1300e-01, 5.7400e-01,\n",
      "        2.2400e-01]) tensor(6.0237, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.8515, 2.0162, 0.5375, 0.0533, 0.1455, 0.4697, 0.1004],\n",
      "       grad_fn=<AddBackward0>) tensor([0.8000, 0.5000, 0.3000, 0.0000, 0.0710, 0.2500, 0.0770]) tensor(1.6748, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.3586e+01, 3.2215e+00, 4.1273e+00, 1.2031e-02, 8.9273e-02, 5.3047e-01,\n",
      "        2.4210e-01], grad_fn=<AddBackward0>) tensor([7.5000, 2.2000, 4.8000, 0.0240, 0.0890, 0.4930, 0.3130]) tensor(5.5059, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.0465, 2.7909, 0.9648, 0.0486, 0.1365, 0.5088, 0.0913],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 2.0000, 0.7000, 0.0590, 0.1100, 0.4900, 0.0830]) tensor(0.9546, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0773, 1.7524, 2.5205, 0.0252, 0.1086, 0.4764, 0.2041],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 1.2000, 1.3000, 0.0130, 0.0880, 0.3480, 0.2000]) tensor(4.3897, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.7372, 2.4281, 1.1683, 0.0558, 0.1555, 0.4815, 0.1363],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 0.2000, 0.0000, 0.0000, 0.0290, 0.4030, 0.0000]) tensor(4.3927, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.0297, 3.3573, 0.6902, 0.0607, 0.1532, 0.5083, 0.0704],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 1.7000, 0.5000, 0.0670, 0.1360, 0.5150, 0.0710]) tensor(1.1729, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0040,  4.7886,  2.1072,  0.0649,  0.1839,  0.5319,  0.1603],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 2.1000, 0.6000, 0.0710, 0.1790, 0.4220, 0.0930]) tensor(11.6908, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1271, 4.7229, 1.0465, 0.0729, 0.1878, 0.5443, 0.0875],\n",
      "       grad_fn=<AddBackward0>) tensor([14.3000, 10.0000,  4.3000,  0.0820,  0.2100,  0.5330,  0.1900]) tensor(10.9355, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0375,  4.4707,  2.0699,  0.0494,  0.1545,  0.5530,  0.1378],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 1.1000, 0.8000, 0.0200, 0.0830, 0.5160, 0.0910]) tensor(7.4130, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5249, 4.4403, 2.2749, 0.0508, 0.1471, 0.5544, 0.1292],\n",
      "       grad_fn=<AddBackward0>) tensor([ 4.7000, 10.3000,  2.1000,  0.1360,  0.3550,  0.4260,  0.1140]) tensor(8.2448, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.2579,  5.6050,  4.3892,  0.0373,  0.1525,  0.5824,  0.2545],\n",
      "       grad_fn=<AddBackward0>) tensor([18.2000,  6.4000,  1.9000,  0.1010,  0.1540,  0.5170,  0.1180]) tensor(1.1392, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.5889,  6.2478,  3.3003,  0.0591,  0.1823,  0.5962,  0.1850],\n",
      "       grad_fn=<AddBackward0>) tensor([15.0000,  9.7000,  0.9000,  0.0850,  0.2650,  0.5160,  0.0510]) tensor(2.8908, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5831, 2.1229, 2.1254, 0.0284, 0.1068, 0.4994, 0.1599],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 0.9000, 1.5000, 0.0080, 0.0730, 0.4450, 0.1930]) tensor(2.6520, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.7030, 3.0412, 0.9222, 0.0597, 0.1597, 0.5018, 0.1022],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 3.1000, 0.6000, 0.1060, 0.2100, 0.5250, 0.0750]) tensor(0.2229, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.9795, 2.1975, 0.7576, 0.0520, 0.1402, 0.4900, 0.0950],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 2.2000, 0.4000, 0.0000, 0.1630, 0.4970, 0.0450]) tensor(0.0443, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.6220, 4.1639, 0.1442, 0.0815, 0.1877, 0.5259, 0.0347],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 3.3000, 0.2000, 0.0870, 0.2770, 0.3600, 0.0230]) tensor(0.9502, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7084,  4.2166,  2.8276,  0.0388,  0.1385,  0.5459,  0.1804],\n",
      "       grad_fn=<AddBackward0>) tensor([10.0000,  2.3000,  3.8000,  0.0190,  0.0880,  0.5430,  0.2510]) tensor(1.7089, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2519,  3.7308,  3.0278,  0.0299,  0.1167,  0.5404,  0.1804],\n",
      "       grad_fn=<AddBackward0>) tensor([15.1000,  3.8000,  2.2000,  0.0210,  0.1010,  0.5890,  0.1030]) tensor(2.2152, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1743,  5.7354,  2.1433,  0.0630,  0.1779,  0.5771,  0.1239],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 7.8000, 1.2000, 0.0850, 0.2110, 0.5580, 0.0570]) tensor(1.9172, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2114, 3.7113, 2.1118, 0.0505, 0.1477, 0.5160, 0.1507],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 2.9000, 3.3000, 0.0520, 0.1570, 0.4530, 0.2860]) tensor(5.1236, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.5608,  5.1199,  2.9928,  0.0476,  0.1590,  0.5685,  0.1824],\n",
      "       grad_fn=<AddBackward0>) tensor([14.8000,  7.6000,  1.7000,  0.1080,  0.1940,  0.5650,  0.1110]) tensor(1.1271, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0273,  3.1115,  2.2580,  0.0399,  0.1340,  0.5101,  0.1718],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 3.1000, 2.5000, 0.0260, 0.1180, 0.5680, 0.1720]) tensor(0.1318, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6885, 4.2290, 0.7549, 0.0693, 0.1750, 0.5230, 0.0719],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 4.0000, 0.5000, 0.0920, 0.1870, 0.4740, 0.0540]) tensor(0.3338, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3654, 2.3773, 2.4085, 0.0336, 0.1105, 0.4939, 0.1663],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 1.0000, 1.1000, 0.0110, 0.1010, 0.4180, 0.1720]) tensor(4.4771, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9395,  5.1760,  2.5711,  0.0564,  0.1673,  0.5671,  0.1547],\n",
      "       grad_fn=<AddBackward0>) tensor([12.0000,  5.8000,  2.4000,  0.0700,  0.1310,  0.5540,  0.1350]) tensor(0.1862, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2755, 5.1924, 1.3538, 0.0711, 0.1859, 0.5567, 0.0930],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 2.2000, 1.2000, 0.0350, 0.1210, 0.5320, 0.1130]) tensor(3.8950, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5174,  4.2275,  2.4367,  0.0423,  0.1383,  0.5418,  0.1514],\n",
      "       grad_fn=<AddBackward0>) tensor([13.7000,  3.4000,  4.9000,  0.0170,  0.0970,  0.5870,  0.2190]) tensor(2.4129, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.0420,  5.8718,  3.0412,  0.0501,  0.1693,  0.5882,  0.1790],\n",
      "       grad_fn=<AddBackward0>) tensor([11.9000,  4.7000,  0.7000,  0.0520,  0.1980,  0.5260,  0.0530]) tensor(3.4330, grad_fn=<MseLossBackward0>)\n",
      "tensor([22.1200,  5.9573,  4.6951,  0.0353,  0.1580,  0.5883,  0.2816],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9300e+01, 3.5000e+00, 6.1000e+00, 1.9000e-02, 7.8000e-02, 5.8100e-01,\n",
      "        2.6100e-01]) tensor(8.5102, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3960,  4.3571,  2.7121,  0.0403,  0.1357,  0.5490,  0.1612],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 3.5000, 1.6000, 0.0390, 0.1330, 0.4990, 0.1120]) tensor(2.6790, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2490,  6.6912,  1.2911,  0.0888,  0.2266,  0.5787,  0.0931],\n",
      "       grad_fn=<AddBackward0>) tensor([6.2000, 5.7000, 0.7000, 0.1380, 0.2100, 0.5710, 0.0620]) tensor(5.4181, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1300, 0.5695, 1.6181, 0.0269, 0.1074, 0.4455, 0.1827],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 1.1000, 0.1000, 0.0930, 0.1560, 0.5410, 0.0420]) tensor(1.9586, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4914, 4.0866, 1.4433, 0.0631, 0.1685, 0.5281, 0.1149],\n",
      "       grad_fn=<AddBackward0>) tensor([6.7000, 4.6000, 1.0000, 0.0920, 0.1300, 0.5900, 0.0600]) tensor(0.5255, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4163,  4.2975,  2.3962,  0.0420,  0.1362,  0.5525,  0.1418],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 5.5000, 1.9000, 0.0920, 0.1520, 0.6020, 0.1130]) tensor(1.3757, grad_fn=<MseLossBackward0>)\n",
      "tensor([-3.9555, -0.1329, -0.8431,  0.0495,  0.1235,  0.3934,  0.0556],\n",
      "       grad_fn=<AddBackward0>) tensor([0.5000, 0.8000, 0.0000, 0.0000, 0.1430, 0.4100, 0.0000]) tensor(3.0626, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.9668, 1.4982, 0.4333, 0.0467, 0.1321, 0.4770, 0.0910],\n",
      "       grad_fn=<AddBackward0>) tensor([0.5000, 0.3000, 0.2000, 0.0220, 0.0240, 0.1250, 0.0330]) tensor(1.1021, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.3240, 3.9112, 0.5991, 0.0756, 0.1872, 0.5128, 0.0831],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4000, 2.3000, 0.2000, 0.1360, 0.1770, 0.5020, 0.0440]) tensor(2.5940, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7498,  5.7043,  2.4498,  0.0600,  0.1687,  0.5776,  0.1287],\n",
      "       grad_fn=<AddBackward0>) tensor([6.9000, 6.8000, 3.2000, 0.0520, 0.1990, 0.4590, 0.1570]) tensor(3.6143, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.3959, 3.0216, 0.4047, 0.0650, 0.1607, 0.5039, 0.0644],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 1.7000, 0.3000, 0.0820, 0.1900, 0.3730, 0.0580]) tensor(0.7672, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.9214, 1.9424, 0.9721, 0.0462, 0.1325, 0.4883, 0.1117],\n",
      "       grad_fn=<AddBackward0>) tensor([6.7000, 2.2000, 1.7000, 0.0280, 0.0870, 0.5330, 0.1010]) tensor(0.5378, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3746, 2.5837, 2.2575, 0.0299, 0.1080, 0.5065, 0.1536],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 3.6000, 4.4000, 0.0260, 0.1300, 0.3970, 0.2730]) tensor(0.9721, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9188, 4.3530, 1.2586, 0.0648, 0.1675, 0.5350, 0.0894],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 3.1000, 0.4000, 0.1180, 0.2190, 0.4140, 0.0600]) tensor(2.9972, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.5007, 0.5614, 0.1673, 0.0414, 0.1173, 0.4347, 0.0934],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 1.9000, 0.3000, 0.1170, 0.0940, 0.5130, 0.0450]) tensor(0.8902, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5664,  6.6023,  2.2626,  0.0725,  0.2082,  0.5913,  0.1447],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 3.1000, 0.5000, 0.0890, 0.1260, 0.4230, 0.0540]) tensor(13.9451, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.2777, 3.7734, 1.0346, 0.0663, 0.1690, 0.5134, 0.0951],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 3.8000, 0.6000, 0.0890, 0.1540, 0.4960, 0.0560]) tensor(1.1297, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9112, 4.6296, 1.2805, 0.0658, 0.1771, 0.5429, 0.1006],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 1.8000, 0.3000, 0.0800, 0.1290, 0.5430, 0.0600]) tensor(6.6171, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4948, 2.2049, 1.9868, 0.0358, 0.1206, 0.4889, 0.1620],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 1.0000, 0.2000, 0.0360, 0.1040, 0.4170, 0.0400]) tensor(4.5215, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.3036,  5.5928,  2.9920,  0.0516,  0.1671,  0.5842,  0.1755],\n",
      "       grad_fn=<AddBackward0>) tensor([19.2000,  5.5000,  2.0000,  0.0280,  0.1370,  0.6000,  0.0890]) tensor(2.3120, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.0106,  5.8200,  2.6835,  0.0591,  0.1770,  0.5799,  0.1558],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 6.3000, 1.5000, 0.1260, 0.2180, 0.4640, 0.1220]) tensor(5.5702, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2533,  4.8829,  2.0640,  0.0561,  0.1658,  0.5567,  0.1342],\n",
      "       grad_fn=<AddBackward0>) tensor([13.8000, 11.0000,  1.5000,  0.1260,  0.2420,  0.5390,  0.0730]) tensor(6.3197, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7901,  2.3453,  2.8513,  0.0203,  0.1038,  0.4992,  0.2091],\n",
      "       grad_fn=<AddBackward0>) tensor([7.2000, 2.3000, 1.4000, 0.0160, 0.1120, 0.4940, 0.1280]) tensor(2.1434, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.9027,  8.9353,  3.0599,  0.0864,  0.2438,  0.6488,  0.1613],\n",
      "       grad_fn=<AddBackward0>) tensor([20.0000,  9.5000,  2.6000,  0.1210,  0.1840,  0.5540,  0.1280]) tensor(0.1943, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.0898, 3.7937, 1.1880, 0.0624, 0.1549, 0.5246, 0.0804],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 2.5000, 1.0000, 0.0830, 0.1800, 0.5290, 0.1270]) tensor(1.7907, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.3859e+01, 2.7839e+00, 4.0104e+00, 4.5485e-03, 8.1543e-02, 5.2963e-01,\n",
      "        2.4738e-01], grad_fn=<AddBackward0>) tensor([9.4000, 2.1000, 1.9000, 0.0230, 0.0960, 0.4760, 0.1760]) tensor(3.5453, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7821, 4.2247, 1.7736, 0.0570, 0.1582, 0.5399, 0.1185],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 5.0000, 1.8000, 0.0570, 0.1610, 0.4490, 0.1000]) tensor(0.8312, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0052,  3.9405,  3.4345,  0.0344,  0.1371,  0.5351,  0.2297],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 2.5000, 3.6000, 0.0140, 0.1250, 0.5070, 0.2920]) tensor(6.1620, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.6899,  4.3310,  4.3432,  0.0251,  0.1289,  0.5645,  0.2679],\n",
      "       grad_fn=<AddBackward0>) tensor([21.8000,  3.4000,  7.9000,  0.0220,  0.0830,  0.5320,  0.3790]) tensor(3.3151, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9441, 2.8360, 1.2358, 0.0528, 0.1506, 0.5040, 0.1223],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 3.8000, 0.9000, 0.0690, 0.1260, 0.4730, 0.0650]) tensor(0.2543, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.2298, 2.0658, 1.8845, 0.0345, 0.1186, 0.4827, 0.1619],\n",
      "       grad_fn=<AddBackward0>) tensor([10.8000,  5.2000,  3.3000,  0.0360,  0.1540,  0.4960,  0.1640]) tensor(3.5106, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0709, 3.2133, 1.8153, 0.0428, 0.1284, 0.5180, 0.1245],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 2.7000, 0.6000, 0.0320, 0.1230, 0.4770, 0.0550]) tensor(2.2810, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.1812, 2.4214, 1.1591, 0.0506, 0.1401, 0.4896, 0.1152],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 0.8000, 0.5000, 0.0390, 0.0800, 0.4650, 0.1080]) tensor(1.7079, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9154,  5.2175,  1.7084,  0.0628,  0.1778,  0.5664,  0.1130],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 3.0000, 0.5000, 0.0660, 0.1620, 0.5220, 0.0560]) tensor(3.8245, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.4934, 2.3717, 1.7005, 0.0397, 0.1244, 0.4824, 0.1453],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 2.1000, 0.9000, 0.0140, 0.0980, 0.4700, 0.0730]) tensor(0.6152, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2165,  4.6643,  3.2901,  0.0356,  0.1342,  0.5662,  0.1906],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 2.1000, 3.4000, 0.0230, 0.0890, 0.5560, 0.2490]) tensor(4.6817, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.7521,  5.5819,  3.5680,  0.0426,  0.1497,  0.5893,  0.1922],\n",
      "       grad_fn=<AddBackward0>) tensor([13.6000,  3.4000,  2.6000,  0.0420,  0.0860,  0.5870,  0.1390]) tensor(1.4766, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1108, 4.0662, 1.4511, 0.0543, 0.1588, 0.5439, 0.1116],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 2.4000, 0.5000, 0.0430, 0.1550, 0.4400, 0.0570]) tensor(5.1869, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.6047, 5.2418, 0.0855, 0.0883, 0.2115, 0.5510, 0.0356],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 5.9000, 0.8000, 0.1250, 0.2250, 0.5340, 0.0620]) tensor(0.1874, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.8840, 2.8837, 0.7016, 0.0548, 0.1487, 0.5043, 0.0854],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 0.9000, 0.1000, 0.0280, 0.1040, 0.5490, 0.0300]) tensor(1.5689, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4835, 2.6453, 2.9490, 0.0264, 0.1048, 0.5125, 0.1894],\n",
      "       grad_fn=<AddBackward0>) tensor([10.5000,  2.8000,  2.9000,  0.0130,  0.1060,  0.6030,  0.2120]) tensor(0.1526, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.4486, 3.8612, 0.6577, 0.0665, 0.1751, 0.5179, 0.0828],\n",
      "       grad_fn=<AddBackward0>) tensor([6.7000, 5.1000, 0.9000, 0.0730, 0.1860, 0.4430, 0.0670]) tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.6804,  5.4705,  3.7403,  0.0401,  0.1547,  0.5803,  0.2228],\n",
      "       grad_fn=<AddBackward0>) tensor([18.8000,  3.2000,  6.2000,  0.0250,  0.0920,  0.5350,  0.3440]) tensor(1.7828, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3935, 3.5764, 1.0374, 0.0583, 0.1532, 0.5193, 0.0849],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 0.9000, 0.5000, 0.0280, 0.1230, 0.6410, 0.1210]) tensor(2.9119, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4708, 1.8630, 2.5408, 0.0268, 0.1096, 0.4799, 0.2029],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 1.4000, 0.8000, 0.0480, 0.1280, 0.2500, 0.1540]) tensor(9.7769, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3938,  4.8399,  2.5941,  0.0439,  0.1470,  0.5667,  0.1534],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 3.7000, 2.3000, 0.0320, 0.1230, 0.5310, 0.1400]) tensor(1.6555, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 4.5389,  4.2590, -0.7122,  0.1001,  0.2310,  0.5029,  0.0347],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 1.2000, 0.3000, 0.0860, 0.2500, 0.5500, 0.1580]) tensor(2.6370, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9911, 4.4151, 2.1679, 0.0508, 0.1451, 0.5497, 0.1229],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 4.8000, 3.0000, 0.0140, 0.1460, 0.5590, 0.1310]) tensor(0.5288, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.1745, 4.6572, 0.7694, 0.0703, 0.1754, 0.5392, 0.0606],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 3.0000, 0.4000, 0.1040, 0.1230, 0.5210, 0.0380]) tensor(2.6691, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5753, 1.2576, 2.2529, 0.0174, 0.0841, 0.4810, 0.1721],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 1.1000, 1.1000, 0.0270, 0.1010, 0.4490, 0.1680]) tensor(2.5662, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.7532,  5.1272,  3.5222,  0.0396,  0.1454,  0.5771,  0.2022],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000e+01, 3.8000e+00, 2.5000e+00, 1.4000e-02, 8.9000e-02, 5.9900e-01,\n",
      "        1.0800e-01]) tensor(2.9793, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1426,  3.6381,  2.3014,  0.0409,  0.1409,  0.5267,  0.1692],\n",
      "       grad_fn=<AddBackward0>) tensor([8.5000, 3.9000, 0.8000, 0.0400, 0.1310, 0.5450, 0.0570]) tensor(1.3313, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4832,  5.7019,  2.3179,  0.0662,  0.1933,  0.5714,  0.1562],\n",
      "       grad_fn=<AddBackward0>) tensor([14.0000,  5.6000,  1.8000,  0.0650,  0.1700,  0.5710,  0.1100]) tensor(0.0735, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8825,  3.9983,  3.5434,  0.0270,  0.1147,  0.5484,  0.2033],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([6.9000, 2.1000, 3.6000, 0.0240, 0.1090, 0.5440, 0.3160]) tensor(5.6299, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.5125e+01, 3.5873e+00, 4.5520e+00, 7.3337e-03, 8.3191e-02, 5.5383e-01,\n",
      "        2.5066e-01], grad_fn=<AddBackward0>) tensor([9.6000, 3.6000, 5.3000, 0.0250, 0.1000, 0.4720, 0.2670]) tensor(4.4422, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4585,  4.8615,  2.8294,  0.0484,  0.1514,  0.5542,  0.1679],\n",
      "       grad_fn=<AddBackward0>) tensor([10.5000,  8.6000,  1.9000,  0.0970,  0.1860,  0.5270,  0.0970]) tensor(2.6693, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0985, 3.2751, 1.1820, 0.0548, 0.1542, 0.5139, 0.1112],\n",
      "       grad_fn=<AddBackward0>) tensor([7.7000, 4.6000, 2.8000, 0.0200, 0.1820, 0.5080, 0.1650]) tensor(0.6771, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3428, 2.5222, 1.7449, 0.0442, 0.1359, 0.4874, 0.1530],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 3.9000, 1.0000, 0.0510, 0.1150, 0.4570, 0.0590]) tensor(0.3818, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.2579,  5.8343,  2.9188,  0.0530,  0.1703,  0.5848,  0.1703],\n",
      "       grad_fn=<AddBackward0>) tensor([15.1000,  3.4000,  1.0000,  0.0370,  0.1060,  0.5630,  0.0650]) tensor(1.3783, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.9949e+01, 4.3490e+00, 5.5739e+00, 3.8127e-03, 9.1100e-02, 5.7152e-01,\n",
      "        3.0858e-01], grad_fn=<AddBackward0>) tensor([20.4000,  4.2000,  8.3000,  0.0280,  0.0950,  0.5040,  0.3660]) tensor(1.0951, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7731, 3.1018, 1.3909, 0.0518, 0.1527, 0.4965, 0.1359],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 2.2000, 0.3000, 0.0750, 0.1520, 0.5540, 0.0450]) tensor(2.1118, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.0868, 3.1727, 0.7915, 0.0624, 0.1664, 0.5014, 0.0999],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 0.5000, 0.0000, 0.0000, 0.1430, 0.2500, 0.0000]) tensor(4.8175, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4121,  5.9414,  2.6157,  0.0544,  0.1705,  0.5890,  0.1482],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 3.6000, 0.4000, 0.0990, 0.1540, 0.4870, 0.0350]) tensor(14.4138, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7374, 3.1332, 2.2998, 0.0388, 0.1312, 0.5123, 0.1685],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 0.7000, 0.3000, 0.0330, 0.0660, 0.5300, 0.0580]) tensor(6.1220, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4137,  4.5206,  4.1120,  0.0261,  0.1232,  0.5660,  0.2371],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5900e+01, 2.5000e+00, 3.0000e+00, 1.5000e-02, 7.2000e-02, 5.2100e-01,\n",
      "        1.4900e-01]) tensor(0.7994, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0838,  6.3724,  2.5486,  0.0599,  0.1827,  0.5946,  0.1456],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 2.9000, 0.6000, 0.0290, 0.1390, 0.5220, 0.0500]) tensor(11.6029, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9745, 2.4334, 1.4888, 0.0455, 0.1375, 0.4888, 0.1415],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 2.8000, 0.6000, 0.1100, 0.1870, 0.5770, 0.0900]) tensor(2.1697, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.8532, 5.4959, 1.0699, 0.0779, 0.1923, 0.5595, 0.0669],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 3.4000, 0.6000, 0.0810, 0.1910, 0.4500, 0.0650]) tensor(4.3086, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.2223,  5.5715,  3.4396,  0.0455,  0.1621,  0.5823,  0.2062],\n",
      "       grad_fn=<AddBackward0>) tensor([16.2000,  4.0000,  4.5000,  0.0460,  0.0880,  0.5300,  0.2410]) tensor(0.6641, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8865, 5.6053, 1.2928, 0.0706, 0.1829, 0.5718, 0.0734],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 8.4000, 0.4000, 0.1280, 0.2900, 0.6110, 0.0230]) tensor(3.6179, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1248,  5.2974,  2.8261,  0.0503,  0.1629,  0.5761,  0.1684],\n",
      "       grad_fn=<AddBackward0>) tensor([18.2000,  5.8000,  1.9000,  0.0360,  0.1460,  0.5700,  0.0890]) tensor(2.5320, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1063,  5.2239,  1.9009,  0.0577,  0.1684,  0.5689,  0.1172],\n",
      "       grad_fn=<AddBackward0>) tensor([7.5000, 4.9000, 1.2000, 0.0870, 0.1600, 0.5400, 0.0920]) tensor(1.9434, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3986,  4.7061,  2.5339,  0.0477,  0.1522,  0.5574,  0.1579],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 2.4000, 1.7000, 0.0250, 0.1630, 0.5310, 0.2040]) tensor(8.0582, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6310,  5.8930,  2.0829,  0.0658,  0.1907,  0.5782,  0.1355],\n",
      "       grad_fn=<AddBackward0>) tensor([13.2000,  6.8000,  1.7000,  0.0890,  0.2070,  0.5480,  0.1260]) tensor(0.1653, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.5016,  4.6885,  4.5623,  0.0243,  0.1293,  0.5619,  0.2733],\n",
      "       grad_fn=<AddBackward0>) tensor([17.6000,  3.7000,  2.8000,  0.0230,  0.1230,  0.4690,  0.1830]) tensor(0.7018, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8132, 2.3809, 1.9006, 0.0423, 0.1278, 0.4962, 0.1480],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 1.0000, 0.4000, 0.0200, 0.1500, 0.3980, 0.1000]) tensor(4.3309, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1714e+00, 7.1258e-01, 3.2699e+00, 3.2644e-03, 6.8974e-02, 4.5938e-01,\n",
      "        2.4544e-01], grad_fn=<AddBackward0>) tensor([2.5000, 0.4000, 0.6000, 0.0000, 0.0610, 0.4250, 0.1900]) tensor(7.3912, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3434,  3.8563,  3.1059,  0.0305,  0.1203,  0.5476,  0.1863],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 1.7000, 1.1000, 0.0080, 0.0990, 0.5620, 0.1050]) tensor(6.9886, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.6181, 3.6068, 1.4872, 0.0517, 0.1487, 0.5274, 0.1133],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 3.1000, 1.1000, 0.0320, 0.1660, 0.5260, 0.0920]) tensor(0.1128, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6488,  6.1082,  2.5545,  0.0596,  0.1809,  0.5917,  0.1477],\n",
      "       grad_fn=<AddBackward0>) tensor([11.1000,  3.6000,  1.6000,  0.0230,  0.1390,  0.5450,  0.1130]) tensor(2.8289, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4455,  6.4574,  2.9593,  0.0583,  0.1833,  0.5954,  0.1698],\n",
      "       grad_fn=<AddBackward0>) tensor([15.4000,  6.1000,  1.8000,  0.0770,  0.1400,  0.5480,  0.0990]) tensor(0.3678, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7636, 1.6874, 2.5511, 0.0206, 0.0970, 0.4825, 0.1964],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 1.3000, 1.6000, 0.0290, 0.1550, 0.4220, 0.3280]) tensor(5.2348, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4119, 2.0799, 0.5852, 0.0499, 0.1527, 0.4645, 0.1227],\n",
      "       grad_fn=<AddBackward0>) tensor([0.1000, 0.1000, 0.0000, 0.0000, 0.1250, 0.1020, 0.0000]) tensor(4.6612, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.6679, 1.4640, 0.4770, 0.0507, 0.1358, 0.4653, 0.0969],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 2.8000, 0.4000, 0.1220, 0.2410, 0.5090, 0.0750]) tensor(0.3222, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1261, 4.1560, 1.6762, 0.0570, 0.1619, 0.5297, 0.1260],\n",
      "       grad_fn=<AddBackward0>) tensor([12.0000,  9.0000,  1.4000,  0.0700,  0.2260,  0.5170,  0.0620]) tensor(4.5440, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.1493, 1.5682, 1.3774, 0.0381, 0.1210, 0.4617, 0.1457],\n",
      "       grad_fn=<AddBackward0>) tensor([9.0000, 3.5000, 2.7000, 0.0100, 0.1460, 0.5300, 0.1940]) tensor(2.9024, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3439,  2.9477,  3.0153,  0.0211,  0.1052,  0.5260,  0.1977],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 1.3000, 3.2000, 0.0270, 0.0710, 0.5540, 0.3070]) tensor(3.6096, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4151,  6.0954,  2.8612,  0.0578,  0.1796,  0.5883,  0.1666],\n",
      "       grad_fn=<AddBackward0>) tensor([19.0000,  7.5000,  3.0000,  0.0650,  0.1770,  0.5600,  0.1350]) tensor(2.1208, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0943,  4.0439,  2.2504,  0.0542,  0.1615,  0.5240,  0.1664],\n",
      "       grad_fn=<AddBackward0>) tensor([5.2000, 1.8000, 0.4000, 0.0500, 0.1020, 0.5420, 0.0540]) tensor(6.1740, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1006,  4.8075,  2.6076,  0.0464,  0.1490,  0.5645,  0.1532],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 5.1000, 1.7000, 0.0650, 0.1570, 0.5120, 0.1040]) tensor(0.9540, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5868,  3.7569,  3.6734,  0.0203,  0.1052,  0.5484,  0.2143],\n",
      "       grad_fn=<AddBackward0>) tensor([11.4000,  2.4000,  3.7000,  0.0260,  0.0730,  0.5750,  0.2250]) tensor(0.9466, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2652,  6.1525,  2.0865,  0.0707,  0.1909,  0.5850,  0.1162],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 4.1000, 2.8000, 0.0450, 0.1400, 0.5780, 0.1620]) tensor(2.1065, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6030,  5.2216,  1.7953,  0.0636,  0.1756,  0.5617,  0.1138],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 4.7000, 0.4000, 0.1350, 0.2230, 0.5780, 0.0400]) tensor(4.3362, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4890,  4.1545,  3.0599,  0.0337,  0.1268,  0.5457,  0.1845],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 1.9000, 4.7000, 0.0140, 0.0720, 0.5010, 0.2590]) tensor(4.5266, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.7846,  7.4363,  3.0661,  0.0709,  0.2065,  0.6150,  0.1653],\n",
      "       grad_fn=<AddBackward0>) tensor([12.2000,  4.4000,  2.2000,  0.0490,  0.1440,  0.5070,  0.1450]) tensor(5.8820, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.7123,  4.5010,  4.3399,  0.0243,  0.1193,  0.5605,  0.2487],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000e+01, 2.3000e+00, 5.6000e+00, 9.0000e-03, 9.6000e-02, 5.4100e-01,\n",
      "        3.5000e-01]) tensor(5.5820, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7931, 4.9741, 1.2590, 0.0759, 0.1972, 0.5410, 0.1078],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 2.9000, 0.6000, 0.1170, 0.1730, 0.5120, 0.0850]) tensor(6.5160, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.3137,  4.7381,  3.2353,  0.0367,  0.1386,  0.5664,  0.1908],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5100e+01, 3.2000e+00, 3.6000e+00, 1.4000e-02, 8.6000e-02, 5.3600e-01,\n",
      "        1.7000e-01]) tensor(0.4460, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9624, 3.5751, 0.8840, 0.0617, 0.1673, 0.5239, 0.0939],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 1.8000, 0.1000, 0.0910, 0.2730, 0.4220, 0.0430]) tensor(4.8041, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.7075, 10.7693,  1.6875,  0.1310,  0.3186,  0.6727,  0.0829],\n",
      "       grad_fn=<AddBackward0>) tensor([21.5000, 11.5000,  2.9000,  0.1140,  0.2320,  0.5780,  0.1400]) tensor(0.7482, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.5458, 4.0812, 1.6496, 0.0549, 0.1561, 0.5426, 0.1143],\n",
      "       grad_fn=<AddBackward0>) tensor([11.9000,  4.6000,  2.7000,  0.0560,  0.1250,  0.5880,  0.1430]) tensor(1.8038, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.6890,  5.5368,  3.5778,  0.0428,  0.1506,  0.5879,  0.1942],\n",
      "       grad_fn=<AddBackward0>) tensor([9.0000, 4.1000, 1.0000, 0.0290, 0.2030, 0.5040, 0.0760]) tensor(7.6394, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2364, 4.7190, 1.9345, 0.0539, 0.1538, 0.5552, 0.1130],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 3.5000, 2.1000, 0.0290, 0.1280, 0.6020, 0.1120]) tensor(0.4718, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9179,  4.6633,  1.8897,  0.0585,  0.1703,  0.5391,  0.1385],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 2.3000, 1.5000, 0.0790, 0.1320, 0.4920, 0.2000]) tensor(9.3302, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6530, 4.9538, 1.2362, 0.0717, 0.1917, 0.5477, 0.1023],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 3.2000, 0.1000, 0.0960, 0.1880, 0.5900, 0.0080]) tensor(2.7463, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.2817,  4.5016,  3.2709,  0.0407,  0.1493,  0.5503,  0.2131],\n",
      "       grad_fn=<AddBackward0>) tensor([10.3000,  3.4000,  3.1000,  0.0170,  0.1780,  0.5010,  0.2810]) tensor(3.7241, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.5276, 2.7410, 0.2414, 0.0654, 0.1607, 0.4850, 0.0642],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 1.3000, 0.2000, 0.1280, 0.0810, 0.5870, 0.0420]) tensor(0.3977, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.1520, 0.5995, 1.1379, 0.0298, 0.1024, 0.4435, 0.1395],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 4.0000, 2.0000, 0.0530, 0.1760, 0.5830, 0.1820]) tensor(3.8773, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.3903,  5.7820,  2.6688,  0.0618,  0.1829,  0.5818,  0.1594],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 4.5000, 1.7000, 0.0540, 0.2270, 0.5640, 0.1450]) tensor(4.2176, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.6814,  6.8459,  3.2868,  0.0617,  0.1894,  0.6079,  0.1801],\n",
      "       grad_fn=<AddBackward0>) tensor([12.0000,  4.4000,  1.3000,  0.0820,  0.1210,  0.5150,  0.0990]) tensor(6.0326, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7622, 5.0036, 1.6315, 0.0618, 0.1723, 0.5592, 0.1059],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 3.5000, 0.7000, 0.0780, 0.1790, 0.5100, 0.0710]) tensor(4.2545, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2485,  6.3468,  0.9773,  0.0873,  0.2174,  0.5736,  0.0698],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 4.7000, 0.6000, 0.1310, 0.1990, 0.6030, 0.0610]) tensor(4.6492, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4586, 1.2147, 2.6776, 0.0213, 0.0972, 0.4671, 0.2139],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 0.7000, 0.7000, 0.0090, 0.0990, 0.4140, 0.1670]) tensor(6.1930, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8616, 4.7903, 1.3011, 0.0668, 0.1798, 0.5413, 0.1020],\n",
      "       grad_fn=<AddBackward0>) tensor([10.3000,  7.2000,  0.9000,  0.1000,  0.1780,  0.6010,  0.0530]) tensor(1.1491, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.5354,  4.9550,  4.6750,  0.0237,  0.1233,  0.5810,  0.2604],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6500e+01, 1.7000e+00, 2.5000e+00, 1.2000e-02, 5.4000e-02, 5.5800e-01,\n",
      "        1.4800e-01]) tensor(2.7839, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3590,  4.2544,  2.7343,  0.0370,  0.1295,  0.5561,  0.1582],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 4.3000, 0.5000, 0.0990, 0.1480, 0.6600, 0.0440]) tensor(3.0712, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1617,  3.9135,  3.3511,  0.0307,  0.1285,  0.5371,  0.2178],\n",
      "       grad_fn=<AddBackward0>) tensor([16.6000,  3.6000,  4.1000,  0.0200,  0.0780,  0.5360,  0.1770]) tensor(0.9441, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4270, 5.0808, 1.3907, 0.0634, 0.1738, 0.5612, 0.0925],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 3.0000, 0.3000, 0.0810, 0.1640, 0.4910, 0.0280]) tensor(4.2578, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.7802,  6.0346,  3.5579,  0.0489,  0.1693,  0.5924,  0.2059],\n",
      "       grad_fn=<AddBackward0>) tensor([16.9000,  5.9000,  2.8000,  0.0490,  0.1840,  0.5830,  0.1600]) tensor(0.1957, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2995,  3.7485,  2.3647,  0.0404,  0.1352,  0.5321,  0.1575],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 1.2000, 1.6000, 0.0190, 0.0820, 0.5020, 0.1820]) tensor(6.3269, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.9212, 2.8748, 0.6627, 0.0634, 0.1576, 0.5009, 0.0787],\n",
      "       grad_fn=<AddBackward0>) tensor([0.7000, 1.3000, 0.2000, 0.1000, 0.2170, 0.2860, 0.0430]) tensor(1.8746, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8150,  6.9030,  1.6839,  0.0797,  0.2101,  0.6018,  0.0920],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 4.6000, 0.4000, 0.1340, 0.1850, 0.5690, 0.0400]) tensor(8.4309, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.3849, 1.3238, 0.9152, 0.0362, 0.1147, 0.4637, 0.1167],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000, 0.6000, 0.5000, 0.0230, 0.0900, 0.3060, 0.1600]) tensor(0.8493, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.2887, 3.9696, 1.2423, 0.0600, 0.1608, 0.5303, 0.0972],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 3.8000, 0.8000, 0.1120, 0.1660, 0.6400, 0.0870]) tensor(0.9190, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0720, 2.7648, 1.7348, 0.0503, 0.1409, 0.4969, 0.1413],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1000, 0.8000, 0.5000, 0.0700, 0.1340, 0.4900, 0.1950]) tensor(5.8648, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0462,  4.2326,  1.4352,  0.0623,  0.1762,  0.5359,  0.1252],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 2.9000, 0.6000, 0.1010, 0.1680, 0.5570, 0.0950]) tensor(6.1073, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8244, 1.2003, 0.2352, 0.0474, 0.1258, 0.4502, 0.0821],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 0.9000, 0.4000, 0.0490, 0.1670, 0.6660, 0.1820]) tensor(0.0259, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1740,  5.0626,  2.2160,  0.0556,  0.1679,  0.5640,  0.1414],\n",
      "       grad_fn=<AddBackward0>) tensor([11.1000,  7.1000,  1.5000,  0.1100,  0.1970,  0.5760,  0.1110]) tensor(0.8317, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1362,  3.5730,  2.4264,  0.0378,  0.1356,  0.5201,  0.1767],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 3.0000, 1.4000, 0.0290, 0.1090, 0.5120, 0.0990]) tensor(0.4534, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.5898, 2.1891, 0.9241, 0.0491, 0.1367, 0.4922, 0.1038],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 1.1000, 0.3000, 0.0590, 0.1180, 0.4800, 0.0660]) tensor(1.1835, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.1641,  4.1136,  4.0471,  0.0214,  0.1173,  0.5573,  0.2434],\n",
      "       grad_fn=<AddBackward0>) tensor([19.6000,  3.0000,  8.0000,  0.0300,  0.0740,  0.5330,  0.3880]) tensor(4.0992, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.9079,  4.7738,  3.4597,  0.0425,  0.1530,  0.5536,  0.2208],\n",
      "       grad_fn=<AddBackward0>) tensor([20.4000,  6.0000,  5.4000,  0.0230,  0.1430,  0.5880,  0.2270]) tensor(3.6356, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8056,  3.5432,  2.2548,  0.0390,  0.1362,  0.5318,  0.1622],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 2.2000, 1.0000, 0.0430, 0.1310, 0.4990, 0.1210]) tensor(4.3542, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.8055,  5.1721,  2.2677,  0.0579,  0.1790,  0.5502,  0.1650],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 2.9000, 0.7000, 0.0670, 0.1630, 0.5000, 0.0860]) tensor(9.5720, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.0942e+01, 2.4369e+00, 3.5303e+00, 9.2883e-03, 7.8629e-02, 5.0853e-01,\n",
      "        2.2013e-01], grad_fn=<AddBackward0>) tensor([6.4000, 2.1000, 5.0000, 0.0110, 0.0770, 0.4910, 0.2600]) tensor(3.2718, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.1468, 0.6951, 0.4630, 0.0480, 0.1273, 0.4351, 0.1085],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 1.5000, 0.0000, 0.0560, 0.1180, 0.5490, 0.0000]) tensor(1.7330, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.8015e+01, 3.8985e+00, 4.5302e+00, 1.2090e-02, 1.0711e-01, 5.5226e-01,\n",
      "        2.7847e-01], grad_fn=<AddBackward0>) tensor([19.4000,  3.1000,  6.2000,  0.0210,  0.0810,  0.5150,  0.3520]) tensor(0.7645, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8122,  5.3812,  1.7069,  0.0640,  0.1797,  0.5645,  0.1114],\n",
      "       grad_fn=<AddBackward0>) tensor([16.0000, 11.7000,  2.1000,  0.0960,  0.2770,  0.5900,  0.0980]) tensor(9.5724, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.4854,  3.6183,  2.6483,  0.0348,  0.1297,  0.5364,  0.1765],\n",
      "       grad_fn=<AddBackward0>) tensor([15.4000,  6.0000,  1.3000,  0.0460,  0.1350,  0.5290,  0.0670]) tensor(3.2610, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.5809, 3.0871, 0.4679, 0.0637, 0.1594, 0.5033, 0.0670],\n",
      "       grad_fn=<AddBackward0>) tensor([ 8.1000, 12.0000,  1.1000,  0.1500,  0.2390,  0.4740,  0.0540]) tensor(14.3253, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.9554,  6.7991,  3.2928,  0.0605,  0.1952,  0.6134,  0.1923],\n",
      "       grad_fn=<AddBackward0>) tensor([20.1000, 10.0000,  2.1000,  0.0800,  0.2470,  0.4890,  0.1190]) tensor(1.8575, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2691,  2.5553,  2.5914,  0.0331,  0.1217,  0.5042,  0.1922],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 1.1000, 0.7000, 0.0230, 0.1160, 0.4580, 0.1700]) tensor(8.1563, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6420,  4.7495,  3.0547,  0.0343,  0.1317,  0.5737,  0.1725],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 1.6000, 2.4000, 0.0180, 0.0650, 0.4570, 0.1690]) tensor(12.6496, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0829,  8.2561,  1.0097,  0.1186,  0.2771,  0.6159,  0.0591],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 4.8000, 0.7000, 0.0920, 0.2050, 0.6590, 0.0510]) tensor(3.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2856, 3.0766, 2.0006, 0.0412, 0.1323, 0.5150, 0.1472],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 2.8000, 0.4000, 0.0690, 0.1620, 0.4660, 0.0510]) tensor(2.4261, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2845e+00, 3.6034e-01, 2.5888e+00, 7.3184e-03, 8.1055e-02, 4.3692e-01,\n",
      "        2.3572e-01], grad_fn=<AddBackward0>) tensor([2.0000, 2.3000, 0.5000, 0.0910, 0.3530, 0.2390, 0.1430]) tensor(6.8213, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2589,  4.6660,  2.8119,  0.0463,  0.1513,  0.5637,  0.1716],\n",
      "       grad_fn=<AddBackward0>) tensor([15.3000,  6.5000,  2.9000,  0.0410,  0.1410,  0.5000,  0.1170]) tensor(1.0778, grad_fn=<MseLossBackward0>)\n",
      "tensor([21.6480,  6.6684,  4.3568,  0.0453,  0.1739,  0.6159,  0.2458],\n",
      "       grad_fn=<AddBackward0>) tensor([25.8000,  6.6000,  5.2000,  0.0410,  0.1640,  0.5650,  0.2530]) tensor(2.5653, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9216,  4.9541,  3.0642,  0.0446,  0.1453,  0.5663,  0.1712],\n",
      "       grad_fn=<AddBackward0>) tensor([10.1000,  3.1000,  1.4000,  0.0150,  0.1200,  0.5190,  0.0840]) tensor(2.0257, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4271, 1.8326, 1.9020, 0.0374, 0.1214, 0.4920, 0.1614],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 0.3000, 0.7000, 0.0000, 0.0670, 0.5810, 0.2000]) tensor(3.7361, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3825, 1.9477, 1.6118, 0.0402, 0.1194, 0.4840, 0.1366],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 0.5000, 0.4000, 0.0100, 0.0480, 0.4760, 0.0620]) tensor(2.1454, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.9338, 3.0389, 0.3637, 0.0694, 0.1760, 0.4873, 0.0875],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 3.6000, 0.4000, 0.0670, 0.2010, 0.4660, 0.0460]) tensor(0.0615, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3052,  4.0816,  2.4496,  0.0407,  0.1390,  0.5494,  0.1575],\n",
      "       grad_fn=<AddBackward0>) tensor([8.8000, 2.1000, 1.0000, 0.0170, 0.1170, 0.6050, 0.0870]) tensor(1.7590, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4318, 2.7806, 1.8778, 0.0406, 0.1388, 0.5086, 0.1624],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 2.0000, 0.3000, 0.1160, 0.1010, 0.4670, 0.0560]) tensor(4.3558, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9373, 5.2618, 1.3930, 0.0692, 0.1859, 0.5475, 0.1033],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 3.5000, 0.6000, 0.1420, 0.1680, 0.4500, 0.0820]) tensor(9.0877, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.5253e+00, 1.1317e+00, 3.4724e+00, 2.4215e-03, 5.9819e-02, 4.7578e-01,\n",
      "        2.2936e-01], grad_fn=<AddBackward0>) tensor([4.4000, 1.5000, 3.4000, 0.0100, 0.0920, 0.5100, 0.3110]) tensor(2.4525, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0531,  5.7888,  2.6507,  0.0570,  0.1776,  0.5782,  0.1639],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6900e+01, 2.6000e+00, 3.0000e+00, 1.3000e-02, 7.0000e-02, 5.7100e-01,\n",
      "        1.3500e-01]) tensor(1.9595, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8472,  2.6950,  3.2305,  0.0199,  0.1037,  0.5169,  0.2164],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4300e+01, 2.5000e+00, 2.0000e+00, 1.4000e-02, 6.8000e-02, 4.9600e-01,\n",
      "        1.0500e-01]) tensor(1.0833, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.6185,  5.3659,  3.6421,  0.0402,  0.1483,  0.5791,  0.2086],\n",
      "       grad_fn=<AddBackward0>) tensor([15.9000,  4.3000,  2.6000,  0.0190,  0.1230,  0.5340,  0.1220]) tensor(0.3927, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9127, 3.7529, 1.9282, 0.0488, 0.1496, 0.5295, 0.1439],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 1.2000, 1.6000, 0.0360, 0.0850, 0.4080, 0.2430]) tensor(7.5810, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1164,  4.4834,  3.1605,  0.0385,  0.1404,  0.5528,  0.1963],\n",
      "       grad_fn=<AddBackward0>) tensor([16.4000,  3.7000,  6.6000,  0.0300,  0.0930,  0.5790,  0.3030]) tensor(2.5247, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.8582,  4.3611,  4.1224,  0.0223,  0.1209,  0.5585,  0.2476],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6900e+01, 3.2000e+00, 6.6000e+00, 1.2000e-02, 9.8000e-02, 5.1200e-01,\n",
      "        3.3200e-01]) tensor(1.0712, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5746, 4.5465, 1.7152, 0.0571, 0.1628, 0.5555, 0.1128],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 6.3000, 0.3000, 0.1200, 0.2140, 0.6430, 0.0230]) tensor(2.0790, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4212,  4.5450,  2.5763,  0.0428,  0.1449,  0.5608,  0.1590],\n",
      "       grad_fn=<AddBackward0>) tensor([11.8000,  3.5000,  3.1000,  0.0330,  0.0950,  0.5680,  0.1550]) tensor(0.2507, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0108,  3.5880,  2.7716,  0.0401,  0.1438,  0.5307,  0.1992],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6400e+01, 4.0000e+00, 3.6000e+00, 1.5000e-02, 1.2300e-01, 4.7900e-01,\n",
      "        2.1600e-01]) tensor(1.7638, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0832,  3.9751,  2.7513,  0.0392,  0.1321,  0.5420,  0.1683],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 2.5000, 1.1000, 0.0380, 0.0910, 0.6010, 0.0870]) tensor(0.9754, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2329,  4.2663,  2.3833,  0.0419,  0.1457,  0.5532,  0.1599],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 5.8000, 0.8000, 0.1240, 0.2240, 0.5060, 0.0660]) tensor(3.3796, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.1973, 2.9204, 1.5785, 0.0421, 0.1330, 0.5043, 0.1323],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 6.4000, 1.1000, 0.1170, 0.1790, 0.6080, 0.0640]) tensor(2.5904, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15.9643,  6.2242,  3.2603,  0.0529,  0.1688,  0.5985,  0.1736],\n",
      "       grad_fn=<AddBackward0>) tensor([10.6000,  4.1000,  1.8000,  0.0400,  0.1340,  0.5230,  0.1020]) tensor(5.0618, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.6526e+01, 3.6961e+00, 4.5406e+00, 1.1393e-02, 9.7757e-02, 5.4349e-01,\n",
      "        2.7015e-01], grad_fn=<AddBackward0>) tensor([11.3000,  2.8000,  4.0000,  0.0210,  0.1190,  0.5100,  0.3010]) tensor(4.0585, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.8647, 3.1683, 0.2442, 0.0685, 0.1669, 0.4977, 0.0575],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 5.3000, 0.9000, 0.1140, 0.2510, 0.4610, 0.0880]) tensor(0.8964, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4360,  2.5137,  3.0888,  0.0279,  0.1112,  0.5044,  0.2110],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 2.5000, 3.1000, 0.0280, 0.1150, 0.5730, 0.2340]) tensor(0.1006, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5035,  4.0033,  3.1693,  0.0371,  0.1315,  0.5488,  0.1903],\n",
      "       grad_fn=<AddBackward0>) tensor([10.1000,  3.2000,  1.6000,  0.0270,  0.1270,  0.6160,  0.1120]) tensor(1.2708, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9078, 3.9781, 1.7717, 0.0532, 0.1486, 0.5379, 0.1138],\n",
      "       grad_fn=<AddBackward0>) tensor([ 8.4000, 11.5000,  0.6000,  0.1270,  0.3390,  0.7030,  0.0330]) tensor(8.3242, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.0162,  8.5378,  2.6682,  0.0845,  0.2419,  0.6369,  0.1527],\n",
      "       grad_fn=<AddBackward0>) tensor([14.1000,  8.6000,  2.5000,  0.1050,  0.2470,  0.4840,  0.1490]) tensor(5.0082, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2169,  3.8253,  3.0026,  0.0318,  0.1244,  0.5358,  0.1894],\n",
      "       grad_fn=<AddBackward0>) tensor([5.9000, 1.8000, 1.1000, 0.0430, 0.0850, 0.5300, 0.1090]) tensor(6.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.3399, 0.9766, 2.7023, 0.0119, 0.0725, 0.4574, 0.1989],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 1.2000, 2.3000, 0.0210, 0.0750, 0.4590, 0.2690]) tensor(1.4394, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7122, 4.0221, 1.4348, 0.0578, 0.1591, 0.5270, 0.1087],\n",
      "       grad_fn=<AddBackward0>) tensor([9.0000, 3.9000, 2.3000, 0.0140, 0.1460, 0.5150, 0.1180]) tensor(0.3463, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2398,  4.7630,  2.7875,  0.0426,  0.1421,  0.5611,  0.1604],\n",
      "       grad_fn=<AddBackward0>) tensor([9.9000, 6.6000, 4.5000, 0.0340, 0.2120, 0.5880, 0.2120]) tensor(1.6843, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.7355, 1.1474, 0.4959, 0.0495, 0.1356, 0.4457, 0.1125],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 2.0000, 2.0000, 0.0000, 0.1330, 0.6270, 0.1430]) tensor(4.3915, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6745,  4.4350,  2.0387,  0.0505,  0.1540,  0.5512,  0.1345],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 2.7000, 0.9000, 0.0270, 0.1430, 0.5130, 0.0910]) tensor(4.1509, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5914,  3.6499,  2.3731,  0.0420,  0.1393,  0.5274,  0.1654],\n",
      "       grad_fn=<AddBackward0>) tensor([12.7000,  4.9000,  2.7000,  0.0210,  0.1440,  0.5480,  0.1390]) tensor(0.8739, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5458,  3.5022,  3.9869,  0.0162,  0.0910,  0.5425,  0.2186],\n",
      "       grad_fn=<AddBackward0>) tensor([7.0000, 1.6000, 1.9000, 0.0100, 0.0700, 0.5790, 0.1390]) tensor(5.5339, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4525,  4.0100,  2.0350,  0.0476,  0.1499,  0.5310,  0.1483],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 5.2000, 0.5000, 0.0790, 0.2600, 0.6440, 0.0510]) tensor(3.3760, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.0333, 2.9786, 1.0578, 0.0567, 0.1423, 0.5107, 0.0824],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 2.5000, 0.6000, 0.0300, 0.2250, 0.3980, 0.0800]) tensor(0.9114, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0046,  4.2272,  1.6462,  0.0579,  0.1684,  0.5321,  0.1325],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 4.3000, 0.6000, 0.0870, 0.1910, 0.4460, 0.0630]) tensor(1.3643, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.3471, 1.6078, 1.1976, 0.0432, 0.1254, 0.4740, 0.1259],\n",
      "       grad_fn=<AddBackward0>) tensor([1.0000, 0.5000, 0.0000, 0.0770, 0.0000, 0.2500, 0.0000]) tensor(1.9925, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.4861e+01, 3.2796e+00, 3.9464e+00, 9.6046e-03, 9.4580e-02, 5.3697e-01,\n",
      "        2.4559e-01], grad_fn=<AddBackward0>) tensor([9.5000, 1.8000, 3.6000, 0.0140, 0.0750, 0.4910, 0.2730]) tensor(4.4366, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2593,  5.1375,  1.3401,  0.0716,  0.1911,  0.5482,  0.1070],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 4.3000, 1.3000, 0.0650, 0.1630, 0.6050, 0.0960]) tensor(2.0139, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2198, 2.8161, 2.8589, 0.0280, 0.1056, 0.5153, 0.1789],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 1.5000, 2.1000, 0.0100, 0.1240, 0.4040, 0.2270]) tensor(6.5922, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.2471, 0.9688, 1.3802, 0.0288, 0.1075, 0.4579, 0.1539],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 0.4000, 0.4000, 0.0000, 0.0570, 0.4600, 0.1180]) tensor(1.6904, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.5137, 3.4343, 0.7986, 0.0578, 0.1606, 0.5204, 0.0902],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 2.7000, 0.4000, 0.1480, 0.1880, 0.4570, 0.0700]) tensor(2.4031, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5956, 4.6895, 0.7314, 0.0738, 0.1894, 0.5347, 0.0782],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 4.2000, 0.8000, 0.1130, 0.1400, 0.5320, 0.0650]) tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4625,  6.3531,  1.8685,  0.0739,  0.1997,  0.5817,  0.1116],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 4.7000, 0.6000, 0.1220, 0.2230, 0.5630, 0.0700]) tensor(6.0462, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0513, 5.3439, 0.8344, 0.0779, 0.1961, 0.5636, 0.0632],\n",
      "       grad_fn=<AddBackward0>) tensor([15.1000, 13.5000,  1.5000,  0.1040,  0.2760,  0.6990,  0.0650]) tensor(16.6678, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4951,  5.5447,  3.0916,  0.0495,  0.1690,  0.5772,  0.1933],\n",
      "       grad_fn=<AddBackward0>) tensor([17.8000,  4.8000,  5.6000,  0.0240,  0.1260,  0.4820,  0.2550]) tensor(1.2235, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.1377,  2.4117, -0.1944,  0.0674,  0.1556,  0.4797,  0.0355],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 1.1000, 0.1000, 0.0930, 0.1510, 0.6660, 0.0200]) tensor(0.3463, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6726, 3.3652, 2.0874, 0.0421, 0.1321, 0.5116, 0.1494],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 2.9000, 1.3000, 0.0550, 0.1120, 0.5190, 0.1030]) tensor(1.1403, grad_fn=<MseLossBackward0>)\n",
      "tensor([25.0842,  8.2270,  4.4110,  0.0603,  0.2107,  0.6472,  0.2462],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1400e+01, 7.0000e+00, 6.6000e+00, 2.6000e-02, 1.7100e-01, 5.6800e-01,\n",
      "        3.1500e-01]) tensor(6.6000, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.7490,  7.9570,  2.0816,  0.0934,  0.2470,  0.6130,  0.1271],\n",
      "       grad_fn=<AddBackward0>) tensor([16.6000,  8.4000,  1.7000,  0.0540,  0.2600,  0.5000,  0.1100]) tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.3481,  5.7269,  1.9698,  0.0661,  0.1812,  0.5683,  0.1164],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 4.6000, 0.3000, 0.1390, 0.2490, 0.5140, 0.0350]) tensor(9.8357, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.0940,  4.4635,  4.3344,  0.0180,  0.1115,  0.5668,  0.2483],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7200e+01, 3.4000e+00, 7.4000e+00, 1.4000e-02, 8.5000e-02, 5.4900e-01,\n",
      "        3.3800e-01]) tensor(1.5070, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4316,  3.4096,  3.5837,  0.0225,  0.1115,  0.5320,  0.2257],\n",
      "       grad_fn=<AddBackward0>) tensor([11.9000,  2.3000,  1.9000,  0.0230,  0.0950,  0.5120,  0.1380]) tensor(0.9172, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.9418,  4.3175,  2.3770,  0.0466,  0.1521,  0.5457,  0.1618],\n",
      "       grad_fn=<AddBackward0>) tensor([19.4000,  6.0000,  3.5000,  0.0280,  0.1530,  0.5470,  0.1590]) tensor(8.5309, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3472, 4.1806, 1.7517, 0.0548, 0.1591, 0.5388, 0.1252],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 2.4000, 0.9000, 0.0100, 0.1150, 0.5210, 0.0640]) tensor(0.8997, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.0580,  6.1897,  1.6995,  0.0777,  0.2161,  0.5744,  0.1299],\n",
      "       grad_fn=<AddBackward0>) tensor([16.8000,  5.3000,  1.5000,  0.0750,  0.1190,  0.5050,  0.0870]) tensor(1.1952, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1832,  5.3893,  2.7365,  0.0530,  0.1621,  0.5737,  0.1557],\n",
      "       grad_fn=<AddBackward0>) tensor([12.4000,  2.8000,  1.7000,  0.0200,  0.0840,  0.5420,  0.0820]) tensor(1.2008, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.2537e+01, 5.1162e+00, 5.1002e+00, 1.8636e-02, 1.3151e-01, 5.8290e-01,\n",
      "        3.0799e-01], grad_fn=<AddBackward0>) tensor([2.9600e+01, 4.3000e+00, 9.3000e+00, 1.4000e-02, 1.0000e-01, 5.9500e-01,\n",
      "        4.2300e-01]) tensor(9.7427, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.3997,  5.3131,  3.9254,  0.0376,  0.1466,  0.5746,  0.2285],\n",
      "       grad_fn=<AddBackward0>) tensor([17.7000,  3.8000,  5.0000,  0.0230,  0.1000,  0.5820,  0.2460]) tensor(0.5053, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6918,  4.9019,  2.4182,  0.0495,  0.1549,  0.5582,  0.1469],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 3.0000, 1.0000, 0.0800, 0.1450, 0.4850, 0.1220]) tensor(7.2022, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.9998,  6.3301,  2.4962,  0.0695,  0.2045,  0.5763,  0.1682],\n",
      "       grad_fn=<AddBackward0>) tensor([10.3000,  5.1000,  1.0000,  0.1210,  0.2100,  0.5470,  0.1080]) tensor(5.1781, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.4959, 2.2212, 1.4973, 0.0404, 0.1275, 0.4842, 0.1409],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 1.8000, 2.5000, 0.0350, 0.0880, 0.4700, 0.2240]) tensor(0.8591, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3677, 3.5301, 0.4222, 0.0708, 0.1789, 0.4986, 0.0806],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 1.9000, 0.1000, 0.1270, 0.1570, 0.5290, 0.0270]) tensor(2.0157, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7824, 4.7246, 1.8757, 0.0549, 0.1577, 0.5593, 0.1143],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 5.7000, 0.7000, 0.1120, 0.1910, 0.5020, 0.0540]) tensor(1.3629, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2519,  3.3733,  2.7297,  0.0322,  0.1192,  0.5280,  0.1740],\n",
      "       grad_fn=<AddBackward0>) tensor([13.2000,  2.3000,  1.0000,  0.0170,  0.0810,  0.5880,  0.0570]) tensor(1.8363, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.0507, 2.8543, 0.1535, 0.0687, 0.1746, 0.4817, 0.0786],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 2.5000, 0.0000, 0.1320, 0.1920, 0.5820, 0.0000]) tensor(0.0332, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.0910,  6.1730,  2.2007,  0.0715,  0.2003,  0.5826,  0.1388],\n",
      "       grad_fn=<AddBackward0>) tensor([19.6000, 10.9000,  3.6000,  0.0680,  0.2700,  0.5490,  0.1820]) tensor(7.8086, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.2033,  7.0274,  3.3733,  0.0614,  0.1910,  0.6137,  0.1825],\n",
      "       grad_fn=<AddBackward0>) tensor([12.2000,  5.1000,  2.0000,  0.0560,  0.1640,  0.5240,  0.1290]) tensor(5.9503, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.4345,  7.4476,  0.7696,  0.1008,  0.2458,  0.6032,  0.0500],\n",
      "       grad_fn=<AddBackward0>) tensor([10.1000,  7.6000,  1.0000,  0.0970,  0.2350,  0.6130,  0.0570]) tensor(0.2654, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.4589,  5.9082,  2.9203,  0.0562,  0.1703,  0.5866,  0.1599],\n",
      "       grad_fn=<AddBackward0>) tensor([10.7000,  5.4000,  2.5000,  0.0460,  0.1510,  0.5170,  0.1270]) tensor(2.0816, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.6856, -0.6562,  0.5716,  0.0181,  0.0799,  0.3886,  0.1321],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 0.2000, 0.0000, 0.0000, 0.0770, 0.0000, 0.0000]) tensor(0.2427, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3656, 2.6198, 2.0617, 0.0428, 0.1369, 0.5006, 0.1712],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 1.9000, 0.7000, 0.1170, 0.1010, 0.4700, 0.1260]) tensor(5.9485, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.3155,  5.6731,  3.4635,  0.0486,  0.1676,  0.5738,  0.2125],\n",
      "       grad_fn=<AddBackward0>) tensor([13.0000,  5.0000,  2.1000,  0.1060,  0.1100,  0.5440,  0.1300]) tensor(2.9929, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.9931,  1.9780, -0.0564,  0.0573,  0.1433,  0.4808,  0.0511],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 1.4000, 0.1000, 0.0710, 0.2070, 0.4720, 0.0190]) tensor(0.0532, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0877,  3.8222,  2.4252,  0.0443,  0.1435,  0.5306,  0.1676],\n",
      "       grad_fn=<AddBackward0>) tensor([14.3000,  4.5000,  6.6000,  0.0280,  0.1370,  0.5580,  0.3240]) tensor(4.0333, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.4702, 3.2365, 1.5416, 0.0524, 0.1388, 0.5140, 0.1056],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 2.3000, 0.5000, 0.0670, 0.1900, 0.5470, 0.0850]) tensor(1.5412, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5008,  3.6457,  3.0301,  0.0306,  0.1186,  0.5285,  0.1893],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 2.5000, 3.1000, 0.0210, 0.1020, 0.4900, 0.2080]) tensor(2.3623, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1073,  2.1308,  3.4846,  0.0158,  0.0858,  0.5003,  0.2211],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 1.6000, 3.1000, 0.0120, 0.0800, 0.4810, 0.2360]) tensor(2.3556, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5510,  3.2282,  3.1048,  0.0262,  0.1120,  0.5269,  0.1981],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.0000, 0.9000, 0.0380, 0.0700, 0.4340, 0.1230]) tensor(11.1316, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6292,  5.5592,  2.2472,  0.0567,  0.1690,  0.5761,  0.1319],\n",
      "       grad_fn=<AddBackward0>) tensor([7.5000, 3.3000, 2.3000, 0.0420, 0.1060, 0.4780, 0.1510]) tensor(4.4900, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.8340,  3.9563,  3.3418,  0.0339,  0.1319,  0.5431,  0.2113],\n",
      "       grad_fn=<AddBackward0>) tensor([18.5000,  4.8000,  3.7000,  0.0220,  0.1290,  0.5510,  0.1840]) tensor(3.2303, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3800, 1.8593, 2.1308, 0.0257, 0.1027, 0.4861, 0.1688],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 1.5000, 1.9000, 0.0270, 0.0560, 0.5040, 0.1560]) tensor(0.2985, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9532, 2.7183, 1.2679, 0.0496, 0.1453, 0.5000, 0.1259],\n",
      "       grad_fn=<AddBackward0>) tensor([6.3000, 3.0000, 0.8000, 0.0710, 0.1370, 0.5440, 0.0710]) tensor(0.1043, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.7178, 3.4368, 0.2379, 0.0685, 0.1696, 0.5094, 0.0546],\n",
      "       grad_fn=<AddBackward0>) tensor([2.5000, 1.0000, 0.5000, 0.0030, 0.1020, 0.4970, 0.0720]) tensor(1.0713, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.3085,  5.5540,  3.2500,  0.0443,  0.1598,  0.5799,  0.1950],\n",
      "       grad_fn=<AddBackward0>) tensor([12.7000,  4.5000,  1.2000,  0.0520,  0.1410,  0.5040,  0.0840]) tensor(2.6219, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6312,  4.7889,  2.6262,  0.0517,  0.1657,  0.5418,  0.1819],\n",
      "       grad_fn=<AddBackward0>) tensor([10.4000,  3.4000,  2.9000,  0.0240,  0.1130,  0.4880,  0.1730]) tensor(1.7787, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.1062,  4.8750,  3.4272,  0.0388,  0.1541,  0.5629,  0.2245],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4900e+01, 4.5000e+00, 4.7000e+00, 1.3000e-02, 1.1200e-01, 5.6100e-01,\n",
      "        2.3300e-01]) tensor(8.9296, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.9464,  7.2567,  2.3577,  0.0783,  0.2169,  0.6108,  0.1336],\n",
      "       grad_fn=<AddBackward0>) tensor([14.9000, 11.3000,  1.9000,  0.1100,  0.2790,  0.6310,  0.1090]) tensor(2.5227, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.8523,  2.1920, -0.2818,  0.0692,  0.1602,  0.4874,  0.0378],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 7.0000, 0.0000, 0.1110, 0.2630, 0.7730, 0.0000]) tensor(7.1129, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.2817e+01,  4.2684e-01,  3.8246e+00, -6.5037e-03,  7.0626e-02,\n",
      "         4.3667e-01,  3.1337e-01], grad_fn=<AddBackward0>) tensor([0., 0., 0., 0., 0., 0., 0.]) tensor(25.6258, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8829, 4.0241, 0.9156, 0.0684, 0.1763, 0.5156, 0.0932],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 5.0000, 1.0000, 0.1270, 0.2350, 0.3440, 0.0920]) tensor(3.2751, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.8794, 3.4754, 1.5749, 0.0476, 0.1428, 0.5282, 0.1187],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 4.1000, 0.8000, 0.0460, 0.1840, 0.5180, 0.0630]) tensor(0.6468, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9318, 4.1500, 1.1000, 0.0609, 0.1623, 0.5389, 0.0837],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 2.8000, 1.0000, 0.0490, 0.1560, 0.5430, 0.0930]) tensor(1.3279, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7117, 2.0445, 2.2764, 0.0283, 0.1058, 0.4921, 0.1710],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.5000, 1.5000, 0.0180, 0.1060, 0.4970, 0.2110]) tensor(2.9092, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.7718, 2.2323, 1.0426, 0.0437, 0.1275, 0.4943, 0.1050],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 0.8000, 0.5000, 0.0160, 0.0850, 0.5460, 0.0900]) tensor(1.2087, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.6936,  2.9715,  3.8282,  0.0132,  0.0904,  0.5234,  0.2326],\n",
      "       grad_fn=<AddBackward0>) tensor([10.9000,  3.2000,  4.9000,  0.0150,  0.1030,  0.5090,  0.2660]) tensor(0.6314, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.6716,  3.8857,  4.1338,  0.0272,  0.1321,  0.5456,  0.2700],\n",
      "       grad_fn=<AddBackward0>) tensor([27.4000,  5.2000,  6.4000,  0.0320,  0.1200,  0.5950,  0.3310]) tensor(14.5016, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.0687e+01, 1.2636e+00, 3.8381e+00, 2.9692e-03, 6.7058e-02, 4.7951e-01,\n",
      "        2.5722e-01], grad_fn=<AddBackward0>) tensor([3.0000, 0.4000, 1.4000, 0.0090, 0.0550, 0.4550, 0.3470]) tensor(9.3987, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.1616, 2.1623, 1.1030, 0.0484, 0.1376, 0.4824, 0.1201],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 1.1000, 0.4000, 0.0610, 0.1640, 0.6480, 0.1210]) tensor(1.7556, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7721, 3.6028, 1.8014, 0.0519, 0.1467, 0.5147, 0.1306],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 1.6000, 0.7000, 0.0290, 0.1110, 0.5680, 0.0830]) tensor(3.4781, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.7802, 0.6950, 1.5376, 0.0257, 0.0991, 0.4478, 0.1619],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 1.0000, 0.5000, 0.0000, 0.4000, 0.0000, 0.2000]) tensor(3.4733, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9113, 2.5903, 1.9886, 0.0353, 0.1211, 0.5021, 0.1540],\n",
      "       grad_fn=<AddBackward0>) tensor([2.2000, 0.6000, 0.9000, 0.0260, 0.0750, 0.4560, 0.2260]) tensor(5.3964, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.7013,  6.0997,  4.2509,  0.0387,  0.1605,  0.6061,  0.2448],\n",
      "       grad_fn=<AddBackward0>) tensor([25.2000,  5.5000,  5.5000,  0.0410,  0.1180,  0.5440,  0.2540]) tensor(3.1663, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5214, 4.6849, 1.5941, 0.0611, 0.1706, 0.5474, 0.1127],\n",
      "       grad_fn=<AddBackward0>) tensor([10.0000,  7.9000,  1.5000,  0.0650,  0.2490,  0.5990,  0.0900]) tensor(1.5120, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.5746, 5.0313, 0.6935, 0.0813, 0.2048, 0.5496, 0.0750],\n",
      "       grad_fn=<AddBackward0>) tensor([11.7000,  6.7000,  1.1000,  0.0980,  0.1630,  0.5670,  0.0670]) tensor(1.8172, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.2655,  4.4613,  1.8350,  0.0515,  0.1560,  0.5560,  0.1223],\n",
      "       grad_fn=<AddBackward0>) tensor([7.5000, 4.3000, 0.6000, 0.0870, 0.1610, 0.6750, 0.0570]) tensor(1.3170, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6494, 2.7880, 2.3718, 0.0258, 0.1051, 0.5222, 0.1567],\n",
      "       grad_fn=<AddBackward0>) tensor([10.8000,  3.6000,  1.3000,  0.0360,  0.1220,  0.6040,  0.0850]) tensor(0.9208, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4955, 4.9474, 1.6731, 0.0604, 0.1625, 0.5550, 0.0961],\n",
      "       grad_fn=<AddBackward0>) tensor([3.5000, 4.5000, 0.3000, 0.1490, 0.1690, 0.5390, 0.0290]) tensor(3.8648, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4495,  3.9920,  1.4963,  0.0586,  0.1735,  0.5245,  0.1401],\n",
      "       grad_fn=<AddBackward0>) tensor([7.5000, 4.2000, 0.8000, 0.0720, 0.2100, 0.5470, 0.0780]) tensor(1.3191, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3593, 2.8099, 0.8207, 0.0552, 0.1520, 0.4878, 0.1046],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 2.9000, 0.7000, 0.0300, 0.1310, 0.5390, 0.0640]) tensor(0.1899, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0173,  6.1089,  2.4190,  0.0614,  0.1768,  0.5910,  0.1280],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 5.0000, 1.4000, 0.0480, 0.1890, 0.5460, 0.0870]) tensor(4.2131, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.7770,  3.6897,  3.6051,  0.0291,  0.1274,  0.5323,  0.2382],\n",
      "       grad_fn=<AddBackward0>) tensor([19.5000,  4.8000,  4.5000,  0.0330,  0.1450,  0.6120,  0.2570]) tensor(3.4782, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.1193,  1.8586, -0.0795,  0.0671,  0.1634,  0.4652,  0.0737],\n",
      "       grad_fn=<AddBackward0>) tensor([1.6000, 0.4000, 0.0000, 0.0710, 0.0770, 0.8000, 0.0000]) tensor(0.3612, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3809,  4.2777,  2.7163,  0.0399,  0.1398,  0.5529,  0.1704],\n",
      "       grad_fn=<AddBackward0>) tensor([7.1000, 1.4000, 1.3000, 0.0150, 0.0840, 0.5430, 0.1190]) tensor(5.4546, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9313,  4.3232,  2.8987,  0.0403,  0.1409,  0.5415,  0.1863],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 2.2000, 0.9000, 0.0250, 0.0850, 0.5400, 0.0660]) tensor(4.8335, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2385,  6.4712,  1.2442,  0.0820,  0.2121,  0.5882,  0.0785],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 5.2000, 0.5000, 0.1340, 0.2190, 0.5140, 0.0470]) tensor(3.6557, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2573,  2.8431,  3.6171,  0.0145,  0.0928,  0.5128,  0.2301],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 1.6000, 3.8000, 0.0140, 0.0840, 0.5540, 0.3190]) tensor(3.0652, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9129,  4.0318,  1.9605,  0.0537,  0.1633,  0.5339,  0.1517],\n",
      "       grad_fn=<AddBackward0>) tensor([10.9000,  3.8000,  1.0000,  0.0600,  0.1520,  0.4930,  0.0910]) tensor(0.1403, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3073, 2.3660, 2.2886, 0.0316, 0.1188, 0.5083, 0.1755],\n",
      "       grad_fn=<AddBackward0>) tensor([3.8000, 1.4000, 1.1000, 0.0400, 0.1360, 0.5400, 0.2260]) tensor(4.6686, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.5038, 3.5606, 0.4977, 0.0656, 0.1706, 0.5184, 0.0728],\n",
      "       grad_fn=<AddBackward0>) tensor([4.4000, 2.9000, 0.6000, 0.0710, 0.1900, 0.5050, 0.0790]) tensor(0.2380, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0598, 3.6236, 1.0866, 0.0600, 0.1636, 0.5121, 0.1066],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 4.3000, 1.0000, 0.0490, 0.1520, 0.5970, 0.0740]) tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9257, 2.4324, 2.3153, 0.0337, 0.1254, 0.4919, 0.1901],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000e+00, 1.2000e+00, 1.0000e+00, 3.0000e-03, 1.1400e-01, 4.4600e-01,\n",
      "        1.6400e-01]) tensor(6.0018, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.3596, 2.2271, 1.3588, 0.0535, 0.1458, 0.4914, 0.1339],\n",
      "       grad_fn=<AddBackward0>) tensor([6.7000, 4.3000, 3.0000, 0.0000, 0.1510, 0.5360, 0.1730]) tensor(1.0161, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5087,  4.0622,  2.8757,  0.0401,  0.1339,  0.5464,  0.1724],\n",
      "       grad_fn=<AddBackward0>) tensor([6.8000, 1.9000, 1.3000, 0.0240, 0.0810, 0.5280, 0.1140]) tensor(4.1909, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4855, 3.5180, 2.1980, 0.0440, 0.1375, 0.5178, 0.1555],\n",
      "       grad_fn=<AddBackward0>) tensor([6.9000, 4.2000, 2.0000, 0.0460, 0.1350, 0.5730, 0.1280]) tensor(1.0276, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1620, 4.1159, 1.2178, 0.0679, 0.1788, 0.5272, 0.1079],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 3.8000, 0.3000, 0.1280, 0.2920, 0.4980, 0.0380]) tensor(3.3773, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.2719, 3.5267, 1.7768, 0.0493, 0.1397, 0.5277, 0.1180],\n",
      "       grad_fn=<AddBackward0>) tensor([3.1000, 2.3000, 0.5000, 0.0520, 0.1130, 0.4040, 0.0480]) tensor(2.9372, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5469,  4.7997,  1.9222,  0.0578,  0.1718,  0.5481,  0.1390],\n",
      "       grad_fn=<AddBackward0>) tensor([8.9000, 3.1000, 1.5000, 0.0360, 0.1280, 0.4910, 0.1270]) tensor(1.4399, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8617, 3.7522, 2.5372, 0.0373, 0.1209, 0.5331, 0.1470],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 1.6000, 1.3000, 0.0120, 0.0880, 0.4740, 0.1130]) tensor(5.7897, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.1490,  2.5339, -0.5763,  0.0665,  0.1736,  0.4567,  0.0566],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2580, 0.0000]) tensor(0.9789, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9930, 3.3072, 1.3136, 0.0485, 0.1450, 0.5055, 0.1185],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 2.8000, 0.7000, 0.0740, 0.1150, 0.6000, 0.0660]) tensor(0.2064, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6365, 3.1965, 0.6511, 0.0667, 0.1733, 0.4890, 0.0994],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 1.3000, 0.3000, 0.0420, 0.1700, 0.5850, 0.0780]) tensor(2.6355, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1736,  4.6772,  3.3524,  0.0413,  0.1437,  0.5629,  0.1968],\n",
      "       grad_fn=<AddBackward0>) tensor([18.0000,  6.0000,  5.7000,  0.0620,  0.1220,  0.5480,  0.2610]) tensor(3.1297, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4972,  5.5131,  3.4142,  0.0449,  0.1537,  0.5870,  0.1881],\n",
      "       grad_fn=<AddBackward0>) tensor([12.3000,  7.2000,  1.7000,  0.0440,  0.2270,  0.5130,  0.0890]) tensor(2.2895, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.6987,  5.8299,  4.1659,  0.0413,  0.1610,  0.5945,  0.2428],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1600e+01, 5.1000e+00, 5.9000e+00, 1.9000e-02, 1.3400e-01, 4.8700e-01,\n",
      "        2.9400e-01]) tensor(1.0243, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.6343,  4.7047,  4.3369,  0.0237,  0.1226,  0.5685,  0.2505],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3100e+01, 1.8000e+00, 4.4000e+00, 9.0000e-03, 6.7000e-02, 5.6900e-01,\n",
      "        2.9700e-01]) tensor(4.1439, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.1427,  5.9861,  3.1033,  0.0531,  0.1723,  0.5904,  0.1789],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([17.6000,  6.2000,  2.5000,  0.0480,  0.1580,  0.5350,  0.1200]) tensor(0.3629, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4846,  5.9760,  1.4000,  0.0759,  0.1978,  0.5713,  0.0890],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 3.4000, 0.4000, 0.0560, 0.2000, 0.4980, 0.0400]) tensor(7.0992, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.3910,  3.0825,  3.0727,  0.0232,  0.1016,  0.5275,  0.1865],\n",
      "       grad_fn=<AddBackward0>) tensor([12.4000,  2.6000,  5.3000,  0.0190,  0.0720,  0.6100,  0.2570]) tensor(1.3204, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.9676, 1.8899, 2.4440, 0.0299, 0.1184, 0.4925, 0.2010],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2000, 0.2000, 0.2000, 0.0670, 0.0000, 0.5100, 0.1110]) tensor(12.1123, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0834, 5.3029, 1.1731, 0.0735, 0.1899, 0.5640, 0.0811],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 6.7000, 0.9000, 0.1280, 0.1230, 0.5740, 0.0500]) tensor(0.3289, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5586,  6.1452,  1.5566,  0.0783,  0.2056,  0.5696,  0.1041],\n",
      "       grad_fn=<AddBackward0>) tensor([4.9000, 3.0000, 0.2000, 0.0750, 0.1470, 0.5330, 0.0240]) tensor(8.0115, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2201, 2.8252, 1.8503, 0.0376, 0.1266, 0.5091, 0.1460],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 0.9000, 1.3000, 0.0090, 0.1260, 0.4120, 0.2750]) tensor(5.9274, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3816, 1.5680, 1.1291, 0.0456, 0.1335, 0.4647, 0.1391],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 0.8000, 0.6000, 0.0910, 0.1180, 0.7530, 0.3000]) tensor(1.7737, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.0916,  7.2880,  2.8637,  0.0775,  0.2258,  0.6108,  0.1791],\n",
      "       grad_fn=<AddBackward0>) tensor([25.2000, 12.9000,  5.4000,  0.0650,  0.2920,  0.5830,  0.2410]) tensor(10.7499, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.1011, 2.2638, 0.8366, 0.0503, 0.1444, 0.4757, 0.1156],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 1.3000, 0.3000, 0.0740, 0.1670, 0.4850, 0.0740]) tensor(1.5481, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.2074,  1.7113, -0.8578,  0.0668,  0.1635,  0.4397,  0.0440],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 2.0000, 0.6000, 0.0490, 0.1700, 0.7990, 0.0860]) tensor(1.6263, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.5168, 0.8793, 1.3677, 0.0299, 0.1044, 0.4579, 0.1470],\n",
      "       grad_fn=<AddBackward0>) tensor([0., 0., 0., 0., 0., 0., 0.]) tensor(3.3269, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.8141,  6.1217,  3.1981,  0.0498,  0.1691,  0.5928,  0.1834],\n",
      "       grad_fn=<AddBackward0>) tensor([14.4000,  4.3000,  3.0000,  0.0490,  0.0890,  0.4870,  0.1490]) tensor(1.3150, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.8811,  4.6280,  2.9973,  0.0465,  0.1592,  0.5458,  0.2039],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 2.1000, 1.2000, 0.0270, 0.0910, 0.4840, 0.0960]) tensor(6.1517, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.1480,  4.0650,  2.5839,  0.0429,  0.1400,  0.5472,  0.1615],\n",
      "       grad_fn=<AddBackward0>) tensor([13.4000,  4.1000,  1.0000,  0.0250,  0.1330,  0.6140,  0.0560]) tensor(1.0853, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2366,  3.2582,  3.6213,  0.0148,  0.0971,  0.5335,  0.2229],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 1.1000, 0.7000, 0.0220, 0.0900, 0.4950, 0.1050]) tensor(12.0548, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.3092,  4.4613,  3.7403,  0.0316,  0.1302,  0.5441,  0.2283],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 2.2000, 3.4000, 0.0140, 0.0900, 0.5030, 0.2520]) tensor(8.3797, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7662,  4.3129,  3.5298,  0.0280,  0.1198,  0.5549,  0.2031],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 2.0000, 3.5000, 0.0140, 0.0790, 0.5620, 0.2240]) tensor(3.1267, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0877, 4.1986, 1.5630, 0.0522, 0.1508, 0.5440, 0.1043],\n",
      "       grad_fn=<AddBackward0>) tensor([7.6000, 5.2000, 0.9000, 0.0670, 0.1790, 0.5560, 0.0570]) tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6527,  5.0350,  2.4849,  0.0566,  0.1740,  0.5509,  0.1717],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 2.9000, 1.1000, 0.0470, 0.1500, 0.5430, 0.1150]) tensor(5.8191, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.3262, 1.0740, 0.6469, 0.0400, 0.1047, 0.4683, 0.0779],\n",
      "       grad_fn=<AddBackward0>) tensor([0.8000, 1.3000, 1.1000, 0.0050, 0.1530, 0.6840, 0.1870]) tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9308,  3.0925,  2.5928,  0.0322,  0.1247,  0.5094,  0.1901],\n",
      "       grad_fn=<AddBackward0>) tensor([9.2000, 2.6000, 1.8000, 0.0310, 0.1000, 0.4920, 0.1200]) tensor(0.5532, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.4246,  4.0715,  2.5406,  0.0397,  0.1321,  0.5470,  0.1525],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 5.1000, 0.9000, 0.0850, 0.1490, 0.6370, 0.0590]) tensor(0.5941, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2153,  6.0318,  2.2290,  0.0608,  0.1782,  0.5946,  0.1228],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 9.7000, 0.8000, 0.1330, 0.2590, 0.5650, 0.0490]) tensor(6.1021, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.0978, 2.5781, 0.3600, 0.0652, 0.1658, 0.4783, 0.0890],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 3.1000, 0.0000, 0.0960, 0.1950, 0.5150, 0.0000]) tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5186,  2.7275,  3.5793,  0.0195,  0.0960,  0.5173,  0.2211],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 2.4000, 7.4000, 0.0160, 0.0610, 0.4870, 0.3620]) tensor(2.5763, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9905,  5.8423,  2.6830,  0.0627,  0.1868,  0.5834,  0.1642],\n",
      "       grad_fn=<AddBackward0>) tensor([14.2000,  5.5000,  2.9000,  0.0490,  0.1730,  0.5300,  0.1660]) tensor(0.1132, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6965, 2.1819, 2.3194, 0.0216, 0.1017, 0.5000, 0.1751],\n",
      "       grad_fn=<AddBackward0>) tensor([6.9000, 3.5000, 1.7000, 0.0300, 0.1620, 0.5070, 0.1550]) tensor(0.7647, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.3963,  5.0257,  3.7976,  0.0416,  0.1554,  0.5628,  0.2372],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0200e+01, 4.1000e+00, 2.1000e+00, 2.0000e-02, 1.0000e-01, 6.0300e-01,\n",
      "        9.9000e-02]) tensor(1.6605, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.8003,  6.3263,  2.1105,  0.0691,  0.1949,  0.5930,  0.1239],\n",
      "       grad_fn=<AddBackward0>) tensor([11.5000,  9.3000,  2.4000,  0.0770,  0.2500,  0.5650,  0.1190]) tensor(2.0317, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8212, 3.4146, 1.5858, 0.0522, 0.1549, 0.5111, 0.1394],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 2.1000, 1.1000, 0.0100, 0.1050, 0.5700, 0.0920]) tensor(0.6126, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0842,  2.4385,  2.8744,  0.0231,  0.1095,  0.4999,  0.2115],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 2.1000, 1.5000, 0.0230, 0.1320, 0.5070, 0.2080]) tensor(3.8351, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.4070, 3.7236, 0.8955, 0.0628, 0.1661, 0.5126, 0.0928],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 3.4000, 1.0000, 0.0980, 0.1750, 0.5160, 0.1060]) tensor(1.8754, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.2566, 4.6912, 1.0684, 0.0687, 0.1756, 0.5511, 0.0745],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 2.8000, 0.8000, 0.0590, 0.1610, 0.5490, 0.0960]) tensor(1.8560, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3535, 0.8799, 3.0451, 0.0108, 0.0848, 0.4634, 0.2394],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 1.0000, 0.3000, 0.0000, 0.1110, 0.4290, 0.1000]) tensor(5.1759, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.6126e+01, 3.5493e+00, 4.5082e+00, 7.0035e-03, 8.9864e-02, 5.4562e-01,\n",
      "        2.6537e-01], grad_fn=<AddBackward0>) tensor([12.0000,  2.1000,  4.1000,  0.0130,  0.0670,  0.4810,  0.2340]) tensor(2.7563, grad_fn=<MseLossBackward0>)\n",
      "tensor([-11.7689,  -1.6768,  -2.5215,   0.0502,   0.1067,   0.3386,  -0.0212],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.2500, 0.0000, 0.0000]) tensor(21.7382, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.3480,  3.7755,  3.3603,  0.0272,  0.1183,  0.5311,  0.2130],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 2.1000, 2.8000, 0.0390, 0.0980, 0.5050, 0.2770]) tensor(9.2453, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.3553,  7.7691,  2.1031,  0.0856,  0.2327,  0.6153,  0.1224],\n",
      "       grad_fn=<AddBackward0>) tensor([16.3000,  7.9000,  2.1000,  0.0900,  0.1740,  0.5590,  0.1080]) tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.3527,  6.3495,  2.2482,  0.0699,  0.1929,  0.5872,  0.1267],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 8.0000, 1.9000, 0.0900, 0.2120, 0.5080, 0.1030]) tensor(2.7539, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.9342,  5.9083,  3.8681,  0.0416,  0.1560,  0.5914,  0.2167],\n",
      "       grad_fn=<AddBackward0>) tensor([16.7000,  5.0000,  2.9000,  0.0400,  0.1160,  0.4960,  0.1420]) tensor(0.4717, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9727,  5.3183,  1.8731,  0.0599,  0.1720,  0.5718,  0.1137],\n",
      "       grad_fn=<AddBackward0>) tensor([9.1000, 7.7000, 2.8000, 0.1060, 0.2230, 0.5640, 0.1610]) tensor(1.4351, grad_fn=<MseLossBackward0>)\n",
      "tensor([-1.8952, -1.1048,  0.1193,  0.0260,  0.0888,  0.3841,  0.1171],\n",
      "       grad_fn=<AddBackward0>) tensor([0., 0., 0., 0., 0., 0., 0.]) tensor(0.7138, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3625, 1.5024, 2.3001, 0.0279, 0.1096, 0.4772, 0.1933],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 0.3000, 3.3000, 0.0000, 0.0260, 0.1500, 0.4480]) tensor(7.1028, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.7961,  3.6402,  3.2169,  0.0281,  0.1245,  0.5334,  0.2149],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 1.9000, 0.7000, 0.0080, 0.1140, 0.4870, 0.0790]) tensor(7.3692, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4025, 2.5218, 1.9263, 0.0450, 0.1409, 0.5160, 0.1619],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 0.8000, 0.0000, 0.1820, 0.1670, 0.6000, 0.0000]) tensor(9.8824, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.9482, 2.4493, 0.4204, 0.0550, 0.1496, 0.4928, 0.0818],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 1.8000, 0.1000, 0.0830, 0.1870, 0.4240, 0.0180]) tensor(0.6757, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.0556e+01, 1.9026e+00, 3.6990e+00, 3.1311e-03, 6.7573e-02, 5.0389e-01,\n",
      "        2.3113e-01], grad_fn=<AddBackward0>) tensor([5.0000, 1.1000, 1.9000, 0.0220, 0.0640, 0.5070, 0.2160]) tensor(4.9636, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6999, 4.6525, 1.9494, 0.0557, 0.1576, 0.5477, 0.1219],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 2.9000, 1.0000, 0.0910, 0.1010, 0.5710, 0.0940]) tensor(4.7338, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7771, 2.7554, 2.1775, 0.0376, 0.1206, 0.5034, 0.1555],\n",
      "       grad_fn=<AddBackward0>) tensor([8.1000, 2.4000, 1.7000, 0.0190, 0.1010, 0.6000, 0.1190]) tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.4787,  4.3396,  2.9075,  0.0389,  0.1368,  0.5454,  0.1796],\n",
      "       grad_fn=<AddBackward0>) tensor([8.9000, 3.5000, 5.0000, 0.0360, 0.0900, 0.5900, 0.2480]) tensor(2.5571, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.0800,  5.5351,  4.1425,  0.0378,  0.1472,  0.5883,  0.2312],\n",
      "       grad_fn=<AddBackward0>) tensor([15.5000,  6.4000,  9.2000,  0.0400,  0.1640,  0.4850,  0.4220]) tensor(4.7186, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8016,  2.9861,  2.9160,  0.0286,  0.1185,  0.5227,  0.2005],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000e+00, 7.0000e-01, 5.0000e-01, 4.0000e-03, 9.5000e-02, 5.6900e-01,\n",
      "        9.4000e-02]) tensor(10.0562, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9055, 1.7236, 2.1302, 0.0303, 0.1186, 0.4755, 0.1930],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 0.2000, 0.1000, 0.0370, 0.0420, 0.4690, 0.0510]) tensor(8.9717, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.7010, 1.7951, 0.7518, 0.0493, 0.1326, 0.4761, 0.0957],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 3.2000, 1.4000, 0.0910, 0.1690, 0.6080, 0.2590]) tensor(0.3501, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.1159,  2.0848, -0.3840,  0.0608,  0.1479,  0.4808,  0.0325],\n",
      "       grad_fn=<AddBackward0>) tensor([0.9000, 0.7000, 0.2000, 0.0620, 0.1330, 0.5950, 0.0870]) tensor(0.4128, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9183,  4.9226,  2.0588,  0.0593,  0.1685,  0.5515,  0.1338],\n",
      "       grad_fn=<AddBackward0>) tensor([8.2000, 4.8000, 1.0000, 0.0620, 0.1730, 0.6110, 0.0640]) tensor(1.2191, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0698,  5.2536,  2.0209,  0.0635,  0.1802,  0.5590,  0.1355],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 2.2000, 0.6000, 0.0670, 0.1670, 0.5120, 0.0950]) tensor(10.9243, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.4995,  7.6874,  3.7714,  0.0650,  0.2038,  0.6291,  0.2015],\n",
      "       grad_fn=<AddBackward0>) tensor([21.3000,  9.2000,  2.4000,  0.0960,  0.1800,  0.5760,  0.1150]) tensor(0.6888, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3233,  4.1289,  2.7456,  0.0454,  0.1480,  0.5345,  0.1840],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 2.1000, 1.0000, 0.0260, 0.1120, 0.5660, 0.0900]) tensor(3.9479, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4235,  4.1023,  2.7702,  0.0419,  0.1480,  0.5339,  0.1950],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5300e+01, 2.6000e+00, 1.4000e+00, 1.4000e-02, 6.9000e-02, 5.3400e-01,\n",
      "        6.7000e-02]) tensor(1.0970, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3709, 5.4072, 1.2639, 0.0701, 0.1846, 0.5730, 0.0793],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 5.7000, 1.8000, 0.1080, 0.1940, 0.6120, 0.1260]) tensor(2.0854, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.1690, 2.0285, 1.9584, 0.0307, 0.1101, 0.4893, 0.1576],\n",
      "       grad_fn=<AddBackward0>) tensor([5.9000, 2.5000, 1.3000, 0.0270, 0.1210, 0.5060, 0.1150]) tensor(0.3241, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.1471,  9.0253,  1.9588,  0.0993,  0.2614,  0.6501,  0.1035],\n",
      "       grad_fn=<AddBackward0>) tensor([13.5000,  8.2000,  1.7000,  0.0990,  0.2230,  0.5700,  0.1040]) tensor(3.1931, grad_fn=<MseLossBackward0>)\n",
      "tensor([-0.4561,  1.8526, -0.6480,  0.0675,  0.1582,  0.4678,  0.0303],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 2.0000, 0.0000, 0.0450, 0.1570, 0.5090, 0.0000]) tensor(1.5782, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6710, 2.1035, 1.5897, 0.0400, 0.1238, 0.4772, 0.1426],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 1.6000, 0.6000, 0.0150, 0.1030, 0.4460, 0.0590]) tensor(0.3121, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5553,  6.1162,  2.4100,  0.0651,  0.1847,  0.5875,  0.1352],\n",
      "       grad_fn=<AddBackward0>) tensor([12.9000,  9.9000,  1.2000,  0.1310,  0.2450,  0.6260,  0.0650]) tensor(2.3178, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9265, 3.3611, 1.0076, 0.0585, 0.1565, 0.5148, 0.0948],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 2.7000, 1.0000, 0.0430, 0.1550, 0.5830, 0.0990]) tensor(0.0952, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.3908,  3.0289,  3.7179,  0.0143,  0.0969,  0.5350,  0.2306],\n",
      "       grad_fn=<AddBackward0>) tensor([15.4000,  3.5000,  5.8000,  0.0160,  0.0930,  0.5430,  0.2620]) tensor(1.2278, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7589,  3.9323,  2.8427,  0.0327,  0.1254,  0.5409,  0.1777],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 0.8000, 1.6000, 0.0150, 0.0740, 0.4890, 0.2470]) tensor(10.9018, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.1082,  2.1679,  3.7834,  0.0155,  0.0923,  0.4990,  0.2497],\n",
      "       grad_fn=<AddBackward0>) tensor([7.7000, 1.8000, 3.9000, 0.0220, 0.0970, 0.5120, 0.3650]) tensor(2.7992, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8270, 0.1606, 0.4930, 0.0341, 0.1039, 0.4341, 0.1103],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 2.5000, 1.2000, 0.0430, 0.1610, 0.4550, 0.1570]) tensor(1.5287, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.7712,  5.2723,  5.0586,  0.0228,  0.1299,  0.5875,  0.2872],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7300e+01, 5.6000e+00, 6.0000e+00, 2.5000e-02, 1.3100e-01, 5.7000e-01,\n",
      "        2.8600e-01]) tensor(6.2314, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.5721, 1.4541, 0.6383, 0.0418, 0.1214, 0.4682, 0.0971],\n",
      "       grad_fn=<AddBackward0>) tensor([2.1000, 2.3000, 0.6000, 0.0530, 0.1270, 0.3180, 0.0720]) tensor(0.1376, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9694,  4.4468,  2.3860,  0.0428,  0.1415,  0.5481,  0.1479],\n",
      "       grad_fn=<AddBackward0>) tensor([10.1000,  3.8000,  1.9000,  0.0260,  0.1190,  0.5300,  0.0990]) tensor(0.2020, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2435,  5.4448,  2.0115,  0.0620,  0.1734,  0.5693,  0.1187],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 4.4000, 0.4000, 0.1280, 0.2120, 0.5620, 0.0400]) tensor(6.6454, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.0690,  5.9635,  2.9787,  0.0592,  0.1831,  0.5813,  0.1811],\n",
      "       grad_fn=<AddBackward0>) tensor([13.7000,  6.5000,  2.6000,  0.0960,  0.1880,  0.5120,  0.1620]) tensor(0.8643, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.9687,  3.6108,  3.6201,  0.0188,  0.1067,  0.5438,  0.2214],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 2.3000, 3.7000, 0.0240, 0.0840, 0.4740, 0.2480]) tensor(4.3649, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4202,  6.5757,  2.7344,  0.0676,  0.1986,  0.5857,  0.1678],\n",
      "       grad_fn=<AddBackward0>) tensor([8.5000, 4.5000, 0.5000, 0.0990, 0.1760, 0.5710, 0.0450]) tensor(10.2924, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9419, 4.5682, 1.4275, 0.0621, 0.1649, 0.5498, 0.0920],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 5.0000, 0.9000, 0.0540, 0.1390, 0.5920, 0.0500]) tensor(0.3305, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.5307,  3.9980,  2.1621,  0.0435,  0.1419,  0.5421,  0.1458],\n",
      "       grad_fn=<AddBackward0>) tensor([6.4000, 2.3000, 1.1000, 0.0230, 0.1100, 0.5070, 0.0990]) tensor(3.0113, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6710,  7.3406,  1.5465,  0.0944,  0.2389,  0.5853,  0.1030],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 2.3000, 0.5000, 0.0520, 0.1530, 0.4950, 0.0730]) tensor(17.1499, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7832,  2.8199,  2.7862,  0.0253,  0.1122,  0.5021,  0.2007],\n",
      "       grad_fn=<AddBackward0>) tensor([7.8000, 2.2000, 2.7000, 0.0310, 0.0800, 0.4710, 0.1950]) tensor(1.3276, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6137,  5.6266,  2.0522,  0.0622,  0.1761,  0.5758,  0.1190],\n",
      "       grad_fn=<AddBackward0>) tensor([11.3000,  5.2000,  4.0000,  0.0560,  0.1190,  0.5520,  0.1850]) tensor(0.5832, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.0938,  3.8430, -0.8582,  0.0906,  0.2096,  0.4977,  0.0148],\n",
      "       grad_fn=<AddBackward0>) tensor([0.7000, 1.2000, 0.5000, 0.1010, 0.1760, 0.3100, 0.2000]) tensor(1.5491, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.6932,  5.8383,  1.4663,  0.0789,  0.2103,  0.5553,  0.1162],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 3.4000, 0.7000, 0.0560, 0.1390, 0.5010, 0.0600]) tensor(2.4842, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.2912, 3.7309, 2.2691, 0.0407, 0.1321, 0.5324, 0.1461],\n",
      "       grad_fn=<AddBackward0>) tensor([1.2800e+01, 3.7000e+00, 2.0000e+00, 1.1000e-02, 1.0900e-01, 5.3100e-01,\n",
      "        9.7000e-02]) tensor(1.7699, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7344,  4.3587,  2.4043,  0.0440,  0.1467,  0.5485,  0.1578],\n",
      "       grad_fn=<AddBackward0>) tensor([18.1000,  7.2000,  2.6000,  0.0610,  0.1760,  0.5680,  0.1230]) tensor(6.9478, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8071, 4.4347, 0.7579, 0.0681, 0.1779, 0.5378, 0.0724],\n",
      "       grad_fn=<AddBackward0>) tensor([4.2000, 3.8000, 0.8000, 0.0810, 0.2040, 0.4520, 0.0850]) tensor(1.0300, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.8021,  5.5730,  3.1844,  0.0452,  0.1582,  0.5815,  0.1857],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 3.7000, 1.4000, 0.0240, 0.1440, 0.5440, 0.1010]) tensor(6.4525, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.5625, 3.7766, 2.0556, 0.0403, 0.1296, 0.5434, 0.1264],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 9.6000, 0.8000, 0.1530, 0.2780, 0.5930, 0.0530]) tensor(5.0797, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.6503,  7.0519,  3.1765,  0.0638,  0.1944,  0.6102,  0.1735],\n",
      "       grad_fn=<AddBackward0>) tensor([14.6000,  7.6000,  2.1000,  0.0960,  0.1700,  0.5130,  0.1090]) tensor(1.5399, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1527, 1.9425, 2.7003, 0.0214, 0.0992, 0.4929, 0.1974],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 1.5000, 2.2000, 0.0240, 0.0820, 0.4820, 0.2340]) tensor(1.6698, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.2875,  4.9112,  2.6145,  0.0562,  0.1704,  0.5453,  0.1767],\n",
      "       grad_fn=<AddBackward0>) tensor([18.1000,  7.7000,  3.7000,  0.0530,  0.1920,  0.5420,  0.1720]) tensor(4.5880, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8823,  4.7420,  2.0793,  0.0519,  0.1624,  0.5578,  0.1409],\n",
      "       grad_fn=<AddBackward0>) tensor([10.5000,  3.7000,  1.5000,  0.0210,  0.1100,  0.5000,  0.0790]) tensor(0.4776, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.8233e+01, 4.2311e+00, 4.9158e+00, 1.2231e-02, 1.0244e-01, 5.6431e-01,\n",
      "        2.7980e-01], grad_fn=<AddBackward0>) tensor([1.9600e+01, 2.4000e+00, 3.4000e+00, 1.8000e-02, 6.3000e-02, 5.7100e-01,\n",
      "        1.7900e-01]) tensor(1.0760, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7359,  2.3303,  3.6119,  0.0157,  0.0915,  0.5121,  0.2315],\n",
      "       grad_fn=<AddBackward0>) tensor([12.4000,  2.9000,  8.3000,  0.0200,  0.0850,  0.4940,  0.3870]) tensor(3.2526, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.4626,  4.8263,  1.6120,  0.0657,  0.1872,  0.5474,  0.1313],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 1.6000, 0.2000, 0.0610, 0.1840, 0.4330, 0.0470]) tensor(12.9955, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9813, 2.7342, 2.7082, 0.0272, 0.1053, 0.5173, 0.1732],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 2.3000, 2.7000, 0.0290, 0.0870, 0.5860, 0.1740]) tensor(0.1652, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2913, 1.8925, 2.4342, 0.0217, 0.0982, 0.4945, 0.1812],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 1.3000, 1.0000, 0.0220, 0.1190, 0.5170, 0.1800]) tensor(3.3556, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0979,  2.6261,  2.6819,  0.0245,  0.1068,  0.5170,  0.1843],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 1.0000, 0.9000, 0.0400, 0.1000, 0.4520, 0.1700]) tensor(8.6505, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.7494, 4.5641, 1.4653, 0.0716, 0.1896, 0.5387, 0.1199],\n",
      "       grad_fn=<AddBackward0>) tensor([8.0000, 2.3000, 0.7000, 0.0450, 0.0980, 0.5580, 0.0730]) tensor(1.2548, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.1831, 0.7426, 1.8307, 0.0271, 0.0961, 0.4431, 0.1679],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 0.9000, 0.3000, 0.0270, 0.1040, 0.2520, 0.0590]) tensor(1.5326, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7554,  3.1998,  3.1281,  0.0308,  0.1248,  0.5182,  0.2164],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 2.5000, 3.5000, 0.0170, 0.1040, 0.4690, 0.2790]) tensor(1.7964, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9322, 2.3282, 1.5538, 0.0378, 0.1199, 0.4971, 0.1284],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 2.8000, 0.9000, 0.0280, 0.1280, 0.4820, 0.0720]) tensor(0.1923, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5091, 3.6498, 2.1941, 0.0453, 0.1401, 0.5328, 0.1472],\n",
      "       grad_fn=<AddBackward0>) tensor([6.9000, 3.1000, 0.6000, 0.0240, 0.1350, 0.5090, 0.0450]) tensor(1.3803, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.5509,  5.9055,  2.9872,  0.0539,  0.1667,  0.5873,  0.1622],\n",
      "       grad_fn=<AddBackward0>) tensor([11.2000,  6.6000,  1.4000,  0.0420,  0.2020,  0.5510,  0.0720]) tensor(2.0344, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6999, 3.9583, 1.9339, 0.0533, 0.1562, 0.5230, 0.1443],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 4.6000, 0.5000, 0.1260, 0.2180, 0.4820, 0.0530]) tensor(3.7852, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2981,  6.2174,  2.1990,  0.0642,  0.1904,  0.5868,  0.1358],\n",
      "       grad_fn=<AddBackward0>) tensor([11.4000,  7.1000,  1.9000,  0.0760,  0.2110,  0.5090,  0.1110]) tensor(1.3250, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8611, 5.8673, 0.0559, 0.1077, 0.2557, 0.5326, 0.0646],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 1.8000, 0.9000, 0.0530, 0.1180, 0.4590, 0.1310]) tensor(5.1866, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.5314,  6.6157,  2.3684,  0.0727,  0.2018,  0.5945,  0.1362],\n",
      "       grad_fn=<AddBackward0>) tensor([15.4000, 11.2000,  1.4000,  0.1370,  0.2560,  0.5060,  0.0730]) tensor(3.2467, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.9987, 1.9112, 0.6865, 0.0455, 0.1233, 0.4810, 0.0810],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 3.8000, 0.4000, 0.1070, 0.2340, 0.4890, 0.0620]) tensor(0.5294, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.2827,  3.9328,  3.9836,  0.0244,  0.1277,  0.5382,  0.2645],\n",
      "       grad_fn=<AddBackward0>) tensor([19.6000,  4.6000,  5.0000,  0.0290,  0.1090,  0.5120,  0.2440]) tensor(0.9785, grad_fn=<MseLossBackward0>)\n",
      "tensor([2.1387e+01, 5.2871e+00, 5.0394e+00, 1.8643e-02, 1.2541e-01, 5.9438e-01,\n",
      "        2.8659e-01], grad_fn=<AddBackward0>) tensor([19.7000,  3.9000,  2.5000,  0.0400,  0.1050,  0.5310,  0.1710]) tensor(1.6052, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1634, 3.5448, 2.2606, 0.0463, 0.1394, 0.5200, 0.1547],\n",
      "       grad_fn=<AddBackward0>) tensor([9.9000, 4.6000, 2.0000, 0.0410, 0.1390, 0.5520, 0.1010]) tensor(0.2468, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.3935,  6.6300,  2.8710,  0.0637,  0.1925,  0.6028,  0.1628],\n",
      "       grad_fn=<AddBackward0>) tensor([12.2000,  5.7000,  1.2000,  0.0980,  0.1390,  0.4810,  0.0720]) tensor(3.0386, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7265,  4.3871,  2.8751,  0.0415,  0.1425,  0.5551,  0.1768],\n",
      "       grad_fn=<AddBackward0>) tensor([16.7000,  5.0000,  1.4000,  0.0290,  0.1280,  0.5700,  0.0660]) tensor(2.6219, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.8333, 4.6284, 1.4186, 0.0681, 0.1789, 0.5425, 0.1046],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 7.6000, 0.9000, 0.1160, 0.1880, 0.4870, 0.0560]) tensor(1.3088, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.6754, 2.5865, 2.9954, 0.0246, 0.1025, 0.5030, 0.1973],\n",
      "       grad_fn=<AddBackward0>) tensor([4.1000, 1.8000, 2.9000, 0.0210, 0.0780, 0.4380, 0.1970]) tensor(4.5311, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6161,  5.3842,  2.5595,  0.0584,  0.1735,  0.5617,  0.1616],\n",
      "       grad_fn=<AddBackward0>) tensor([13.9000,  5.8000,  2.2000,  0.0240,  0.1710,  0.6130,  0.0970]) tensor(0.0558, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7777, 3.4041, 1.6073, 0.0500, 0.1512, 0.5298, 0.1305],\n",
      "       grad_fn=<AddBackward0>) tensor([6.2000, 2.9000, 1.1000, 0.0770, 0.1400, 0.5040, 0.1160]) tensor(1.0226, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1870,  5.0484,  3.0025,  0.0437,  0.1455,  0.5720,  0.1672],\n",
      "       grad_fn=<AddBackward0>) tensor([12.6000,  6.5000,  2.5000,  0.0780,  0.1320,  0.5690,  0.1200]) tensor(0.3868, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9168,  4.1736,  3.8348,  0.0257,  0.1179,  0.5569,  0.2230],\n",
      "       grad_fn=<AddBackward0>) tensor([1.1300e+01, 1.8000e+00, 5.0000e+00, 8.0000e-03, 6.6000e-02, 5.6400e-01,\n",
      "        2.7100e-01]) tensor(2.8683, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.3074,  5.0263,  4.0836,  0.0303,  0.1283,  0.5718,  0.2253],\n",
      "       grad_fn=<AddBackward0>) tensor([13.4000,  4.4000,  7.8000,  0.0400,  0.0990,  0.5200,  0.3290]) tensor(3.2387, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.3021,  4.1155,  3.5053,  0.0238,  0.1155,  0.5585,  0.2070],\n",
      "       grad_fn=<AddBackward0>) tensor([14.0000,  4.0000,  6.5000,  0.0270,  0.1050,  0.5250,  0.2820]) tensor(1.2971, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.4660,  2.9901,  2.2698,  0.0429,  0.1480,  0.4988,  0.1959],\n",
      "       grad_fn=<AddBackward0>) tensor([2.4000, 0.8000, 0.4000, 0.0540, 0.0690, 0.3550, 0.1030]) tensor(12.9314, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2759, 3.3682, 1.4316, 0.0535, 0.1556, 0.5224, 0.1244],\n",
      "       grad_fn=<AddBackward0>) tensor([9.3000, 3.6000, 1.3000, 0.0390, 0.1300, 0.4990, 0.0870]) tensor(0.1604, grad_fn=<MseLossBackward0>)\n",
      "tensor([17.7679,  4.8069,  3.9924,  0.0282,  0.1341,  0.5646,  0.2435],\n",
      "       grad_fn=<AddBackward0>) tensor([20.8000,  3.1000,  3.0000,  0.0280,  0.0690,  0.5600,  0.1460]) tensor(1.8723, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.0455, 2.2398, 1.3157, 0.0403, 0.1340, 0.4901, 0.1394],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 1.6000, 0.4000, 0.0180, 0.2500, 0.5740, 0.1290]) tensor(2.6363, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 0.4315, -0.5286,  1.1534,  0.0245,  0.0791,  0.4278,  0.1347],\n",
      "       grad_fn=<AddBackward0>) tensor([0.4000, 0.4000, 1.2000, 0.0000, 0.0800, 0.5000, 0.3530]) tensor(0.1313, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.8908,  6.4315,  2.6410,  0.0703,  0.2021,  0.5791,  0.1672],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AddBackward0>) tensor([10.4000,  6.4000,  1.6000,  0.0950,  0.2100,  0.5380,  0.1120]) tensor(4.4626, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4105, 4.9088, 1.6267, 0.0579, 0.1633, 0.5558, 0.1028],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 6.6000, 0.8000, 0.1250, 0.2110, 0.4860, 0.0540]) tensor(3.8141, grad_fn=<MseLossBackward0>)\n",
      "tensor([-5.2529e+00,  4.6575e-01, -1.1586e+00,  5.6866e-02,  1.2549e-01,\n",
      "         4.3384e-01,  1.9494e-03], grad_fn=<AddBackward0>) tensor([0., 0., 0., 0., 0., 0., 0.]) tensor(4.1943, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2857,  5.6871,  1.7360,  0.0700,  0.1957,  0.5734,  0.1197],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 3.0000, 0.5000, 0.0910, 0.1680, 0.5390, 0.0730]) tensor(6.7165, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.2162,  7.3310,  2.2242,  0.0781,  0.2190,  0.6097,  0.1312],\n",
      "       grad_fn=<AddBackward0>) tensor([20.2000, 15.2000,  2.5000,  0.1360,  0.3360,  0.5930,  0.1160]) tensor(11.1264, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6642,  5.0303,  3.1129,  0.0438,  0.1464,  0.5699,  0.1761],\n",
      "       grad_fn=<AddBackward0>) tensor([8.8000, 2.6000, 1.9000, 0.0300, 0.0760, 0.5110, 0.1160]) tensor(4.4357, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.7496, 6.2778, 0.3893, 0.0955, 0.2307, 0.5582, 0.0497],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 4.2000, 1.0000, 0.0760, 0.1580, 0.6140, 0.0930]) tensor(2.0887, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.4041, 3.3301, 2.2430, 0.0393, 0.1311, 0.5224, 0.1559],\n",
      "       grad_fn=<AddBackward0>) tensor([3.4000, 2.5000, 1.4000, 0.0650, 0.1730, 0.4420, 0.1790]) tensor(5.3513, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0838,  4.8346,  1.8338,  0.0616,  0.1760,  0.5564,  0.1288],\n",
      "       grad_fn=<AddBackward0>) tensor([15.6000,  7.4000,  1.5000,  0.0380,  0.2130,  0.5770,  0.0790]) tensor(3.8706, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.0868,  6.1309,  1.4903,  0.0733,  0.1955,  0.5880,  0.0864],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 4.6000, 1.1000, 0.0890, 0.1640, 0.4920, 0.0750]) tensor(4.5034, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2864, 5.3909, 1.2351, 0.0705, 0.1801, 0.5618, 0.0732],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 3.1000, 0.2000, 0.1110, 0.1080, 0.5460, 0.0140]) tensor(5.3626, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.1249, 3.8668, 1.8894, 0.0470, 0.1435, 0.5398, 0.1284],\n",
      "       grad_fn=<AddBackward0>) tensor([8.6000, 5.5000, 1.4000, 0.1010, 0.1570, 0.4850, 0.0850]) tensor(0.4558, grad_fn=<MseLossBackward0>)\n",
      "tensor([20.0918,  8.9285,  2.2702,  0.1027,  0.2733,  0.6435,  0.1377],\n",
      "       grad_fn=<AddBackward0>) tensor([26.2000, 12.5000,  3.1000,  0.1190,  0.2600,  0.5560,  0.1590]) tensor(7.2519, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9442,  3.6489,  3.1598,  0.0285,  0.1112,  0.5363,  0.1833],\n",
      "       grad_fn=<AddBackward0>) tensor([4.7000, 2.6000, 4.6000, 0.0160, 0.1100, 0.4940, 0.3200]) tensor(6.0264, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2221, 3.7699, 1.5989, 0.0493, 0.1463, 0.5354, 0.1156],\n",
      "       grad_fn=<AddBackward0>) tensor([10.3000,  5.6000,  1.6000,  0.0640,  0.1600,  0.5600,  0.0940]) tensor(1.0955, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.7292,  4.7150,  3.6791,  0.0362,  0.1459,  0.5500,  0.2367],\n",
      "       grad_fn=<AddBackward0>) tensor([21.7000,  4.1000,  2.5000,  0.0230,  0.0880,  0.5370,  0.1120]) tensor(3.7853, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.1937,  4.7743,  2.7915,  0.0433,  0.1488,  0.5606,  0.1716],\n",
      "       grad_fn=<AddBackward0>) tensor([9.6000, 4.1000, 0.7000, 0.0910, 0.1410, 0.5470, 0.0580]) tensor(2.5370, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.2403, 1.6291, 2.9454, 0.0150, 0.0831, 0.4843, 0.2036],\n",
      "       grad_fn=<AddBackward0>) tensor([2.6000, 1.4000, 0.6000, 0.0400, 0.1170, 0.4940, 0.1130]) tensor(5.3396, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.9661, 2.8924, 1.1087, 0.0484, 0.1405, 0.5105, 0.1041],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.1000, 0.6000, 0.0130, 0.1080, 0.5460, 0.1040]) tensor(1.5119, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.3974, 4.1596, 2.1833, 0.0454, 0.1387, 0.5443, 0.1323],\n",
      "       grad_fn=<AddBackward0>) tensor([5.7000, 3.5000, 1.0000, 0.0570, 0.1120, 0.5470, 0.0620]) tensor(2.2160, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.9157,  5.2853,  3.1026,  0.0496,  0.1571,  0.5758,  0.1735],\n",
      "       grad_fn=<AddBackward0>) tensor([11.3000,  5.2000,  4.6000,  0.0400,  0.1440,  0.5410,  0.2120]) tensor(1.2992, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.7053,  5.5351,  2.0148,  0.0602,  0.1794,  0.5704,  0.1318],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 5.4000, 0.9000, 0.1010, 0.1670, 0.4770, 0.0750]) tensor(2.8300, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.7133,  4.7390,  2.4194,  0.0453,  0.1480,  0.5591,  0.1474],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 4.4000, 2.5000, 0.0260, 0.1480, 0.5080, 0.1500]) tensor(1.5861, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0639, 2.7644, 2.4455, 0.0267, 0.1086, 0.5220, 0.1645],\n",
      "       grad_fn=<AddBackward0>) tensor([8.1000, 2.6000, 2.0000, 0.0290, 0.1130, 0.5400, 0.1460]) tensor(0.1650, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7051,  2.3667,  2.9708,  0.0257,  0.1101,  0.4909,  0.2170],\n",
      "       grad_fn=<AddBackward0>) tensor([12.9000,  2.4000,  6.2000,  0.0190,  0.0720,  0.5190,  0.3240]) tensor(2.1800, grad_fn=<MseLossBackward0>)\n",
      "tensor([3.2692, 2.2605, 0.2156, 0.0612, 0.1572, 0.4827, 0.0775],\n",
      "       grad_fn=<AddBackward0>) tensor([4.0000, 2.3000, 0.7000, 0.0590, 0.1470, 0.5880, 0.1180]) tensor(0.1119, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.3070,  4.9153,  3.3221,  0.0411,  0.1540,  0.5678,  0.2106],\n",
      "       grad_fn=<AddBackward0>) tensor([21.2000, 10.7000,  1.6000,  0.0970,  0.2290,  0.5150,  0.0850]) tensor(8.6282, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.3120, 2.7106, 2.3431, 0.0257, 0.1047, 0.5102, 0.1600],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 1.9000, 1.1000, 0.0270, 0.1000, 0.5190, 0.1040]) tensor(1.0787, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.0259, 3.3368, 1.1709, 0.0611, 0.1704, 0.5026, 0.1284],\n",
      "       grad_fn=<AddBackward0>) tensor([2.9000, 1.5000, 0.1000, 0.0790, 0.1360, 0.4250, 0.0270]) tensor(4.4019, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9106,  4.7292,  3.2621,  0.0373,  0.1423,  0.5641,  0.1990],\n",
      "       grad_fn=<AddBackward0>) tensor([15.5000,  3.1000,  5.2000,  0.0370,  0.0840,  0.5440,  0.2700]) tensor(0.9666, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9658, 3.3232, 1.4869, 0.0477, 0.1441, 0.5195, 0.1229],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 2.1000, 0.6000, 0.0680, 0.0820, 0.5510, 0.0690]) tensor(0.8246, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.3991, 3.6342, 0.9440, 0.0601, 0.1666, 0.5258, 0.0983],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 2.1000, 0.5000, 0.0490, 0.1580, 0.5180, 0.0730]) tensor(2.3193, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9178,  3.7807,  2.5771,  0.0349,  0.1268,  0.5353,  0.1658],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.0000, 2.1000, 0.0260, 0.0830, 0.5390, 0.3010]) tensor(9.4302, grad_fn=<MseLossBackward0>)\n",
      "tensor([-12.2488,  -1.8625,  -2.7720,   0.0418,   0.0997,   0.3129,  -0.0174],\n",
      "       grad_fn=<AddBackward0>) tensor([0., 0., 0., 0., 0., 0., 0.]) tensor(23.0424, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.6658,  7.4621,  1.8993,  0.0817,  0.2211,  0.6095,  0.1061],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 8.2000, 0.5000, 0.0830, 0.2140, 0.4670, 0.0260]) tensor(3.7436, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6997,  4.7914,  2.1274,  0.0573,  0.1641,  0.5433,  0.1389],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 3.5000, 0.8000, 0.0290, 0.1470, 0.5280, 0.0640]) tensor(3.9205, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1547,  5.5633,  1.1775,  0.0775,  0.2014,  0.5502,  0.0958],\n",
      "       grad_fn=<AddBackward0>) tensor([3.0000, 2.7000, 0.5000, 0.0900, 0.1500, 0.4410, 0.0570]) tensor(8.5520, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0337,  3.3498,  1.8272,  0.0486,  0.1534,  0.5079,  0.1604],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 1.6000, 0.3000, 0.0710, 0.1440, 0.4370, 0.0770]) tensor(7.4437, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.4949,  3.9931,  2.3450,  0.0465,  0.1512,  0.5366,  0.1660],\n",
      "       grad_fn=<AddBackward0>) tensor([5.0000, 1.8000, 0.6000, 0.0640, 0.1360, 0.4930, 0.0990]) tensor(7.1494, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.0082,  6.2939,  2.9498,  0.0558,  0.1771,  0.5990,  0.1647],\n",
      "       grad_fn=<AddBackward0>) tensor([13.4000,  5.5000,  2.6000,  0.0390,  0.1490,  0.5390,  0.1270]) tensor(1.0802, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.5107,  4.7600,  1.9387,  0.0597,  0.1733,  0.5452,  0.1408],\n",
      "       grad_fn=<AddBackward0>) tensor([4.8000, 3.5000, 0.3000, 0.1030, 0.1550, 0.5700, 0.0330]) tensor(7.0459, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.6788,  5.5820,  2.5049,  0.0551,  0.1712,  0.5764,  0.1512],\n",
      "       grad_fn=<AddBackward0>) tensor([9.7000, 7.4000, 0.8000, 0.1390, 0.2480, 0.5790, 0.0630]) tensor(3.1519, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9961, 4.2271, 2.0731, 0.0493, 0.1433, 0.5496, 0.1219],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 4.8000, 2.3000, 0.0320, 0.1240, 0.5680, 0.0980]) tensor(0.4654, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5936,  4.1666,  3.3502,  0.0303,  0.1240,  0.5573,  0.1976],\n",
      "       grad_fn=<AddBackward0>) tensor([10.8000,  2.7000,  3.1000,  0.0300,  0.0920,  0.5370,  0.1880]) tensor(1.4314, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4473,  5.0518,  3.8394,  0.0323,  0.1343,  0.5838,  0.2143],\n",
      "       grad_fn=<AddBackward0>) tensor([15.5000,  4.4000,  3.7000,  0.0310,  0.1070,  0.5420,  0.1770]) tensor(0.1922, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.4132, 2.8117, 2.2970, 0.0339, 0.1178, 0.5152, 0.1580],\n",
      "       grad_fn=<AddBackward0>) tensor([5.5000, 2.3000, 1.3000, 0.0340, 0.1160, 0.5270, 0.1130]) tensor(1.3921, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.5838,  3.8592,  2.9247,  0.0338,  0.1313,  0.5378,  0.1919],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8100e+01, 3.4000e+00, 2.4000e+00, 1.5000e-02, 9.0000e-02, 5.2700e-01,\n",
      "        1.2500e-01]) tensor(4.4174, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 2.2082,  2.4510, -0.1430,  0.0689,  0.1661,  0.4816,  0.0555],\n",
      "       grad_fn=<AddBackward0>) tensor([1.5000, 1.6000, 0.3000, 0.1040, 0.3450, 0.6700, 0.0830]) tensor(0.2131, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.5721,  5.3043,  4.4744,  0.0236,  0.1296,  0.5849,  0.2565],\n",
      "       grad_fn=<AddBackward0>) tensor([15.3000,  4.9000,  5.3000,  0.0750,  0.0960,  0.4890,  0.2730]) tensor(2.7299, grad_fn=<MseLossBackward0>)\n",
      "tensor([-1.6851,  1.9230, -0.8787,  0.0755,  0.1698,  0.4375,  0.0295],\n",
      "       grad_fn=<AddBackward0>) tensor([0.0000, 0.2000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000]) tensor(0.9717, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.2391,  2.9558,  3.1320,  0.0213,  0.1030,  0.5183,  0.2034],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 2.6000, 5.0000, 0.0230, 0.0780, 0.4830, 0.2670]) tensor(0.8133, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.0769,  2.7788,  3.5215,  0.0206,  0.1067,  0.5180,  0.2342],\n",
      "       grad_fn=<AddBackward0>) tensor([8.4000, 1.4000, 1.5000, 0.0330, 0.0530, 0.5070, 0.1360]) tensor(3.9821, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5726, 1.6693, 2.0200, 0.0292, 0.1110, 0.4772, 0.1751],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 0.4000, 0.9000, 0.0200, 0.0380, 0.4170, 0.2000]) tensor(4.3823, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.4245, 3.4717, 0.6122, 0.0700, 0.1798, 0.5073, 0.0924],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 0.7000, 0.4000, 0.0890, 0.0870, 0.5760, 0.1280]) tensor(4.1611, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.4350, 2.8486, 1.2657, 0.0530, 0.1528, 0.5100, 0.1249],\n",
      "       grad_fn=<AddBackward0>) tensor([3.2000, 1.1000, 0.1000, 0.0430, 0.1270, 0.4310, 0.0140]) tensor(3.1959, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.5427,  3.2865,  3.8997,  0.0240,  0.1102,  0.5120,  0.2478],\n",
      "       grad_fn=<AddBackward0>) tensor([3.3000, 1.1000, 0.6000, 0.0280, 0.1050, 0.4910, 0.0960]) tensor(17.2295, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.2293, 1.5625, 1.1562, 0.0380, 0.1186, 0.4644, 0.1277],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 1.5000, 0.5000, 0.0000, 0.1250, 0.1820, 0.0800]) tensor(1.2998, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.9135,  7.1480,  3.2499,  0.0619,  0.1963,  0.6131,  0.1841],\n",
      "       grad_fn=<AddBackward0>) tensor([12.0000,  5.5000,  0.6000,  0.0560,  0.2050,  0.5150,  0.0440]) tensor(8.2234, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.4309,  4.3547,  3.7932,  0.0279,  0.1251,  0.5580,  0.2252],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9700e+01, 3.0000e+00, 4.4000e+00, 1.0000e-02, 7.4000e-02, 5.4500e-01,\n",
      "        1.7700e-01]) tensor(2.9192, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0091,  5.1083,  3.3762,  0.0416,  0.1470,  0.5674,  0.1965],\n",
      "       grad_fn=<AddBackward0>) tensor([17.9000,  4.7000,  7.4000,  0.0350,  0.1140,  0.5670,  0.3360]) tensor(3.5336, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1687,  3.5776,  2.0818,  0.0514,  0.1546,  0.5135,  0.1636],\n",
      "       grad_fn=<AddBackward0>) tensor([14.3000,  4.0000,  3.5000,  0.0310,  0.0950,  0.5370,  0.1680]) tensor(2.7517, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0796, 4.2182, 1.6811, 0.0513, 0.1529, 0.5427, 0.1175],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 1.9000, 0.8000, 0.0610, 0.1240, 0.4760, 0.1050]) tensor(3.8755, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.1517, 2.6959, 1.8418, 0.0429, 0.1299, 0.5057, 0.1401],\n",
      "       grad_fn=<AddBackward0>) tensor([3.6000, 1.5000, 1.0000, 0.0770, 0.0670, 0.4960, 0.1240]) tensor(2.1084, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.0142,  5.5188,  3.1469,  0.0574,  0.1794,  0.5762,  0.1949],\n",
      "       grad_fn=<AddBackward0>) tensor([17.3000,  9.9000,  1.1000,  0.0680,  0.2460,  0.4910,  0.0680]) tensor(3.5809, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1158,  3.8081,  2.5250,  0.0336,  0.1204,  0.5435,  0.1512],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 1.6000, 2.8000, 0.0110, 0.0830, 0.5720, 0.2150]) tensor(4.3024, grad_fn=<MseLossBackward0>)\n",
      "tensor([11.8859,  4.4756,  1.9751,  0.0587,  0.1745,  0.5383,  0.1540],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 3.8000, 0.7000, 0.1150, 0.2480, 0.6040, 0.1090]) tensor(6.8780, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.9535,  6.2057,  2.6651,  0.0600,  0.1809,  0.5976,  0.1491],\n",
      "       grad_fn=<AddBackward0>) tensor([10.3000,  6.4000,  1.1000,  0.0870,  0.2220,  0.5530,  0.0780]) tensor(3.4502, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.3998,  3.2854, -0.7681,  0.0842,  0.1922,  0.4975,  0.0139],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 2.3000, 0.1000, 0.0980, 0.1530, 0.3640, 0.0140]) tensor(0.2620, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.0981,  2.9002,  3.6359,  0.0141,  0.0904,  0.5213,  0.2229],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3200e+01, 2.2000e+00, 4.6000e+00, 1.3000e-02, 7.4000e-02, 5.7000e-01,\n",
      "        2.4300e-01]) tensor(0.3767, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.7464,  5.6136,  1.9597,  0.0654,  0.1773,  0.5726,  0.1101],\n",
      "       grad_fn=<AddBackward0>) tensor([7.9000, 4.7000, 3.5000, 0.0440, 0.1430, 0.5020, 0.1740]) tensor(1.6171, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.1956, 4.1131, 1.0171, 0.0628, 0.1670, 0.5385, 0.0845],\n",
      "       grad_fn=<AddBackward0>) tensor([5.8000, 4.6000, 1.1000, 0.0470, 0.1740, 0.5150, 0.0800]) tensor(0.3132, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.6331,  5.0415,  2.0140,  0.0580,  0.1649,  0.5681,  0.1183],\n",
      "       grad_fn=<AddBackward0>) tensor([11.3000,  9.9000,  0.9000,  0.1180,  0.2210,  0.7080,  0.0440]) tensor(3.6175, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.1449, 3.6913, 1.9858, 0.0491, 0.1423, 0.5198, 0.1359],\n",
      "       grad_fn=<AddBackward0>) tensor([3.7000, 1.6000, 0.7000, 0.0350, 0.1070, 0.5360, 0.0740]) tensor(3.6843, grad_fn=<MseLossBackward0>)\n",
      "tensor([ 1.1556, -0.3680,  0.5311,  0.0218,  0.0942,  0.3924,  0.1440],\n",
      "       grad_fn=<AddBackward0>) tensor([0.6000, 0.0000, 0.2000, 0.0000, 0.0000, 0.2730, 0.1180]) tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.9589, 2.8127, 0.2533, 0.0625, 0.1513, 0.5018, 0.0482],\n",
      "       grad_fn=<AddBackward0>) tensor([2.0000, 5.1000, 0.2000, 0.0750, 0.2950, 0.4920, 0.0190]) tensor(0.7511, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.5791,  5.1664,  3.1347,  0.0415,  0.1537,  0.5748,  0.1924],\n",
      "       grad_fn=<AddBackward0>) tensor([13.3000,  4.6000,  1.3000,  0.0520,  0.1480,  0.4990,  0.0920]) tensor(1.2710, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.1895, 1.8617, 1.7465, 0.0361, 0.1213, 0.4758, 0.1623],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8000, 0.7000, 0.7000, 0.0200, 0.0930, 0.4710, 0.1640]) tensor(4.4989, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0311,  5.1009,  3.3116,  0.0429,  0.1519,  0.5684,  0.1968],\n",
      "       grad_fn=<AddBackward0>) tensor([11.4000,  5.3000,  0.8000,  0.0680,  0.2150,  0.5050,  0.0670]) tensor(2.7941, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.8978,  3.4065,  2.6133,  0.0348,  0.1271,  0.5277,  0.1766],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 2.0000, 3.0000, 0.0320, 0.1240, 0.4190, 0.3610]) tensor(6.5292, grad_fn=<MseLossBackward0>)\n",
      "tensor([18.8730,  6.2722,  4.0345,  0.0445,  0.1638,  0.5978,  0.2241],\n",
      "       grad_fn=<AddBackward0>) tensor([22.2000,  8.9000,  1.9000,  0.0710,  0.2050,  0.5490,  0.0900]) tensor(3.2219, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.2484,  4.7865,  2.6940,  0.0471,  0.1501,  0.5623,  0.1583],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 4.9000, 1.6000, 0.0790, 0.1700, 0.4620, 0.1220]) tensor(5.7521, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.8593,  6.8364,  1.3649,  0.0918,  0.2343,  0.5871,  0.0965],\n",
      "       grad_fn=<AddBackward0>) tensor([7.3000, 4.6000, 0.9000, 0.1240, 0.2320, 0.6640, 0.1280]) tensor(5.1616, grad_fn=<MseLossBackward0>)\n",
      "tensor([4.2230, 2.2226, 0.8632, 0.0478, 0.1346, 0.4902, 0.0993],\n",
      "       grad_fn=<AddBackward0>) tensor([4.3000, 3.1000, 0.7000, 0.0160, 0.1950, 0.4690, 0.0610]) tensor(0.1156, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.1546,  3.9477,  2.3133,  0.0461,  0.1429,  0.5389,  0.1504],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 4.0000, 1.4000, 0.0750, 0.1050, 0.5410, 0.0830]) tensor(0.1385, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.6738, 3.3771, 0.6479, 0.0662, 0.1731, 0.5027, 0.0909],\n",
      "       grad_fn=<AddBackward0>) tensor([2.3000, 1.8000, 0.1000, 0.1230, 0.1690, 0.5460, 0.0290]) tensor(2.0255, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9649,  2.9596,  3.7922,  0.0132,  0.0916,  0.5273,  0.2318],\n",
      "       grad_fn=<AddBackward0>) tensor([5.6000, 1.4000, 2.6000, 0.0150, 0.0850, 0.4880, 0.2520]) tensor(8.2996, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.4148,  6.2839,  3.2209,  0.0541,  0.1740,  0.5987,  0.1770],\n",
      "       grad_fn=<AddBackward0>) tensor([11.7000,  4.2000,  1.7000,  0.0380,  0.1320,  0.5100,  0.1020]) tensor(4.1287, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5145, 1.9175, 2.3859, 0.0230, 0.0971, 0.4850, 0.1763],\n",
      "       grad_fn=<AddBackward0>) tensor([6.5000, 2.0000, 1.8000, 0.0080, 0.0840, 0.5150, 0.1430]) tensor(0.1974, grad_fn=<MseLossBackward0>)\n",
      "tensor([15.0246,  5.4646,  3.2463,  0.0512,  0.1637,  0.5763,  0.1866],\n",
      "       grad_fn=<AddBackward0>) tensor([15.4000,  6.3000,  1.5000,  0.0360,  0.1450,  0.5250,  0.0630]) tensor(0.5581, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.1901,  6.1778,  2.8738,  0.0594,  0.1733,  0.5939,  0.1489],\n",
      "       grad_fn=<AddBackward0>) tensor([8.8000, 8.5000, 1.1000, 0.1050, 0.2560, 0.5110, 0.0610]) tensor(5.3737, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.6897, 3.9880, 1.9828, 0.0484, 0.1429, 0.5395, 0.1255],\n",
      "       grad_fn=<AddBackward0>) tensor([6.1000, 4.8000, 0.6000, 0.1000, 0.1780, 0.5690, 0.0480]) tensor(1.3269, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.3491,  2.8765,  3.7709,  0.0135,  0.0905,  0.5140,  0.2345],\n",
      "       grad_fn=<AddBackward0>) tensor([8.1000, 2.2000, 3.6000, 0.0170, 0.0810, 0.4940, 0.2390]) tensor(2.6488, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.5618, 2.2604, 2.1507, 0.0331, 0.1248, 0.4982, 0.1816],\n",
      "       grad_fn=<AddBackward0>) tensor([1.4000, 0.6000, 0.6000, 0.0490, 0.1000, 0.3190, 0.2560]) tensor(10.2594, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.2670,  7.3424,  1.4500,  0.0888,  0.2344,  0.6064,  0.0944],\n",
      "       grad_fn=<AddBackward0>) tensor([6.7000, 4.4000, 0.3000, 0.1370, 0.2050, 0.5540, 0.0420]) tensor(9.6071, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.8564, 0.9401, 1.2362, 0.0297, 0.1238, 0.4339, 0.1826],\n",
      "       grad_fn=<AddBackward0>) tensor([1.3000, 0.0000, 0.3000, 0.0000, 0.0000, 0.2860, 0.1430]) tensor(4.6677, grad_fn=<MseLossBackward0>)\n",
      "tensor([1.5693e+01, 3.4532e+00, 4.3765e+00, 9.4174e-03, 9.2072e-02, 5.4731e-01,\n",
      "        2.5748e-01], grad_fn=<AddBackward0>) tensor([12.2000,  2.8000,  5.7000,  0.0240,  0.0810,  0.4910,  0.3110]) tensor(2.0546, grad_fn=<MseLossBackward0>)\n",
      "tensor([12.9066,  4.9676,  2.5217,  0.0544,  0.1668,  0.5486,  0.1671],\n",
      "       grad_fn=<AddBackward0>) tensor([8.8000, 3.3000, 1.1000, 0.0360, 0.1410, 0.5990, 0.0930]) tensor(3.0964, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.9432, 4.6091, 0.4426, 0.0811, 0.2059, 0.5384, 0.0748],\n",
      "       grad_fn=<AddBackward0>) tensor([16.4000,  6.8000,  1.8000,  0.0700,  0.1720,  0.5660,  0.1020]) tensor(11.1663, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.9706,  3.8849,  2.4904,  0.0371,  0.1311,  0.5404,  0.1605],\n",
      "       grad_fn=<AddBackward0>) tensor([6.6000, 1.9000, 0.4000, 0.0400, 0.0910, 0.5140, 0.0370]) tensor(3.9185, grad_fn=<MseLossBackward0>)\n",
      "tensor([14.3329,  7.1738,  1.8498,  0.0813,  0.2194,  0.6070,  0.1069],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 6.7000, 0.7000, 0.1200, 0.1770, 0.5520, 0.0460]) tensor(3.1577, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.5647, 5.0331, 0.5821, 0.0822, 0.2025, 0.5336, 0.0695],\n",
      "       grad_fn=<AddBackward0>) tensor([2.8000, 4.1000, 0.4000, 0.1920, 0.2020, 0.5640, 0.0490]) tensor(3.3742, grad_fn=<MseLossBackward0>)\n",
      "tensor([6.9410, 2.2120, 1.4831, 0.0460, 0.1383, 0.4900, 0.1437],\n",
      "       grad_fn=<AddBackward0>) tensor([1.7000, 0.3000, 0.3000, 0.0000, 0.0690, 0.4210, 0.1540]) tensor(4.6479, grad_fn=<MseLossBackward0>)\n",
      "tensor([16.8451,  4.8669,  3.9856,  0.0348,  0.1408,  0.5610,  0.2392],\n",
      "       grad_fn=<AddBackward0>) tensor([16.4000,  4.1000,  2.9000,  0.0430,  0.0950,  0.5010,  0.1550]) tensor(0.2825, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0706, 2.3532, 2.6278, 0.0272, 0.1076, 0.5025, 0.1847],\n",
      "       grad_fn=<AddBackward0>) tensor([6.0000, 1.8000, 0.8000, 0.0300, 0.0970, 0.4840, 0.0880]) tensor(1.8693, grad_fn=<MseLossBackward0>)\n",
      "tensor([9.0595, 2.6257, 2.8021, 0.0218, 0.0970, 0.5106, 0.1806],\n",
      "       grad_fn=<AddBackward0>) tensor([5.3000, 1.4000, 2.6000, 0.0140, 0.0990, 0.5300, 0.2730]) tensor(2.2409, grad_fn=<MseLossBackward0>)\n",
      "tensor([19.0552,  5.3364,  4.3634,  0.0293,  0.1372,  0.5796,  0.2520],\n",
      "       grad_fn=<AddBackward0>) tensor([1.8600e+01, 3.1000e+00, 3.0000e+00, 1.8000e-02, 9.2000e-02, 5.5800e-01,\n",
      "        1.6800e-01]) tensor(1.0110, grad_fn=<MseLossBackward0>)\n",
      "tensor([13.4468,  5.7851,  2.7750,  0.0569,  0.1687,  0.5814,  0.1507],\n",
      "       grad_fn=<AddBackward0>) tensor([4.6000, 2.9000, 1.0000, 0.0510, 0.1570, 0.4600, 0.0960]) tensor(12.8227, grad_fn=<MseLossBackward0>)\n",
      "tensor([8.9175, 3.5201, 1.7629, 0.0515, 0.1517, 0.5122, 0.1427],\n",
      "       grad_fn=<AddBackward0>) tensor([5.1000, 3.5000, 1.0000, 0.0370, 0.1680, 0.4860, 0.0940]) tensor(2.1656, grad_fn=<MseLossBackward0>)\n",
      "tensor([10.0295,  4.5589,  1.4729,  0.0733,  0.1930,  0.5268,  0.1290],\n",
      "       grad_fn=<AddBackward0>) tensor([9.8000, 6.5000, 1.3000, 0.1110, 0.2120, 0.5800, 0.0900]) tensor(0.5509, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.7864, 4.2680, 1.3820, 0.0529, 0.1518, 0.5443, 0.0942],\n",
      "       grad_fn=<AddBackward0>) tensor([4.5000, 4.5000, 2.4000, 0.0680, 0.1380, 0.4830, 0.1540]) tensor(1.6997, grad_fn=<MseLossBackward0>)\n",
      "tensor([5.3561, 2.7468, 1.1056, 0.0521, 0.1438, 0.5042, 0.1045],\n",
      "       grad_fn=<AddBackward0>) tensor([2.7000, 1.7000, 0.1000, 0.0970, 0.1530, 0.5740, 0.0300]) tensor(1.3106, grad_fn=<MseLossBackward0>)\n",
      "tensor([7.6469, 3.6902, 0.7267, 0.0722, 0.1892, 0.4999, 0.1097],\n",
      "       grad_fn=<AddBackward0>) tensor([1.9000, 0.8000, 0.2000, 0.0290, 0.1100, 0.4120, 0.0670]) tensor(5.9537, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([620., 445., 311., 237., 170., 143., 106.,  67.,  56.,  50.,  36.,\n",
       "         21.,  18.,  11.,  12.,   5.,   5.,   7.,   1.,   3.,   3.,   3.,\n",
       "          1.,   4.]),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24]),\n",
       " <BarContainer object of 24 artists>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = []\n",
    "x = []\n",
    "for i in range(len(test_x)):\n",
    "    print(neural(test_x[i]), test_y[i], loss(neural(test_x[i]), test_y[i]))\n",
    "    l.append(loss(neural(test_x[i]), test_y[i]).item())\n",
    "    x.append(i)\n",
    "plt.hist(l, range(0,25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3pSid13aHNU"
   },
   "source": [
    "K-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5M-VXudYR7Y"
   },
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = nd.concat(X_train, X_part, dim=0)\n",
    "            y_train = nd.concat(y_train, y_part, dim=0)\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "def k_fold(k, X_train, y_train, num_epochs,\n",
    "           learning_rate, weight_decay, batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_net()\n",
    "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
    "                                   weight_decay, batch_size)\n",
    "        train_l_sum += train_ls[-1]\n",
    "        valid_l_sum += valid_ls[-1]\n",
    "        if i == 0:\n",
    "            plt.semilogy(range(1, num_epochs + 1), train_ls)\n",
    "            plt.semilogy(range(1, num_epochs + 1), valid_ls)\n",
    "            plt.legend(['train', 'valid'])\n",
    "        print('fold %d, train rmse: %f, valid rmse: %f' % (\n",
    "            i, train_ls[-1], valid_ls[-1]))\n",
    "    return train_l_sum / k, valid_l_sum / k"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COMP562.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
